CMU和DeepMind新方法：让视觉-语言模型自生成记忆以应对数据不足问题  
  
  - 原文标题：数据不够致Scaling Law撞墙？CMU和DeepMind新方法可让VLM自己生成记忆  
  - 链接：[机器之心](https://mp.weixin.qq.com/s?src=11&timestamp=1735955552&ver=5729&signature=B4gIeSxsH0mHdChOk7-uEV1vyxv2QswU26en8b31Iu0DUtOE13hntWN26xoZeyjVC9TL5ilvhNGAsv56Kio5FfxiKib6Y-LQ49KwJz9cufOiWQ3TnwipbeEWQtCQiFwk&new=1)  

- **文章类别**：新闻报道  

---

**内容整理**：

### 文章信息

- 论文标题：VLM Agents Generate Their Own Memories: Distilling Experience into Embodied Programs of Thought
- 论文地址：[openreview](https://openreview.net/pdf?id=5G7MRfPngt)
- 项目地址：[ical-learning](https://ical-learning.github.io/)
- 代码地址：[GitHub](https://github.com/Gabesarch/ICAL)

### 背景知识
- **Scaling Law的挑战**：AI社区正在讨论Scaling Law是否撞墙的问题，主要原因是高质量数据的耗尽。预计到2028年左右，现有的数据储量将被全部利用完.
- **人类学习能力**：人类具有出色的少样本学习能力，可以通过观察行为与内部世界模型结合，快速泛化到相关情况，分辨成功与不相关的因素，并通过练习和反馈找到正确的抽象，帮助模仿和调整任务以适应各种情况。

### 研究方法
- **ICAL方法**：卡内基梅隆大学（CMU）和Google DeepMind的研究团队提出了一种名为In-Context Abstraction Learning（ICAL）的方法，通过使用低质量数据和反馈，让大型语言模型（LLM）和视觉-语言模型（VLM）根据次优演示和人工反馈创建有效的提示词，从而改善决策并减少对专家演示的依赖。
- **抽象类型**：ICAL可以处理四种类型的认知抽象：
  - **任务和因果关系**：确定实现目标所需的基本原则或行动，以及要素如何通过因果关系相互关联.
  - **对象状态的变化**：描述对象将采取的各种形式或条件.
  - **时间抽象**：将任务分解为子目标.
  - **任务建构**：突出任务中的关键视觉细节.
- **抽象生成过程**：每一轮迭代都始于一个有噪声的轨迹，分为两个阶段进行抽象：
  - **抽象阶段**：VLM借助语言评论来纠正错误，并让序列更加丰富.
  - **有人类参与的阶段**：在此阶段，序列会在环境中执行，其抽象过程由以自然语言传达的人类反馈指导，具体流程包括优化轨迹的执行、监控与干预、反馈整合与轨迹修正、环境重置与重试、成功标准与反馈限度、保存示例.

### 实验与结果
- **实验环境**：研究者在TEACh、VisualWebArena和Ego4D基准测试中测试了ICAL的任务规划能力和动作预测能力.
  - **TEACh**：针对家庭环境中的对话式教学，ICAL在未见过的任务上表现优于作为上下文示例的未被处理的演示，成功率比带有预测动作的原始演示提高了17.9%，比带有真实动作注释的演示提高了8.6%.
  - **VisualWebArena**：多模态自动化网络任务，ICAL获得了SOTA性能，使用GPT4V时从14.3%提高到22.7%，使用GPT4o时从18.9%提高到23.4%.
  - **Ego4D**：用于视频动作预测，ICAL的表现优于使用思维链的少样本GPT4V，分别将名词和动作编辑距离缩短了6.4和1.7，并且与完全监督式方法相差无几，但使用的领域内训练数据减少了639倍.

### 关键结论
- **减少对专家示例的依赖**：新方法显著减少了对专家示例的依赖，并且相比于使用“缺乏此类抽象的动作规划和轨迹”的上下文学习，新方法始终更优.
- **良好的Scaling能力**：随着示例数量增长，ICAL也能获得明显的提升，表明这种新方法也能很好地Scaling.

### 研究亮点
- **创新性**：提出了一种新的方法，通过上下文抽象学习来解决高质量数据不足的问题，为AI模型的发展提供了新的思路.
- **实用性**：在多个实验环境中验证了ICAL的有效性，展示了其在不同任务和环境中的应用潜力.
- **减少数据依赖**：通过利用低质量数据和反馈，减少了对高质量数据的依赖，为AI模型的进一步发展提供了可能.