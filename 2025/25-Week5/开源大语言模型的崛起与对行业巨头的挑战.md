开源大语言模型的崛起与对行业巨头的挑战
    
- 原文标题：Google 旧闻重温：《我们没有护城河，OpenAI 也没有》
    
- 链接：[https://baoyu.io/blog/google-openai-no-moat](https://baoyu.io/blog/google-openai-no-moat)
    
- **文章类别**：博客文章
    

---

**内容整理**：

**文章框架**：

复制

```
├── 导读
│   ├── DeepSeek 开源模型 R1 的发布及意义
│   ├── 开源创新的活力与行业影响
│   ├── 未来人工智能领域的趋势
├── Google 内部文件《我们没有护城河，OpenAI 也没有》
│   ├── 开源社区的崛起
│   ├── 开源模型的优势
│   ├── 开源模型对 Google 和 OpenAI 的影响
├── 开源模型的发展历程
│   ├── LLaMA 模型的泄露
│   ├── 开源社区的快速创新
│   ├── 开源模型的迭代速度与优势
├── 开源模型对商业战略的影响
│   ├── 免费开源模型的竞争力
│   ├── 开源模型的创新优势
├── 拥抱开源生态系统的必要性
│   ├── Meta 的开源策略成功
│   ├── Google 和 OpenAI 的应对策略
└── 时间线
    ├── 2023 年 2 月 - 4 月 开源模型的关键进展
```

**文章内容**：

### 导读

- **DeepSeek 开源模型 R1 的发布及意义**  
    DeepSeek 公布了其全新开源模型 R1，其性能已非常接近 OpenAI 的 o1 模型。这一进展表明开源创新的活力正在持续爆发，小型团队通过轻量化微调、聚焦高质量数据以及快速版本迭代，能够迅速推出性能出众的模型。
    
- **开源创新的活力与行业影响**  
    开源社区在大语言模型上的迭代与竞争力不断攀升，甚至有可能颠覆当前闭源模型的领先地位。开源模型的开源性为开发者和研究者提供了自由掌握和调试模型的机会，降低了行业应用的门槛。
    
- **未来人工智能领域的趋势**  
    未来人工智能领域的版图将从“单点领先”转向“群体协同”，创新速度会更快。谁能率先拥抱开源生态、加快迭代脚步，谁就更有机会赢得用户和口碑。
    

### Google 内部文件《我们没有护城河，OpenAI 也没有》

- **开源社区的崛起**  
    开源社区在大语言模型领域的发展速度惊人，已经解决了许多 Google 和 OpenAI 认为的“重大未解决问题”，例如在手机上运行的大语言模型、可扩展的个人 AI、多模态模型等。
    
- **开源模型的优势**  
    开源模型具有速度快、可定制性强、注重隐私且功能全面等优势。开源社区通过低成本的微调方法（如 LoRA）和高质量的小数据集，实现了快速迭代和创新。
    
- **开源模型对 Google 和 OpenAI 的影响**  
    开源模型的快速发展对 Google 和 OpenAI 的商业战略产生了直接冲击。免费且无限制的开源模型可能会使用户不愿付费使用受限的闭源模型。
    

### 开源模型的发展历程

- **LLaMA 模型的泄露**  
    2023 年 3 月初，Meta 的 LLaMA 模型权重意外泄露，开源社区迅速基于此模型进行创新。
    
- **开源社区的快速创新**  
    在短短一个多月内，开源社区实现了指令微调、量化、质量提升、人工评测、多模态、RLHF 等多种改进，迭代速度远超大型企业。
    
- **开源模型的迭代速度与优势**  
    开源模型通过低成本的微调方法（如 LoRA）和高质量的小数据集，实现了快速迭代和创新。这些模型的训练成本低，迭代速度快，且不受版权许可的限制。
    

### 开源模型对商业战略的影响

- **免费开源模型的竞争力**  
    如果能免费获得质量可比且无使用限制的模型，用户将更倾向于选择开源模型而非付费使用受限的闭源模型。
    
- **开源模型的创新优势**  
    开源模型的创新速度和灵活性使其在某些领域超越了闭源模型。开源社区的创新成果可以被任何个人或机构直接利用，进一步推动了技术的发展。
    

### 拥抱开源生态系统的必要性

- **Meta 的开源策略成功**  
    Meta 通过开源 LLaMA 模型，获得了全球范围内的免费人力资源，开源创新大多基于其体系结构，Meta 可以直接将这些成果纳入自己的产品。
    
- **Google 和 OpenAI 的应对策略**  
    Google 和 OpenAI 需要重新评估自己的策略，考虑通过开源部分模型权重等方式，与开源社区合作，共同推动技术发展，而不是单纯依赖闭源模型。
    

### 时间线

- **2023 年 2 月 24 日** - LLaMA 上线  
    Meta 发布了 LLaMA 模型，开源了代码但未开源权重。
    
- **2023 年 3 月 3 日** - LLaMA 权重泄露  
    LLaMA 权重在网上被泄露，开源社区开始基于此进行创新。
    
- **2023 年 3 月 12 日** - 在树莓派上运行语言模型  
    成功在树莓派上运行 LLaMA 模型，尽管速度较慢。
    
- **2023 年 3 月 13 日** - 在笔记本上进行微调  
    斯坦福发布了 Alpaca，利用 LoRA 实现了低成本的指令微调。
    
- **2023 年 3 月 18 日** - 无需 GPU 的快速运行方案  
    使用 4 位量化在 MacBook CPU 上运行 LLaMA，速度足够快可实际使用。
    
- **2023 年 3 月 19 日** - 130 亿参数模型媲美 Bard  
    发布 Vicuna 模型，训练成本仅 300 美元，性能接近 Bard。
    
- **2023 年 3 月 25 日** - 多模型生态系统  
    Nomic 创建了 GPT4All，包含多个开源模型，训练成本仅 100 美元。
    
- **2023 年 3 月 28 日** - 开源的 GPT-3  
    Cerebras 使用最优计算调度训练 GPT-3 架构，超越已有 GPT-3 克隆。
    
- **2023 年 3 月 28 日** - 一小时内完成多模态训练  
    LLaMA-Adapter 在一小时内完成指令微调和多模态微调。
    
- **2023 年 4 月 3 日** - 130 亿参数模型与 ChatGPT 难分伯仲  
    伯克利推出 Koala，训练成本仅 100 美元，性能接近 ChatGPT。
    
- **2023 年 4 月 15 日** - 开源 RLHF 达到 ChatGPT 水平  
    Open Assistant 发布开源 RLHF 数据集，训练成本低且易于使用。
    

**文章标签**：

#开源模型， #人工智能， #大语言模型， #行业趋势