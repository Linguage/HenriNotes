强化学习基础：概念、贝尔曼方程与策略评估
- 原文标题：强化学习：概念和贝尔曼方程
- 链接：https://mp.weixin.qq.com/s/I1TjXMwSInlW7CFe1SXlGw 

- **文章类别**：博客 

---
**内容整理**： 

```markdown
├── 引言
│   ├── 文章基于西湖大学赵世钰老师《强化学习的数学原理》课程整理
│   ├── 提供课程书籍与视频链接
├── 一、基本概念
│   ├── 1. 策略
│   │   ├── 以表格形式表示，如在S1状态下，执行a2、a3的概率都是0.5
│   │   ├── 编程实现策略示例代码
│   ├── 2. 奖励Reward
│   │   ├── 一个实数，可正可负，正代表奖励，负代表惩罚，0也是奖励
│   │   ├── 奖励依赖于当前的状态和动作，而非下一个状态
│   ├── 3. 回报Return
│   │   ├── 沿着轨迹收集的所有奖励之和
│   │   ├── 用来评估一个策略的好坏
│   ├── 4. Episode回合
│   │   ├── 一个轨迹称为Episode
├── 二、马尔可夫决策过程MDP
│   ├── Markov property：无记忆性，给定当前状态，过去的任何信息都不会对未来状态的预测提供额外的帮助
│   ├── decision：策略定义了在给定状态下，智能体应采取何种动作的概率分布
│   ├── process：涉及State、Action、Reward、概率分布（状态转移概率、奖励概率）
├── 三、Bellman方程
│   ├── 1950年左右，理查德·贝尔曼提出最优性原理和贝尔曼方程
│   ├── 最优性原理：如果一个策略在每个子问题上都是最优的，那么它对整个问题是全局最优的
│   ├── 贝尔曼方程描述了如何通过未来的可能价值来计算当前的价值，有状态价值函数和动作价值函数两种形式
├── 四、State value状态值
│   ├── 多步轨迹的折现回报的期望（平均值）
│   ├── 回报与状态值的区别：回报针对单个轨迹，状态值是对多个轨迹求回报后取平均值
│   ├── 状态值函数的元素形式与矩阵-向量形式
│   ├── 通过示例说明如何计算状态值
├── 五、Action Value动作值
│   ├── 表示从某个状态开始，执行某个特定动作可期望获得的平均回报
│   ├── 状态值和动作值的关系：从一个状态出发的状态值等于选择不同动作得到动作值的平均值
├── 六、Bellman最优性方程
│   ├── 策略比较与最优策略定义
│   ├── 最优策略存在且唯一
├── 七、小结
│   ├── 1. 状态值和动作值
│   │   ├── 涉及“bootstrap”概念，贝尔曼方程描述所有状态之间值的关系
│   │   ├── 策略评价通过求解贝尔曼方程得到状态值
│   │   ├── 动作值在寻找最优政策方面起更直接作用
│   ├── 2. 状态值、动作值和最优策略
│   │   ├── 通过Q值可得最优策略，V值仍有多个重要用途
│   │   ├── 状态值函数与策略之间的关系
│   ├── 3. Bootstrapping概念解释
└── 结语
    ├── 文章通过具体示例和详细解释，帮助读者理解强化学习的基本概念和贝尔曼方程的重要性
```

文章从强化学习的基本概念入手，逐步深入到马尔可夫决策过程、贝尔曼方程等核心内容，最后通过小结部分强化了状态值、动作值与最优策略之间的关系，为读者提供了一个清晰的强化学习知识框架。
