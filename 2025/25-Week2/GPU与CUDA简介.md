GPU与CUDA简介
- 原文标题：GPU 与 CUDA 简介 - 知乎
- 链接：[GPU 与 CUDA 简介 - 知乎](https://zhuanlan.zhihu.com/p/686772546)

- **文章类别**：博客文章

---

## 内容整理

### 文章框架

```markdown
一、计算机基础知识
    1.1 硬盘与IO设备
    1.2 CPU与内存
    1.3 并行与并发
    1.4 编译器
    1.5 堆与栈
二、GPU基础知识
    2.1 GPU是计算设备
    2.2 GPU的执行单元是流多处理器
    2.3 GPU产品和架构
    2.4 GPU内存
三、CUDA基础知识
    3.1 CUDA是通用并行计算平台和编程模型
    3.2 CUDA编程基础
    3.3 CUDA线程模型
    3.4 CUDA多维线程模型
    3.5 CUDA内存管理
    3.6 nvcc编译
四、总结与引用
    4.1 总结
    4.2 未提及引用
```

### 一、计算机基础知识

#### 1.1 硬盘与IO设备
- **IO设备**：种类繁多，包括键盘、鼠标、手柄、显示器、声卡、网卡等。驱动程序是使这些设备在电脑上正常运行的程序。
- **硬盘**：存储数据的地方，常见类型有光盘CD、闪存、机械硬盘HDD、固态硬盘SSD等。光盘一般是只读的，U盘、存储卡和固态硬盘SSD属于闪存，相较于机械硬盘HDD，具有读写速度快、功耗低、价格贵等特点。

#### 1.2 CPU与内存
- **CPU**：主要由控制单元CU和运算单元ALU构成。控制单元负责读取、分析和执行机器码形式的指令，操控电脑的其它组件，包括输入输出设备的控制、内存和硬盘数据的读写、调用运算单元进行计算以及处理异常情况和特殊请求。CPU中还有存储单元，主要包括寄存器和多级缓存。
- **内存**：一般用SRAM代指多级缓存，DRAM代指内存。多级缓存用于减少访问内存的次数，提高数据访问速度。

#### 1.3 并行与并发
- **并发**：同一时间点有多个任务需要处理，但只有一个工人。常用调度方式有先来先处理、后来先处理、执行时间短的先处理等。并发编程主要用于解决IO密集型任务。
- **并行**：同一时间点有多个任务需要处理，但有多个工人。并行计算强调工人的数量，工人数越多，机器的并行能力越高。CPU的配置中的“核心数”指并行能力，“线程数”指并发能力。

#### 1.4 编译器
- **编译器**：将高级语言代码转换成机器码的程序。编译器的本质是代码转换器。编译和运行可以在不同机器上完成。编译器的设计分为前端和后端，前端负责代码解析和优化，生成中间代码，后端负责将中间代码转换成机器码。

#### 1.5 堆与栈
- **堆**：由程序员用malloc函数主动分配的内存，内存释放有两种方式：由程序员用free函数主动释放或程序结束时自动释放。
- **栈**：由程序自动分配和释放，主要用于存放函数的参数值、局部变量等内容。栈区域和栈数据结构的思想相似，都采用FILO的方式。

### 二、GPU基础知识

#### 2.1 GPU是计算设备
- **GPU**：全称为Graphics Processing Unit，即图形处理单元。GPU属于外接设备，需要安装显卡驱动才能保证程序的正常运行。GPU的硬件架构等价于CPU+内存，可以进行独立的计算。在GPU编程时，将计算机称为主机，GPU称为设备，两者之间是异步执行的关系。

#### 2.2 GPU的执行单元是流多处理器
- **流多处理器（SM）**：GPU的基本执行单元。一个CPU Core在一个时间点只能执行1个任务，而一个SM在一个时间点最多可以执行32个任务。SM中的实际运算单元称为CUDA Core，一个SM中包含多个CUDA Core，因此可以并行执行多个任务。

#### 2.3 GPU产品和架构
- **GPU产品**：面向消费者，如GeForce系列面向游戏玩家，Quadro系列用于高精度科学计算，Tesla系列用于深度学习领域等。
- **GPU架构**：GPU的底层设计，面向开发者。相同架构不同产品的GPU具有相同的参数，如SM中的CUDA Core数量、L1/L2 Cache的大小等。GPU架构用科学家的名字命名，如Tesla、Fermi、Kepler等。

#### 2.4 GPU内存
- **GPU内存层级**：从高到低依次是寄存器文件、L1 Cache、L2 Cache和DRAM。寄存器文件和L1 Cache属于片上内存，存在于SM上；L2 Cache和DRAM属于片外内存，存在于SM之外。GPU DRAM和主机的DRAM通过PCIe总线相连。

### 三、CUDA基础知识

#### 3.1 CUDA是通用并行计算平台和编程模型
- **CUDA**：英伟达官方定义为通用的并行计算平台和编程模型。安装显卡驱动后，可以使用nvidia-smi指令查看显卡的运行状况。开发者需要安装CUDA Toolkit，其中包含nvcc编译器和Nvidia Nsight工具集。CUDA的主要开发语言是C/C++。

#### 3.2 CUDA编程基础
- **修饰符**：CUDA中定义了许多新的修饰符，用于修饰函数。主机上运行的函数用__host__修饰，设备上运行的函数用__device__修饰。同时用__device__和__host__修饰的函数，既是主机函数又是设备函数。
- **内存管理**：CUDA中有对应的内存管理函数，如cudaMalloc、cudaMemcpy、cudaMemset和cudaFree，这些函数都是主机函数。

#### 3.3 CUDA线程模型
- **线程模型**：CUDA编程的核心是理解线程模型。线程结构是嵌套关系的，多个线程构成线程块，多个线程块构成网格。核函数是实际在显卡上运行的函数，只能调用设备函数。

#### 3.4 CUDA多维线程模型
- **多维线程模型**：grid和block可以是一维数组、二维数组或三维数组。在核函数中，gridDim、blockIdx、blockDim、threadIdx都是dim3类型的对象，有x、y和z三个属性值。

#### 3.5 CUDA内存管理
- **内存类型**：线程的内存分为寄存器内存和本地内存。寄存器内存属于稀缺资源，如果寄存器内存不够用，会发生溢出到L1 Cache、L2 Cache或HBM中，溢出部分的内存称为本地内存。共享内存存在于L1 Cache之上，一个block中的所有线程都可以访问，用于减少访问HBM的次数。

#### 3.6 nvcc编译
- **nvcc编译器**：CUDA程序使用nvcc编译器，其参数和gcc编译器很像。nvcc在编译时，会将代码分成主机代码和设备代码。主机代码的编译和gcc一样，设备代码会先转换成PTX伪汇编代码，再编译成cubin形式的目标代码。

### 四、总结与引用

#### 4.1 总结
- 文章总结了计算机知识体系和数学知识体系的差异，指出计算机知识体系中会抽象出很多概念帮助读者理解计算机的运作方式，但真实的运作方式并不会详细告知读者。同时，提到了计算机领域的从业人员喜欢“起名”，以突出产品的亮点。

#### 4.2 未提及引用
- 文章列出了一些未提及的引用链接，包括英伟达产品系列分类、架构、数据中心GPU所有型号及参数汇总，理解C语言中几个常见修饰符，一文读懂堆与栈的区别，CUDA编程基础入门系列等。