深度学习的困境：人工智能的未来需要混合模型  
  原文标题：Deep Learning Is Hitting a Wall  
  链接：[Deep Learning Is Hitting a Wall - Nautilus](https://nautil.us/deep-learning-is-hitting-a-wall-238440/?utm_source=substack&utm_medium=email)  

- **文章类别**：博客文章  

---

**内容整理**：  

**文章框架**：  
```
├── 引言：深度学习的局限性
│   └── Geoffrey Hinton的观点与现实的对比
├── 深度学习的现状与问题
│   ├── 深度学习在医学影像等领域的局限性
│   ├── 深度学习系统的不可靠性
│   └── 深度学习在自动驾驶中的挑战
├── 深度学习的“规模化”困境
│   ├── OpenAI的规模化理论
│   └── 规模化可能带来的问题
├── 符号操作与深度学习的结合
│   ├── 符号操作的历史与重要性
│   └── 混合模型的必要性
├── 混合模型的前景与挑战
│   ├── 混合模型的成功案例
│   └── 混合模型面临的挑战
└── 结论：混合模型是未来人工智能的方向
```  

**文章标签**：  
#深度学习， #人工智能， #混合模型， #符号操作  

**文章内容**：  

**引言：深度学习的局限性**  
文章开头引用了深度学习领域的“教父”Geoffrey Hinton在2016年的一次演讲，他预测深度学习将在五年内取代放射科医生。然而，到2022年，这一预测并未实现。作者指出，尽管深度学习在图像识别等领域取得了巨大进展，但在需要高精度和可靠性的领域（如放射学和自动驾驶）仍面临重大挑战。  

**深度学习的现状与问题**  
文章指出，深度学习在处理低风险任务（如照片标记）时表现出色，但在高风险任务（如放射学诊断和自动驾驶）中存在明显不足。例如，深度学习系统在处理“异常”情况时容易出现错误，如特斯拉自动驾驶系统未能识别手持停车牌的人。此外，深度学习系统在语言理解方面也存在缺陷，如OpenAI的GPT-3模型在生成文本时容易产生错误和误导性信息。  

**深度学习的“规模化”困境**  
文章提到，OpenAI的研究表明，随着数据量的增加，深度学习模型的性能会不断提高。然而，作者认为这种“规模化”理论存在缺陷。首先，当前的测试方法无法真正衡量模型的深度理解能力。其次，所谓的“规模化定律”并非普遍规律，而是基于当前数据的观察，可能在未来不再成立。此外，最新的研究表明，随着模型规模的扩大，其在毒性、真实性、推理和常识等方面的性能并未显著提升。  

**符号操作与深度学习的结合**  
作者强调，符号操作是计算机科学的基础，也是实现人工智能的关键。符号操作能够提供一种结构化的方式来表示和处理信息，而深度学习则擅长从大量数据中学习模式。作者认为，将两者结合的混合模型可能是未来人工智能的发展方向。文章提到，早期的人工智能研究者曾尝试将符号操作与神经网络结合，但由于历史原因，这一方向被忽视了。  

**混合模型的前景与挑战**  
文章指出，混合模型已经在一些领域取得了成功，如AlphaGo和AlphaFold2。这些模型结合了符号操作和深度学习的优点，既能够处理复杂的逻辑和推理任务，又能够从大量数据中学习。然而，混合模型也面临一些挑战，如如何更好地整合符号操作和深度学习的技术，以及如何设计出更加高效和可靠的混合架构。  

**结论：混合模型是未来人工智能的方向**  
作者认为，未来的人工智能需要结合深度学习和符号操作的优点，以实现更可靠、更智能的系统。文章呼吁人工智能研究者重新审视符号操作的重要性，并积极探索混合模型的可能性。作者强调，只有通过开放的心态和跨领域的合作，人工智能才能真正实现其潜力。


---

回顾过去，我写这篇文章写得太早了；世界还没有准备好接受我所要表达的观点。但它所表达的很多内容都得到了证实，尤其是在过去的三个月，特别是最近几周，与文章的结论完全一致。

以下是五个观察：

论文的一个关键论点是，纯粹扩展LLM（即仅仅向现有的LLM架构添加更多的数据和计算）并不能将我们带到AGI，而所谓的缩放定律只是经验性的概括，而不是物理定律。长期以来，很少有人相信我，但现在这些结论已被广泛接受，以至于萨蒂亚·纳德拉本人最近几乎一字不差地重复了它们；马克·安德森也接近于此。伊利亚·苏茨克维尔在他的NeurIPS演讲中也表达了类似的观点。（当然，这些人都没有承认我具有远见卓识；政治和经济因素阻止了他们这样做。）诚然，现在有了一个**新的**缩放定律，不是关于增加纯LLM，而是关于增加所谓的测试时计算的**时间**。在一段时间内，这在一定程度上是有效的（但见下文），但我们需要新技术的事实，实际上证实了《深度学习正在撞墙》（DLHW）的另一个核心论点，即除了纯LLM之外，我们还需要新的技术。

我在2022年提出的另一个关键建议是，我们应该使用神经符号技术，将神经网络与经典的符号技术（如规则）结合起来。在某种程度上，较新的模型**确实**在这样做。OpenAI没有透露o1的具体工作方式，但例如DeepSeek的R1模型（OpenAI承认它类似于他们的测试时推理系统o1）明确包含一个“基于规则的奖励系统”，用于验证某些类别的答案。神经符号技术取得了胜利！（AlphaFold获得诺贝尔奖是神经符号技术的又一次胜利。）

自2022年的论文以来，我们取得了一个巨大的进步（当时我对还会有多少次飞跃持怀疑态度），但我们**仍然**没有一个真正配得上GPT-5这个名称的系统。奥特曼本人最近表示4.5即将推出，但没有给出GPT-5的日期。自2022年8月OpenAI向比尔·盖茨演示GPT-4以来，人们一直在不断增加数据和计算，但尽管投入了数千亿美元，纯LLM的扩展并没有产生一些人想象的结果。（请注意，测试时计算系统并**不是**像GPT-4相对于GPT-3或GPT-3相对于GPT-2那样全面的改进，而是在某些领域（如编码和数学）的改进。）DLHW没有明确地说“再来一次巨大的、全面的飞跃，仅此而已”，但我们得到的就是这样的结果，这与我在那里发出的警告基本一致，并且与数百亿美元投资的精神完全背道而驰，这些投资基于这样一种观念，即更多数据和计算的回报基本上是无限的。

即使是像Deep Research这样的最新系统，仍然在一些方面存在不足——而这些不足几乎完全对应于我警告过的LLM的阿喀琉斯之踵：幻觉和推理错误。备受吹捧的Deep Research关税论文显然捏造了一些数字，科林·弗雷泽的实验表明，它在时间推理方面存在问题（例如，关于运动员以及他们过去效力过的球队的推理）。Derek Lowe在《科学》杂志上发表的一篇文章仔细研究了DeepSeek，得出的结论是，Deep Research流畅的输出是不可信的（“与所有LLM输出一样，所有这些东西都以同样流畅、自信的语气呈现：你必须事先了解这些材料，才能意识到你的脚已经穿过了之前坚固的地板。对我来说，这是它们最有害的特征之一。我知道这些东西并非专门设计用来掩盖或隐藏它们的弱点和错误，但它们在这方面做得非常好，这并不是你真正想要的。因此，尽管我发现Deep Research输出的某些部分令人印象深刻，但我发现它更深层次的研究问题难以解决。”）

我在DLHW中提出的论点（我直到2023年下半年和2024年初的文章中才真正阐明）的一个后果是，顶端出现了一种拥挤：如果纯LLM的扩展耗尽，你就会期望有多个团队竞争并达到收益递减点，基本上没有护城河，并且在价格上展开激烈竞争。那个时代也已经明显到来。最近，DeepSeek在很大程度上与OpenAI的o1相匹配，加速了价格战，OpenAI（已经）被迫降价。LLM曾经是新颖的，现在很大程度上是一种商品。这对生成式AI的经济前景意味着什么，还有待观察。

我认为可以公平地说，“深度学习正在撞墙”并没有预测像Deep Research这样的系统会表现得如此出色，但在大多数其他方面，从预测纯LLM的放缓到对神经符号AI的需求，再到持续存在的推理和幻觉问题，这篇文章都非常准确。另一方面，嘲笑是完全错误的，并且象征着一种新的体制，在这种体制下，寡头们试图将其信仰强加于科学，推动市场，但实际上并没有解决仍然摆在我们面前的潜在研究挑战。