以 DeepSeek R1 为例学习推理型大语言模型  
- 原文标题：以 DeepSeek R1 为例学习“推理型大语言模型  
- 链接：[文章链接](https://mp.weixin.qq.com/s/DG8-bENYNji8qg4SwexEmg)  

- **文章类别**：博客  

---

**内容整理**：

### 文章框架
```
├── 引言
│   ├── 文章背景
│   └── 推理型大语言模型（Reasoning LLMs）的定义
├── 推理模型的必要性
│   ├── 推理模型的适用场景
│   └── 推理模型的优劣势
├── DeepSeek R1 的训练流程
│   ├── DeepSeek-R1-Zero（纯 RL）
│   ├── DeepSeek-R1（SFT + RL）
│   └── DeepSeek-R1-Distill（蒸馏）
├── 构建推理模型的四种主要方法
│   ├── 推理阶段扩展（Inference-time scaling）
│   ├── 纯强化学习（Pure RL）
│   ├── 监督微调 + 强化学习（SFT + RL）
│   └── 纯监督微调与蒸馏（SFT + Distill）
├── DeepSeek R1 的性能与评价
│   ├── 与 OpenAI o1 的比较
│   └── 训练成本与开源许可
├── 低成本推理模型的探索
│   ├── Sky-T1（低成本 SFT）
│   └── TinyZero（低成本纯 RL）
└── 结论与未来展望
```

### 文章标签
#推理型大语言模型 ， #DeepSeekR1 ， #强化学习 ， #监督微调 ，#模型蒸馏

### 文章内容详细整理

#### 引言
- **背景**：2024 年以来，大语言模型（LLM）在专业化方向上快速发展，尤其在需要多步推理的复杂任务上，如解题、数学证明、代码生成等。文章以 DeepSeek R1 为例，探讨推理型大语言模型的定义、应用场景、优劣势及主要实现方法。
- **推理型大语言模型的定义**：推理型大语言模型（Reasoning LLMs）是指能够通过多步推理回答复杂问题的模型。与常规 LLM 相比，推理模型通常会包含中间推理步骤，展现出“思考”过程。

#### 推理模型的必要性
- **适用场景**：推理模型主要用于需要复杂中间推理步骤的任务，如解谜、高级数学题、编程挑战等。对于简单的任务，如摘要、翻译或知识检索，推理模型可能并不适用。
- **优劣势**：
  - **优势**：能够处理复杂的推理任务，提供更详细的解答过程。
  - **劣势**：推理模型通常更耗资源、输出更冗长，且可能因“过度思考”而出现错误。

#### DeepSeek R1 的训练流程
- **DeepSeek-R1-Zero（纯 RL）**：
  - 基于 DeepSeek-V3（671B 参数规模）基础模型，通过纯强化学习（RL）训练。
  - 跳过了监督微调（SFT）步骤，直接使用 RL。
  - 使用准确性奖励（如 LeetCode 编译器判定编程答案是否正确）和格式奖励（LLM 评估器判断回答格式）。
  - 模型在训练中涌现出推理能力，能够在回答中自动生成推理线索。
- **DeepSeek-R1（SFT + RL）**：
  - 在 DeepSeek-R1-Zero 的基础上，增加了监督微调（SFT）和多轮 RL 训练。
  - 先用 R1-Zero 生成 SFT 数据，进行指令微调，然后进行 RL。
  - RL 阶段增加了“语言一致性”奖励，防止模型在回答中切换语言。
  - 性能优于 R1-Zero，推理能力显著提升。
- **DeepSeek-R1-Distill（蒸馏）**：
  - 使用 DeepSeek-R1 和 DeepSeek-V3 生成的 SFT 数据微调较小模型（如 Llama 8B、70B，Qwen 1.5B–30B）。
  - 蒸馏模型更小、推理成本更低，但性能稍弱于 DeepSeek-R1。

#### 构建推理模型的四种主要方法
- **推理阶段扩展（Inference-time scaling）**：
  - 在推理阶段投入更多计算资源，以换取更高质量的输出结果。
  - 常见做法包括改进提示工程（如链式思考提示）、投票或搜索策略（如束搜索）。
  - 不需要额外训练模型，但会增加推理成本。
- **纯强化学习（Pure RL）**：
  - 通过纯 RL 训练模型，无需监督微调。
  - DeepSeek-R1-Zero 通过纯 RL 涌现出推理能力，但性能有限。
  - 纯 RL 更适合理论研究，实际应用中通常需要结合 SFT。
- **监督微调 + 强化学习（SFT + RL）**：
  - 先进行监督微调，再进行多轮强化学习。
  - DeepSeek-R1 是这一方法的典型代表，性能最强。
  - 结合 SFT 和 RL 可以显著提升模型的推理能力。
- **纯监督微调与蒸馏（SFT + Distill）**：
  - 使用大模型生成的 SFT 数据微调小模型。
  - 蒸馏模型更小、更高效，但依赖于已训练好的大模型。
  - 适用于资源有限的场景，但无法达到最前沿的性能。

#### DeepSeek R1 的性能与评价
- **与 OpenAI o1 的比较**：
  - DeepSeek-R1 和 o1 在推理能力上表现相当，但 R1 推理效率更高。
  - o1 可能更多依赖推理阶段扩展，导致推理成本更高。
  - 由于缺乏 o1 的详细信息，两者对比仍存在不确定性。
- **训练成本与开源许可**：
  - DeepSeek-R1 的训练成本未明确公布，但 DeepSeek-V3 的训练成本估计为 600 万美元。
  - DeepSeek-R1 在 MIT 许可下开源，许可更宽松，便于研究和使用。

#### 低成本推理模型的探索
- **Sky-T1（低成本 SFT）**：
  - 使用仅 1.7 万条 SFT 数据，训练了一个开源 32B 模型，总花费仅 450 美元。
  - 性能与 o1 相当，展示了低成本推理模型的可能性。
- **TinyZero（低成本纯 RL）**：
  - 一个 3B 参数规模的模型，通过纯 RL 训练，计算成本不到 30 美元。
  - 模型表现出类似“自我验证”的推理能力，验证了纯 RL 在小模型中的可行性。

#### 结论与未来展望
- 推理型大语言模型在复杂任务中展现出强大的能力，但需要根据具体任务选择合适的模型和训练方法。
- DeepSeek R1 通过 SFT + RL 的方法取得了显著的推理性能，为推理模型的发展提供了重要参考。
- 低成本推理模型的探索（如 Sky-T1 和 TinyZero）为资源有限的研究者提供了新的思路。
- 未来，结合 SFT、RL 和蒸馏等多种方法，有望进一步提升推理模型的性能和效率。