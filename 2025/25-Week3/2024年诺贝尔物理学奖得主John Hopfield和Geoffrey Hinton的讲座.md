2024年诺贝尔物理学奖得主John Hopfield和Geoffrey Hinton的讲座
- 原文标题：2024 Nobel Prize lectures in physics | John Hopfield and Geoffrey Hinton - YouTube
- 链接：[https://www.youtube.com/watch?v=lPIVl5eBPh8](https://www.youtube.com/watch?v=lPIVl5eBPh8)

- **文章类别**：视频文稿

---

**内容整理**：

本文是一份关于2024年诺贝尔物理学奖得主John Hopfield和Geoffrey Hinton在颁奖典礼上所做讲座的视频文稿。文稿详细记录了典礼的开场致辞以及两位科学家的演讲内容。

**典礼开场致辞**：
- 欢迎辞：瑞典皇家科学院代表向在场的所有人表示欢迎，包括诺贝尔奖得主、嘉宾、女士们和先生们。
- 诺贝尔奖历史：提到诺贝尔奖自1901年起根据Alfred Nobel的遗嘱颁发，涵盖了物理学、化学和经济学领域。
- 当前世界形势：提及2024年是充满挑战的一年，尽管面临诸多危机，但通胀下降，新冠疫情结束，尽管能源危机和气候变化问题依然存在。
- 科学发现的重要性：强调了新发现、发明和改进对人类的重要性，以及好奇心和深刻见解在科学发现中的驱动作用。

**John Hopfield的演讲**：
- 个人背景：John Hopfield出生于1933年的芝加哥，1954年在WM more College获得学士学位，1958年在Cornell University获得博士学位，后在Princeton University和California Institute of Technology担任教授。
- 研究领域：Hopfield的工作主要集中在物理学和神经科学的交叉领域，特别是人工神经网络和机器学习的基础研究。
- 关键贡献：Hopfield描述了使用物理学基本概念的联想记忆的动态模型，利用描述磁性材料物理的方程和模型来描述大脑中的相互作用。

**Geoffrey Hinton的演讲**：
- 个人背景：Geoffrey Hinton出生于1947年的伦敦，1970年在Cambridge University获得实验心理学学士学位，1978年在University of Edinburgh获得人工智能博士学位，后在Carnegie Mellon University和University of Toronto担任教授。
- 研究领域：Hinton的研究主要集中在人工神经网络和机器学习，特别是多层神经网络的开发和学习算法。
- 关键贡献：Hinton应用统计物理学的概念开发了高效的多层神经网络，并开发了使网络自我改进的学习算法和方法。

**演讲内容总结**：
- 两位科学家的演讲都围绕着人工神经网络在机器学习中的应用，以及这些技术如何影响我们的日常生活和科学研究。
- Hopfield和Hinton的工作为机器学习和人工智能的发展奠定了基础，他们的研究成果在图像识别、语音识别和医学应用等多个领域都有广泛的应用。

**文章标签**：

#诺贝尔物理学奖 ， #人工神经网络 ， #机器学习 ， #JohnHopfield ， #GeoffreyHinton

---

好的，以下是从视频脚本中提取的框架和要点内容，以简体中文输出：

```
 演讲
    ├── 开场致辞 (0:03 - 5:10)
    │   ├── 欢迎及介绍
    │   ├── 诺贝尔奖历史和意义
    │   ├── 当前全球挑战及科学发现的重要性
    │   ├── 本年度获奖者研究领域及其潜在应用概述
    │   └── 引出物理学奖介绍
    ├── 物理学奖介绍 (5:10 - 10:42)
    │   ├── 2024 年诺贝尔物理学奖主题：人工神经网络与机器学习
    │   ├── 机器学习基本概念和人工神经网络作用解释
    │   ├── 获奖者 John Hopfield 和 Jeffrey Hinton 的贡献简述
    │   └── 引出第一位演讲者 John Hopfield
    ├── John Hopfield 演讲 (10:42 - 44:43)
    │   ├── 个人科研经历分享，强调“问题选择”的重要性
    │   ├── 霍普菲尔德模型 (Hopfield Model) 研究历程讲述
    │   │   ├── 强调偶然性、跨学科思维和问题导向的作用
    │   │   └── 从凝聚态物理到生物物理再到神经科学的研究兴趣演变
    │   ├── 霍普菲尔德网络与联想记忆、物理学中自旋系统的关系解释
    │   ├── 1982 年论文写作过程回顾，强调简洁写作和开放性讨论的重要性
    │   ├── 密集联想记忆 (Dense Associative Memory) 模型介绍
    │   ├── 展望物理学启发的人工智能网络的未来
    │   └── 总结：强调跨学科交流和物理学思维方式的重要性
    ├── Jeffrey Hinton 演讲 (45:33 - 1:16:28)
    │   ├── 霍普菲尔德网络 (Hopfield Nets) 原理和应用解释（通俗易懂的方式）
    │   ├── 使用视觉感知的例子解释如何利用隐藏神经元构建对感知输入的解释
    │   ├── 随机神经元和热平衡概念介绍，解释其在避免局部最优解中的作用
    │   ├── 玻尔兹曼机 (Boltzmann Machine) 学习目标和学习算法详细阐述
    │   │   ├── 清醒阶段
    │   │   └── 睡眠阶段
    │   ├── 玻尔兹曼机学习算法原理
    │   │   ├── 简洁性
    │   │   └── 与反向传播算法的区别
    │   ├── 玻尔兹曼机由于计算速度慢而未被广泛应用
    │   ├── 受限玻尔兹曼机 (Restricted Boltzmann Machines, RBM) 概念和对比散度 (Contrastive Divergence) 算法介绍，解释其如何提高学习速度
    │   ├── 如何堆叠 RBM 以构建深度神经网络，解释这种预训练方法如何提高学习速度和泛化能力
    │   ├── RBM 在语音识别领域的应用回顾，以及其作为“历史酶”的意义
    │   └── 重申对睡眠阶段反向学习的信心，并展望其在理解大脑学习机制中的作用
    └── 结束致谢 (1:16:28 - 1:17:53)
        └── 感谢两位获奖者的精彩演讲，共同接受观众的掌声
```


**框架：**

*   **开场致辞 (0:03 - 5:10)**
    *   欢迎参加 2024 年诺贝尔物理学、化学奖及经济学奖演讲
    *   介绍诺贝尔奖的历史和意义
    *   概述当前全球面临的挑战，强调科学发现的重要性
    *   简述本年度获奖者的研究领域及其潜在应用
    *   引出物理学奖的介绍

*   **物理学奖介绍 (5:10 - 10:42)**
    *   介绍 2024 年诺贝尔物理学奖的主题：人工神经网络与机器学习
    *   解释机器学习的基本概念和人工神经网络的作用
    *   简述两位获奖者 John Hopfield 和 Jeffrey Hinton 的贡献
    *   引出第一位演讲者 John Hopfield

*   **John Hopfield 演讲 (10:42 - 44:43)**
    *   分享个人科研经历，强调“问题选择”的重要性
    *   讲述霍普菲尔德模型 (Hopfield Model) 的研究历程，强调偶然性、跨学科思维和问题导向在科研中的作用
    *   从凝聚态物理到生物物理再到神经科学，展示其研究兴趣的演变
    *   解释霍普菲尔德网络与联想记忆、物理学中自旋系统的关系
    *   回顾 1982 年论文的写作过程，强调简洁写作和开放性讨论的重要性
    *   介绍密集联想记忆 (Dense Associative Memory) 模型，并展望物理学启发的人工智能网络的未来
    *   总结：强调跨学科交流和物理学思维方式的重要性

*   **Jeffrey Hinton 演讲 (45:33 - 1:16:28)**
    *   以通俗易懂的方式解释霍普菲尔德网络 (Hopfield Nets) 的原理和应用
    *   使用视觉感知的例子，解释如何利用隐藏神经元构建对感知输入的解释
    *   介绍随机神经元和热平衡的概念，解释其在避免局部最优解中的作用
    *   详细阐述玻尔兹曼机 (Boltzmann Machine) 的学习目标和学习算法（清醒阶段和睡眠阶段）
    *   解释玻尔兹曼机学习算法的原理，强调其简洁性和与反向传播算法的区别
    *   指出玻尔兹曼机由于计算速度慢而未被广泛应用
    *   介绍受限玻尔兹曼机 (Restricted Boltzmann Machines, RBM) 的概念和对比散度 (Contrastive Divergence) 算法，解释其如何提高学习速度
    *   描述如何堆叠 RBM 以构建深度神经网络，并解释这种预训练方法如何提高学习速度和泛化能力
    *   回顾 RBM 在语音识别领域的应用，以及其作为“历史酶”的意义
    *   重申对睡眠阶段反向学习的信心，并展望其在理解大脑学习机制中的作用

*   **结束致谢 (1:16:28 - 1:17:53)**
    *   感谢两位获奖者的精彩演讲
    *   共同接受观众的掌声

**要点内容：**

*   **2024 年诺贝尔物理学奖授予人工神经网络和机器学习领域，表彰 John Hopfield 和 Jeffrey Hinton 的奠基性贡献。**
*   **John Hopfield 的研究历程强调了问题选择、跨学科思维、以及物理学视角在科研中的重要性。他将物理学中自旋系统的概念应用于神经科学，提出了霍普菲尔德模型，为联想记忆和人工神经网络的发展奠定了基础。**
*   **Jeffrey Hinton 详细解释了霍普菲尔德网络、玻尔兹曼机和受限玻尔兹曼机的工作原理，以及如何利用它们构建深度神经网络。他强调了随机神经元、热平衡和对比散度算法在机器学习中的重要性。**
*   **Hopfield 和 Hinton 的研究工作为机器学习和人工智能的革命性发展铺平了道路，对图像识别、语音识别、自然语言处理等领域产生了深远影响。**
*   **尽管玻尔兹曼机由于计算速度的限制而未被广泛应用，但它作为一种概念模型和“历史酶”，启发了后续深度学习算法的发展。**
*   **Hinton 仍然相信睡眠阶段的反向学习在理解大脑学习机制中具有重要作用。**
*   **两位获奖者的演讲都强调了跨学科交流、好奇心驱动的研究以及开放性讨论在科学进步中的重要性。**

---

好的，以下是根据提供的框架和要点，对视频脚本进行的全文翻译整理与输出，并使用了框架中的标题。在整理过程中，我尽量保留了原文的有效信息，并使其更符合中文讲稿的表达习惯。

**开场致辞 (0:03 - 5:10)**

尊敬的诺贝尔物理学奖和化学奖的获得者们，尊敬的经济学奖获得者们，尊敬的各位院士，女士们，先生们，我代表瑞典皇家科学院，欢迎大家参加 2024 年诺贝尔物理学奖、化学奖以及纪念阿尔弗雷德·诺贝尔的斯维里克斯·里克斯班克经济学奖的演讲。

自 1901 年以来，根据阿尔弗雷德·诺贝尔的遗嘱，诺贝尔奖一直都在颁发。阿尔弗雷德·诺贝尔是一位发明家、科学家和商人，他在 1895 年将自己的财产捐赠给了五个国际奖项，这些奖项与他本人的兴趣和活动相对应。诺贝尔物理学奖授予那些造福人类的发现和发明，化学奖则授予那些带来重大改进的发现。斯维里克斯·里克斯班克经济学奖是为了纪念阿尔弗雷德·诺贝尔，于 1968 年在该银行成立三百周年之际设立的。

我们很高兴看到大家来到这里，聆听诺贝尔奖和经济学奖的演讲。2024 年是充满戏剧性的一年，许多危机同时发生。地缘政治格局变得更加不稳定，发生了几次冲突和军事行动。幸运的是，通货膨胀已经下降，新冠疫情也已结束，但能源危机仍在继续，我们还面临着日益严重的气候变化和生物多样性减少的挑战。所有这些挑战都凸显了启用新发现、新发明和新改进的重要性，这些发现、发明和改进将有益于人类，为所有人创造更好的环境和健康。多年来，我们见证了非凡的技术进步，希望这些进步将帮助人类应对这些挑战。

由于我们今天不知道明天的挑战是什么，我们需要采用广泛的、自下而上的方法来进行新的科学发现，而这些发现必须由好奇心和深刻的见解驱动。学术自由很重要，顶尖的科学发现无法被预先安排，通往发现的道路通常不是短暂而笔直的，而是漫长而曲折的，惊喜可能会为突破性的发现铺平道路。

现在，我们有幸聆听杰出的研究人员和获奖者的演讲，他们在三个不同的研究领域从根本上增加了我们的知识。今年的诺贝尔物理学奖获得者利用物理学工具，在人工神经网络方面进行了重要的工作，他们开发的方法为机器学习奠定了基础。机器学习和人工神经网络在许多不同的领域都有应用，包括工业界和学术界，它以多种方式影响着我们所有人，从图像和语音识别到医疗应用。

诺贝尔化学奖的获得者们发现了一种构建全新蛋白质的方法，蛋白质是人体中重要的组成部分。此外，获奖者还开发了一个基于人工智能和机器学习的模型，可以根据蛋白质的一级氨基酸序列预测其复杂结构，这些发现也为未来带来了巨大的潜力。

今年经济学奖的获得者们证明了社会制度对国家繁荣差异的重要性。包容性制度通常在殖民时期较贫穷的国家中引入，随着时间的推移，这些制度会带来人口的繁荣。这是曾经富裕的前殖民地现在却贫穷，反之亦然的一个重要原因。

所有这些成就不仅带来了新的知识和更深入的理解，而且使我们能够以多种方式改善我们的社会，并做出更明智的决定。现在，我们将聆听这些杰出研究人员的获奖演讲，现在和未来的科研人员可以站在他们的肩膀上，相信未来一定会涌现出新的科研巨擘。

再次热烈欢迎各位，并向获奖者们表示最热烈的祝贺！现在，我邀请我的同事，瑞典皇家科学院院士、诺贝尔物理学奖委员会主席埃尔·穆恩斯教授介绍物理学奖的获奖者。

**物理学奖介绍 (5:10 - 10:42)**

尊敬的诺贝尔奖获得者，尊敬的各位阁下，尊敬的各位同事，亲爱的同学们，女士们，先生们，热烈欢迎大家参加 2024 年诺贝尔物理学奖演讲。今年的诺贝尔物理学奖是关于人工神经网络及其在机器学习革命性发展中的核心作用。机器学习是计算机从实例中学习，而不是遵循编程指令或预设规则的能力。人工神经网络是机器学习的基础构建模块，通过向神经网络输入大量数据，无论是图像、语音还是音乐，它都可以被训练来识别模式。一旦它学会了识别这些模式，它就可以生成类似于给定数据集的新数据。例如，给它数百万张猫的图片，它将学会猫的样子，以及它与狗的区别，而无需遵循逐渐缩小的描述哺乳动物分类学以及它们发出的喵喵叫和呼噜声的编程子程序。

我不需要说服你们，今天这项新技术已经极大地改变了我们的日常生活。在粒子物理学、蛋白质晶体学、材料科学、天体物理学等不同的研究领域，人工神经网络都为具有重大意义的发现和发明做出了贡献。

但这一切是如何开始的呢？人工神经网络是如何构建的？它们又是如何工作的呢？早在 20 世纪 80 年代初，今年的诺贝尔奖获得者约翰·霍普菲尔德和杰弗里·辛顿就做出了奠基性的发现和创新，使得利用人工神经网络进行机器学习成为可能。

他们对我们大脑的工作方式有着无限的好奇心，我们的大脑由数百亿个神经元通过无数的连接连接在一起。他们寻找着这些问题的答案：我们人类如何学习？我们如何处理和分类信息？我们如何通过与存储的记忆和经验的关联来识别新信息？

约翰·霍普菲尔德做出了开创性的贡献，他使用物理学中的基本概念描述了联想记忆的动态模型。他使用了描述集体现象的物理学，他使用与控制磁性材料物理学相同的方程和模型来描述大脑中的相互作用。杰弗里·辛顿应用统计物理学的概念开发了具有多层结构的高效神经网络，他开发了神经网络的学习算法和自我改进方法。

现在，我很荣幸地介绍我们的第一位演讲者约翰·霍普菲尔德。约翰·霍普菲尔德 1933 年出生于芝加哥，1954 年在斯沃斯莫尔学院获得学士学位，1958 年在康奈尔大学获得博士学位。1964 年，他被任命为普林斯顿大学物理学教授，1980 年，他成为加州理工学院化学和生物学教授。随后，他回到普林斯顿大学，目前担任分子生物学的荣誉退休教授。请大家和我一起欢迎约翰·霍普菲尔德上台，为我们讲述那些促成今年诺贝尔物理学奖的发展历程。

**John Hopfield 演讲 (10:42 - 44:43)**

谢谢，谢谢，谢谢大家的光临。

我的第一份全职工作是在贝尔电话实验室，晶体管是在那里发明的，那是十年前的事了。我在 1958 年 3 月加入了它的六人理论物理小组。入职半天的时间都花在了行政细节上。我去了我的新办公室，拆开几本书和期刊花了一个小时。去五楼的库房拿了一些笔记本、铅笔和一个手摇铅笔刀。我回到办公室，削了几支铅笔。现在我该做什么？这个严峻的问题在我的研究和教学生涯中多次困扰着我。每个科学家，每个学者，都面临着这个令人烦恼的问题：每天该做什么？大多数人的回应是，今天的工作只是对昨天思路、测量或情节发展的一点点延伸。我们大多数人很少花时间认真思考选择我们接下来要追求的研究领域。你很少听到关于如何选择研究方向的讲座，然而，问题的选择是一个人能在科学上取得什么成就的主要决定因素。

我 40 多年前写的两篇论文构成了通常被称为霍普菲尔德模型的工作基础。我将首先描述这个课题是如何发展的，做出的一些选择，遇到的情况，最终定义了一个有用观点的萌芽，然后可以在其上构建数学。为了方便起见，我将这个过程称为“问题选择”，但它实际上描述了一个有用的观点是如何从对我过去随机事件的分析中产生的。

当以历史的眼光来看待时，一个问题的发展通常被呈现为分支决策树中一系列正确选择的逻辑序列。在没有事后诸葛亮的情况下，研究通常是在没有压倒性洞察力的情况下，沿着随机的方向进行的。对我来说，在父亲和母亲都是物理学家的环境下长大，物理学并不是由主题来定义的，无论是原子、对流层、一块玻璃、我的自行车还是磁铁。顺便说一句，这些都是主题。物理学是一种观点，即我们周围的世界，通过努力、智慧和足够的资源，是可以以一种可预测的和合理的定量方式来理解的。成为一名物理学家就是致力于追求这种理解。当然，我的父母从未谈论过是什么定义了物理学，直到很久以后，我才思考我的成长经历是如何定义了一种年轻的世界观。

我从小就喜欢拆东西，看看它们是如何工作的，修理自行车，探索化学，制作可飞行的模型飞机、矿石收音机和简单的无线电设备，玩电池，学会用我的双手思考和操作真实的物体。我理解的每一件事，比如为什么我的自行车刹车在使用时会变热，如何将一个连贯的故事作为一个更宏大理解的一小部分结合起来，这些都与一个复杂的系统相关。

我没有考虑过成为一名物理学家的经济后果，只是简单地考虑过将其作为一种可能的职业。我的高中化学老师非常出色，而我的物理老师甚至对电和磁的基本原理都缺乏了解。因此，在填写大学申请表的潜在专业时，我写了物理或化学。在斯沃斯莫尔学院，我被分配到威廉·埃尔莫尔教授担任我的导师。我们在他的办公室见面讨论我的未来，他找到了描述我高中记录的学生卡，包括我感兴趣的研究领域。他拿出一支笔，划掉了两个词，说：“我不认为我们需要考虑化学。”这就是我接受教育的方式，比尔·埃尔莫尔的一个武断行为对我今天能站在这里有很大关系。

我开始关注物理学研究生院，着眼于我可以专攻的方向。我的成长经历尤其让我对日常世界的物理学及其技术产生了兴趣。我去了康奈尔大学读研究生，因为在 1954 年，康奈尔大学物理系有一个分支对一个叫做固态物理的领域感兴趣。在康奈尔大学的第二年年中，我找到理论物理学家阿尔伯特·W·奥弗豪泽，问他是否愿意指导我的论文，并帮助我找到一个论文题目。通过课程和做题，我迅速掌握了理论物理学的工具，但我不知道如何找到一个合适的研究问题。

幸运的是，奥弗豪泽有一系列有趣的谜题。它们通常以悖论的形式出现，对特定现象的初步理论分析得出的结果与实验结果完全不同。在奥弗豪泽的这份清单中，大多数悖论，他自己也不知道是什么导致了常识理论观点与实验事实之间的冲突。我选择了一个与晶体中激子的辐射寿命有关的问题。奥弗豪泽是一位非常支持我的导师，他既是一位倾听者，也是一位批评者。当我去找他时，寻找方向和解决技术理论问题完全是我自己的问题。他给我的最大礼物是一个有趣问题的“所有权”，以及对研究和进展的完全责任。我尽我所能，以类似的方式培养另一代独立的学生，其中有几位今天实际上也在这里。

获得博士学位后，我在贝尔实验室的理论小组找到了一份工作。康耶斯·赫林是这个小组的负责人，他看到我正在努力确定我的下一个研究问题。他建议我去拜访实验学家，以了解有趣的谜题在哪里。正是在这样的框架下，我遇到了化学家大卫·托马斯，我们形成了一个理论与实验相结合的工作联盟。这为我提供了意想不到的问题和悖论，持续了十年，并为 AT&T 提供了化合物半导体的主要知识库。这使得大卫和我共同获得了 1969 年美国物理学会颁发的奥利弗·巴克利固态物理奖。当时，没有人会猜到光和化合物半导体的结合将在未来发挥重要的技术作用。

到了 1968 年，在凝聚态物理领域，我已经没有适合我特定才能的问题了。我去了英国剑桥大学的卡文迪许实验室，获得了半年的古根海姆奖学金，希望能找到新的有趣途径，但没有发现什么适合我的。从剑桥回到普林斯顿大学，并在贝尔实验室的半导体小组担任顾问期间，我遇到了罗伯特·舒尔曼，一位正在对血红蛋白进行首次高分辨率核磁实验的化学家。鲍勃告诉我，四个铁原子在广泛分离的蛋白质血红素基团中心的协同氧结合。大量的物理技术被用来研究这种分子：核磁共振、电子顺磁共振、光谱学、中子散射、穆斯堡尔谱学，所有固态物理学的巧妙实验技术似乎都与理解血红蛋白相关。血红蛋白为我提供了一个从凝聚态物理到生物物理的简单入门。在这两个领域，目标都是了解结构和低激发态是如何与可测量和相关的性质联系起来的。看来我在凝聚态物理方面的背景可以在生物学中找到很好的应用。

当两次诺贝尔化学奖和和平奖获得者莱纳斯·鲍林被问及选择什么问题来研究时，他回答说：“我会问自己，我正在考虑的问题是否是我可能做出贡献的问题。” 我显然需要效仿鲍林。我对蛋白质合成过程中选择正确氨基酸时发生的意外少的错误数量产生了兴趣。在一个细胞内，当下一个氨基酸被插入蛋白质时，如何利用我的物理学背景更有可能找到这个问题的解决方案呢？

自由能是关键，像葡萄糖或 ATP 这样的高能分子对细胞来说制造成本很高，在一个简单的过程中使用大量高能分子的生化过程必须为此付出高昂的代价。假设目的是为了校对，去除生物合成错误，也许实验生物化学家没有注意到，因为他们不知道如何寻找。我在这个问题上取得了成功，我找到了一个基于生化细节的蛋白质合成校对方案，并描述了验证该理论的关键实验。1976 年，我在哈佛医学院做了一个演讲，最后简要介绍了校对以及我对蛋白质合成中非化学计量的预测。我演讲后的第一个问题来自罗伯特·C·汤普森，他问：“你想听听这种实验测量的结果吗？” 然后他描述了他尚未发表的实验，在我的测量中，当一个系统被推动进行校对时，以及当它没有被推动进行校对时，他描述了实验事实，这些事实有力地推动了这个想法：校对就是这样工作的。这是我短暂的化学研究生涯中最令人愉快、最意想不到的惊喜之一。

这篇 1974 年的论文对我的生物问题研究方法很重要，因为它让我思考反应网络结构的功能，而不仅仅是生物分子的功能是什么。六年后，我正在推广这种观点，并思考神经元网络，而不是单个神经细胞的特性。撰写这篇论文，进行这项研究，也让我更多地思考生物物理研究中的问题识别。这篇论文出乎意料的成功，是因为它提出了一个新的生物学问题，与我对生物化学或生物物理学的知识无关。机遇眷顾了没有准备但有探索精神的人。

1977 年冬天，我在哥本哈根的玻尔研究所度过，这是该研究所零星但具有历史意义的生物学推广活动的一部分。我的任务是为物理学家举办一系列广泛的现代生物学研讨会。1932 年，尼尔斯·玻尔以题为“光与生命”的演讲开启了国际光疗大会，他提出，对生命本身的深入解释和如何解释量子力学的哲学歧义是否本质上是交织在一起的。然而，在 1977 年，我的杰出生物学专家们强调了他们所理解的内容，他们当然没有将生物科学描述为需要物理学家的帮助。

我回到普林斯顿，没有为自己找到新的问题。我现在正在寻找一个大问题，其解决方案和理解将具有远远超出其正常学科范围的意义，并将重组它们所来自的领域。

我大胆地走向失败。我的普林斯顿大学办公室电话在我上午上课前不久响了，我以为是一个关于重要网球比赛的电话，于是接了电话，结果发现是弗朗西斯·施密特打来的。他说他在麻省理工学院负责神经科学研究项目，下周三将途经普林斯顿，他非常希望能占用我半小时的时间，不能再多了。我从未听说过施密特或 NRP，本来应该拒绝的，但我想到，半小时而已，就同意下周见面。

弗朗西斯·施密特向我介绍了他的神经科学研究项目，该项目主要在波士顿举行小型会议，由 20 名常规成员和 20 名访问者参加。我告诉他我对神经科学一无所知，他说没关系，只要谈谈你感兴趣的内容就行。于是我谈了我的动力学校对理论以及生物合成中准确性的一般问题。听众包括神经学家、免疫学家、电生理学家，他们对我所说的内容知之甚少，但这并不重要。弗兰克希望增加一名物理学家成员，希望引进一位具有不同科学经验的人与他的学科进行互动，并可能帮助它变得更加综合，成为一门更具预测性的科学。

会议上的演讲深深吸引了我，对我来说，思维如何从大脑中产生是我们人类提出的最深刻的问题。NRP 的科学家们以各种各样的挑战和极大的热情，将这个问题分解成一个个小问题来研究。但在我看来，这群科学家永远不可能解决这个大问题，因为解决方案只能用一种合适的数学语言和结构来表达，而当时参与 NRP 的人中，没有人能在这个领域游刃有余。所以我加入了这个小组，希望能在这个领域定义、构建或发现一些我可以做的有用的事情。

我对神经生物学的基础教育来自于参加 NRP 的半年一次的会议，坐在该领域的世界级专家旁边，他们耐心地向我解释演讲者实际上试图谈论的内容。我给自己的任务是找到一种综合的物理学观点，试图超越神经解剖学、电生理学、大鼠学习和阿尔茨海默病讲座中互不相关的细节，并找到某种综合的理论结构。

1979 年，加州理工学院希望我担任化学和生物学的联合教授职位，我离开了普林斯顿。加州理工学院的量子计算设施是一个尝试模型的绝佳环境，它支持多用户实时计算、视频显示、直接键盘输入，所有这些在今天看来都是微不足道的事情，但在 1979 年，它们是一件了不起的事情。

我的见解是，基本的机器或大脑操作是通过遵循状态空间中的机器轨迹来计算的。从你放置程序和数据开始，然后系统的状态随着每一个时钟滴答而移动，最终，当状态停止变化时，计算就完成了。你读出一些寄存器，计算实际上是通过遵循从开始到结束的动态轨迹来完成的。该轨迹需要对扰动具有稳定性，以便在存在噪声和系统缺陷的情况下达到正确的答案。

这幅批处理模式数字计算的图片只是一个示意图，试图让你对霍普菲尔德所认为的计算有一个印象。我最后转向一个更具体的选择问题，而不是一个一般性的问题，并试图专注于动物需要或已知拥有的东西，这变成了与行为密切相关的东西。心理学家可能会称之为联想记忆，工程师可能会称之为内容可寻址存储器，或者另一位心理学家会将其描述为陈述性记忆，你可以描述的东西。这些名字都有一个共同的特征，它们共享一个共同的基本范式，可以描述为通过一个遵循稳定轨迹的动态系统进行计算，那些偏离该轨迹的小扰动会被挤压回原处，这就是你在一个嘈杂的系统中从开始到结束的方式。这就是内容可寻址存储器在一个非常不同的机制下所做的事情，但所有这些与“关联”相关的事情——我无法避免使用这个术语——我当然在说话时正在运用联想记忆，但所有这些事情都需要数学家所说的收敛流。

一旦我理解了这一点，我就知道我已经听够了有关自旋系统数学的知识，因为它们与磁性和复杂现象以及磁性材料有关。我在贝尔实验室呆了足够长的时间，以至于当人们认真讨论这些主题时，我都在场，所以我对自旋系统和轨迹之间的联系有所了解，我只需要说，让我们把生物系统变成一个自旋系统，然后它们应该具有相同的数学。联想记忆是相互的，看到某人会让你想起他们的名字，听到他们的名字会让你想起他们的样子。这一事实可以在网络结构中通过建立相互的连接来表达。这种网络的数学与自旋系统和固体的数学密切相关，也与它们的相变和磁性有关。突然之间，神经生物学和固态物理之间建立了一种联系。

实现收敛流的一种方法是将一个状态与一个能量相关联，并允许状态到状态到状态的转换，只有当能量像一个球在崎岖的地形上滚下山坡一样减少时才允许进行这种转换。你想了解球的轨迹，你可以详细地跟踪地形和球，或者你可以理解这是一个能量，整个过程只是下坡，你从靠近山顶的地方到靠近山底的地方，能量函数是你真正需要理解的，在广泛的系统中理解收敛流。

有了这种联系，三个月后，我就在写一篇论文了。当我看到这两个科学领域实际上可以用同一套数学来描述时，一切都融会贯通了。现在的问题变成了写一篇论文，我有三个目标读者群体：物理学家、计算机科学家和神经生物学家，这是一群截然不同的人。但作为美国国家科学院的院士，我可以在《美国国家科学院院刊》(PNAS) 上发表一篇未经审稿的论文。但对于 PNAS 来说，文章长度绝对限制在五页以内，包括参考文献，而且要针对三个不同的读者群体，有很多话要说。我必须像海明威写非虚构作品那样写科学。

事后看来，几乎显而易见的遗漏反而增加了这篇论文的影响力。未说明的内容变成了对他人的邀请，让他们为这个主题添砖加瓦，从而鼓励和扩大了贡献者群体来研究这种网络模型。这篇 1982 年的论文是我第一次使用“神经元”一词的出版物，它为许多物理学家和计算机科学家提供了一个进入神经科学领域的入口。进一步的研究将这些网络与许多重要的应用联系起来，远远超出了联想记忆，包括新兴的人工神经网络领域。它还吸引了许多统计物理学家进入该领域，并为玻尔兹曼机、连续联想记忆网络的后续发展奠定了基础。连续联想记忆网络也被用作许多当代人工智能系统和复杂计算神经科学中常用的连续循环网络的灵感来源。

现在被称为霍普菲尔德模型的联想记忆网络的一个问题是它的信息存储容量很小。生物网络可以存储比感觉神经元数量多得多的记忆，并且如果环境发生变化，可以动态地调整这些记忆。2015 年，当我在普林斯顿高等研究院担任访问教授时，我开始与德米特里·克罗托夫互动，他是一位前高能理论家，正在寻找一个有趣的问题来研究。经过几个月的讨论，我们提出了一个简单的数学模型，其中记忆容量与空间的维度脱钩。这个新模型允许更密集地存储记忆，我们称之为密集联想记忆。

展望未来，我们希望通过物理系统进行有用计算的想法，不仅在回顾性解释现有 AI 架构中的计算算法时变得有帮助，而且还可能激发新一代受集体物理学启发的人工智能网络。

我在科学上所做的一切都建立在专家的实验和理论研究之上，我对愿意与学科外人士互动的专家们表示敬意，他们欢迎物理学进入新的领域。物理学在其最佳状态下，是一种理解人类及其宇宙整体的观点。谢谢！

**Jeffrey Hinton 演讲 (45:33 - 1:16:28)**

今天我要做一件非常愚蠢的事情，我将尝试向普通听众描述一个复杂的技术概念，而不使用任何方程式。首先，我必须解释霍普菲尔德网络，我将解释具有状态为 1 或 0 的二元神经元的版本。所以在右边，你会看到一个小的霍普菲尔德网络，最重要的是神经元之间具有对称加权的连接。整个网络的全局状态被称为一个“构型”，只是为了让我们听起来有点像物理学。每个构型都有一个“优度”，一个构型的优度就是所有同时处于开启状态的神经元对之间权重的总和。所以那些用红色框起来的权重，你把它们加起来，希望你得到的是 4，这就是该网络构型的优度，而能量就是优度的负值。因此，这些网络将稳定到能量最小值。

霍普菲尔德网络的关键在于，每个神经元都可以局部地计算它需要做什么才能降低能量，其中能量是“坏”的程度。如果来自其他活动神经元的总加权输入为正，则该神经元应该开启；如果来自其他活动神经元的总加权输入为负，则该神经元应该关闭。如果每个神经元都持续使用这个规则，我们随机选择它们并持续应用这个规则，我们最终将稳定到一个能量最小值。

所以右边的构型实际上是一个能量最小值，它的能量为 -4。如果你选择那里的任何一个神经元，那些处于开启状态的神经元想要保持开启，它们获得了总的正输入；那些处于关闭状态的神经元想要保持关闭，它们获得了总的负输入。但这不是唯一的能量最小值，一个霍普菲尔德网络可以有许多能量最小值，它最终会到达哪里取决于你从哪里开始，也取决于你做出的随机决定的顺序，关于更新哪个神经元的随机顺序。所以这是一个更好的能量最小值，现在我们打开了右边的三角形单元，它的优度是 3 + 3 - 1 = 5，所以能量是 -5，这是一个更好的最小值。

现在，霍普菲尔德提出使用这种网络的一个好方法是让能量最小值对应于记忆，然后使用关于是否应该打开神经元的二元决策规则，这可以清理不完整的记忆。所以你从一个部分记忆开始，然后你持续应用这个决策规则，它就会把它清理干净。所以当能量最小值代表记忆时，稳定到能量最小值是一种拥有内容可寻址存储器的方式，你可以通过仅仅打开记忆中的一部分内容来访问它，然后使用这个规则，它就会把它填满。

特里·塞诺斯基和我，特里是霍普菲尔德的学生，我们提出了对这类网络的不同用途。我们可以用它们来构建对感知输入的解释，而不是用它们来存储记忆。这个想法是，你有一个网络，它既有可见神经元也有隐藏神经元，可见神经元是你向它展示感知输入的地方，也许是一个二值图像，隐藏神经元是它构建对该感知输入解释的地方，网络的构型能量代表了解释的“坏”的程度，所以我们想要低能量的解释。

我将举一个具体的例子，考虑一下顶部那个模棱两可的线条图，人们有两种看待它的方式。第一种解释通常是你首先看到的，还有另一种解释，当你把它看作一个凸起的物体时，这显然是对同一个二维线条图的不同三维解释。那么，我们能不能让这些网络中的一个对同一个线条图产生两种不同的解释呢？

好吧，我们需要首先思考图像中的一条线告诉了我们关于三维边缘的什么信息。所以那条绿线是图像平面，想象你正透过窗户看出去，你在窗户上画出了世界上场景中的边缘。所以那条黑色的小线是图像中的一条线，那两条红线是从你的眼睛通过那条线的两端射出的视线。如果你问，世界上哪个边缘可能导致了这条线，有很多边缘可能导致了它。有一个边缘可能导致了那条二维线，但还有另一个，还有另一个，还有另一个，所有这些边缘都会导致图像中的同一条线。所以视觉的问题是从图像中的单条线倒推，找出这些边缘中哪一个是真正存在的。如果物体是不透明的，你一次只能看到其中一个，因为它们会互相遮挡。所以你知道图像中的那条线必须描绘这些边缘中的一个，但你不知道是哪一个。

我们可以构建一个网络，首先将线条转化为线神经元的激活。假设我们已经有了，我们有大量的神经元来表示图像中的线条，我们只打开其中的几个来表示这幅特定图像中的线条。现在，这些线条中的每一条都可以描绘出许多不同的三维边缘，所以我们要做的是将那个线神经元与一大堆三维边缘神经元连接起来，用兴奋性连接，那些是绿色的连接。但我们知道我们一次只能看到其中一个，所以我们让那些边缘神经元相互抑制。现在我们已经捕捉到了很多关于感知的光学信息，我们对所有的线神经元都这样做。现在的问题是，我们应该打开哪些边缘神经元？为此，我们需要更多信息。

我们在解释图像时使用某些原则，如果你在图像中看到两条线，你假设如果它们在图像中相交，它们就在深度上相交，也就是说，它们在两条线在图像中相交的相同深度处。我们可以为此添加额外的连接，我们可以在每一对在深度上相交的三维边缘神经元之间建立连接，在它们具有相同端点的点上。如果它们以直角相交，我们可以建立更强的连接，我们真的很喜欢看到事物以直角相交的图像。所以我们添加了一大堆这样的连接，现在我们希望的是，如果我们正确设置了连接强度，我们就得到了一个有两种可选状态的网络，可以稳定到对应于尼克尔立方体的两种可选解释。

这产生了两个主要问题。第一个问题是，如果我们要使用隐藏神经元来解释在可见神经元状态中表示的图像，那就是搜索问题：我们如何避免陷入局部最优？我们可能会稳定到一个相当差的解释，而无法跳出它到一个更好的解释。第二个问题是学习，我有点暗示我是手工添加所有这些连接的，但我们希望神经网络添加所有这些连接。

第一个问题，搜索问题，我们或多或少通过使神经元具有噪声来解决。如果你有像标准霍普菲尔德网络那样的确定性神经元，如果系统稳定到一个能量最小值，比如那里的球是整个系统的状态，也就是整个系统的构型，它就无法从 A 到 B，因为神经元的决策规则只允许事物在能量上下降，右边的图是决策规则，如果输入是正的就打开，如果输入是负的就关闭。我们希望能够从 A 到 B，但这意味着我们必须在能量上上升。

解决这个问题的方法是使用有噪声的神经元，随机二元神经元。它们仍然只有二元状态，它们的状态要么是 1 要么是 0，但它们是概率性的。如果它们获得一个大的正输入，它们几乎总是会打开；如果获得一个大的负输入，它们几乎总是会关闭。但如果输入是“软”的，如果它在 0 附近，那么它们的行为是概率性的。如果它是正的，它们通常会打开，但偶尔会关闭；如果它是一个小的负输入，它们通常会关闭，但偶尔会打开。但它们没有实数值，它们总是二元的，但它们只是做出这些概率性的决定。

所以现在，如果我们想用这些隐藏神经元来解释一个二值图像，我们所做的就是将二值图像钳制在可见单元上，这指定了输入是什么。然后我们随机选择一个隐藏神经元，我们查看它从其他活动隐藏神经元获得的总输入，我们把它们都设置为随机状态。如果它获得总的正输入，我们可能会打开它，但如果它只是一个小的正输入，我们可能就把它关闭了。所以我们持续执行这个规则：如果它们获得大的正输入就打开它们，如果它们获得大的负输入就关闭它们，但如果它们是“软”的，就做出概率性的决定。如果我们持续选择隐藏神经元并这样做，系统最终将接近所谓的“热平衡”。这对非物理学家来说是一个困难的概念，我稍后会解释。

一旦它达到热平衡，隐藏神经元的状态就是对该输入的一种解释。所以在那个线条图的情况下，你希望每个线单元都有一个隐藏神经元被打开，你会得到一个解释，这将是尼克尔立方体的两种解释之一。我们希望的是，低能量的解释将是对数据的良好解释。

所以对于这个线条图，如果我们能学习 2D 线神经元和 3D 边缘神经元之间的正确权重，并学习 3D 边缘神经元之间的正确权重，那么希望网络的低能量状态将对应于良好的解释，即看到 3D 矩形物体。

热平衡，它不是你首先期望的那样，即系统稳定到一个稳定的状态。稳定的不是系统的状态，稳定的是一个更抽象的东西，很难想象，是系统构型的概率分布。这对普通人来说很难想象，它稳定到一个叫做玻尔兹曼分布的特定分布。在玻尔兹曼分布中，一旦它稳定到热平衡，发现系统处于特定构型的概率仅由该构型的能量决定，你更有可能发现它处于低能量构型。所以热平衡时，好的状态，低能量状态，比坏的状态更有可能。

为了理解热平衡，物理学家有一个技巧，它允许普通人理解这个概念，希望如此。你只需想象一个非常大的、由相同的网络组成的“系综”，数以亿计的相同的网络，你有数以亿计的霍普菲尔德网络，它们都具有完全相同的权重，所以它们本质上是相同的系统，但你把它们都设置为不同的随机状态，它们都做出自己独立的随机决定。将有一定比例的系统具有每种构型，一开始，这个比例将只取决于你如何设置它们的初始状态。也许你把它们设置为随机状态，所以所有构型都是等可能的，在这个庞大的系综中，你将在每种可能的构型中获得相等数量的系统。但然后你开始运行这个算法，以这样一种方式更新神经元，使它们倾向于降低能量，但偶尔也会上升。逐渐地，任何一种构型中系统的比例将稳定下来。所以一个系统可能会离开一个构型，但其他系统将进入该构型，这被称为“细致平衡”，系统比例将保持稳定。物理学部分到此结束。

让我们想象一下生成一幅图像，现在不是解释一幅图像，而是生成一幅图像。要生成一幅图像，你首先为所有神经元、隐藏神经元和可见神经元选择随机状态，然后你选择一个隐藏或可见神经元，你使用通常的随机规则更新它的状态，如果它有很多正输入，可能就打开它；如果有很多负输入，可能就关闭它；如果它是“软”的，它的行为就有点随机性。你持续这样做，如果你反复这样做，直到系统接近热平衡，然后你查看可见单元的状态，这就是这个网络根据它所信任的分布，也就是玻尔兹曼分布，生成的图像，在这个分布中，低能量构型比高能量构型更有可能。但它信任许多可能的替代图像，你可以通过运行这个过程来选择其中一个它所信任的图像。

那么，在玻尔兹曼机中学习的目的是什么呢？在玻尔兹曼机中学习的目的是使网络在生成图像时，把它想象成做梦，它只是随机地想象事物，当它生成图像时，这些图像看起来像它在对真实图像进行感知时所感知的图像。如果我们能做到这一点，那么隐藏神经元的状态实际上将成为解释真实图像的好方法，它们将捕捉到图像的潜在原因，至少我们希望如此。换句话说，学习网络中的权重等同于弄清楚如何使用那些隐藏神经元，以便网络将生成看起来像真实图像的图像。这似乎是一个极其困难的问题，每个人都认为这将非常复杂。结果是，特里和我有一种非常乐观的方法。

问题是，你能否从一个具有大量隐藏神经元的这种随机霍普菲尔德网络的神经网络开始，它们之间有随机的权重，它们与可见神经元之间也有随机的权重，所以它是一个大的随机神经网络。然后你只需向它展示大量的图像，我们希望发生一些看起来很荒谬的事情，即在感知到大量真实图像后，它将创建隐藏单元之间以及隐藏单元与可见单元之间的所有连接，它将正确地加权这些连接，以便它能根据诸如以直角相交的三维边缘等原因对图像进行合理的解释。这听起来非常乐观，你可能认为实现这一点的学习算法会非常复杂。

关于玻尔兹曼机，令人惊奇的是，有一个非常简单的学习算法可以做到这一点。这是特里·塞诺斯基和我在 1983 年发现的。学习算法是这样的，它有两个阶段：有一个清醒阶段，那是网络被呈现图像的阶段，你将一幅图像钳制在可见单元上，你让隐藏单元四处“震荡”并稳定到热平衡。然后，一旦隐藏单元与可见神经元达到热平衡，对于每一对连接的神经元，无论是两个隐藏神经元还是一对可见和隐藏神经元，如果它们都处于开启状态，你就在它们之间的权重上增加一个小的量。这是一个非常简单的学习规则，这是一个相信唐纳德·赫布的人会喜欢的学习规则。然后是一个睡眠阶段，显然，如果你只运行清醒阶段，权重只会变得越来越大，很快它们就会全部变成正的，所有的神经元都会一直处于开启状态，那就没什么用了。你需要与睡眠阶段结合起来，在睡眠阶段，你可以把网络想象成在做梦，你通过更新所有神经元的状态（隐藏的和可见的）来稳定到热平衡。一旦你做到了并达到热平衡，对于每一对连接的神经元，如果它们都处于开启状态，你就在它们之间的权重上减去一个小的量。这是一个非常简单的学习算法，它能做正确的事情，这非常令人惊讶。

平均而言，该学习算法会改变权重，从而增加网络在做梦时生成的图像看起来像它在感知时看到的图像的概率。而不是针对普通听众，所以你们不要读接下来的两行。对于统计学家和机器学习人员来说，该算法在期望中所做的是，这意味着它做得很嘈杂，经常做错事，但平均而言，在期望中，它遵循对数似然的梯度。也就是说，在期望中，它所做的是使网络在做梦时更有可能生成它在清醒时看到的图像类型。或者换句话说，权重的变化使得网络认为合理的、低能量的图像类似于它在清醒时看到的图像。当然，学习过程中发生的事情是，在清醒时，你正在降低网络在看到真实数据时所达到的整个构型的能量，而在睡眠时，你正在提高那些构型的能量。所以你想让它做的是，相信你在清醒时看到的东西，而不相信你在睡觉时梦到的东西。

所以，如果你问稳定到热平衡的过程实现了什么，它实现了一些了不起的事情，就是一个权重需要知道的关于所有其他权重的一切，以及知道如何改变一个权重，你需要知道一些关于所有其他权重的信息，它们相互作用，但你需要知道的一切都出现在两个相关性之间的差异中。它出现在网络观察数据时两个神经元同时处于开启状态的频率，与网络没有观察数据时，也就是做梦时，它们同时处于开启状态的频率之间的差异中。不知何故，在这两种情况下测量的那些相关性告诉了一个权重它需要知道的关于所有其他权重的一切。

这令人惊讶的原因是，在像反向传播这样的算法中，这是现在所有神经网络实际使用的算法，你需要一个反向过程来传递关于其他权重的信息，而那个反向过程的行为与前向过程非常不同。在前向过程中，你正在将神经元的活动传递给后面几层的神经元，在反向过程中，你正在传递敏感性，你正在传递一种完全不同的量。这使得反向传播作为大脑工作原理的理论相当不可信。所以当特里提出这个理论，这个玻尔兹曼机的学习过程时，我们完全相信这一定就是大脑的工作方式，我们决定我们要获得诺贝尔生理学或医学奖。我们从未想过，如果这不是大脑的工作方式，我们可以获得诺贝尔物理学奖。

只有一个问题，问题是稳定到热平衡对于具有大权重的非常大的网络来说是一个非常缓慢的过程。如果权重非常小，你可以很快地完成，但是当权重很大时，在它学习了一些东西之后，它就非常慢了。所以实际上，玻尔兹曼机是一个美妙而浪漫的想法，它们有这个非常简单的学习算法，它正在做一些非常复杂的事情，它正在构建这些解释数据的隐藏单元的整个网络，通过使用一个非常简单的算法，唯一的问题是它们太慢了。所以玻尔兹曼机就到此为止了。

这篇演讲本应该在那里结束，但 17 年后，我意识到如果你对玻尔兹曼机进行很多限制，只使用彼此之间没有连接的隐藏单元，那么你可以得到一个快得多的学习算法。如果没有隐藏神经元之间的连接，那么清醒阶段就变得非常简单。你所做的就是将输入钳制在可见单元上以表示一幅图像，然后现在你可以并行地更新所有隐藏神经元，你就已经达到了热平衡。你只需更新它们一次，它们只需查看可见输入，并根据它们获得的输入量随机选择它们的两个状态之一，现在你一步就达到了热平衡，这对隐藏神经元来说很好。

在睡眠阶段你仍然有一个问题，你必须将网络置于某种随机状态，更新隐藏神经元，更新可见神经元，更新隐藏神经元，更新可见神经元，你必须持续很长时间才能达到热平衡。所以这个算法仍然是没有希望的，但事实证明有一个捷径。这个捷径并没有完全做正确的事情，这很尴尬，但它在实践中效果很好。这个捷径是这样的：你把数据放在可见单元上，那是一幅图像，然后你并行地更新所有隐藏神经元，它们现在已经达到了与数据的热平衡。现在你更新所有可见单元，你会得到我们所说的“重构”，它将类似于数据，但不完全相同。现在你再次更新所有隐藏单元，然后你就停止了，就这样，你完成了。

你进行学习的方式是，当你向它展示数据并且它已经达到了与数据的平衡时，你测量神经元 i 和 j，可见神经元 i 和隐藏神经元 j 同时处于开启状态的频率；当你向它展示重构并且它已经达到了与重构的平衡时，你测量它们同时处于开启状态的频率。这个差异就是你的学习算法，你只需根据这个差异成比例地改变权重，这实际上效果很好，而且快得多，快到足以使玻尔兹曼机最终变得实用。

所以，Netflix 实际上使用了受限玻尔兹曼机，与其他方法相结合，根据各种与你有点像的其他用户的偏好，来决定向你推荐哪些新电影。他们实际上赢得了比赛，这种玻尔兹曼机和其他方法的组合赢得了 Netflix 比赛，关于你能多好地预测用户会喜欢什么。但是，当然，如果隐藏神经元之间没有连接，你就无法构建特征检测器层，而这正是你在图像中识别物体或在语音中识别单词所需要的。

只有一个隐藏单元层，并且它们之间没有连接，这看起来是一个很强的限制，但实际上你可以绕过它。你可以做的是堆叠这些受限玻尔兹曼机。你所做的是获取你的数据，你向受限玻尔兹曼机 RBM 展示数据，它只有一个隐藏层，并使用这种对比散度算法，就是上下再向上，你学习一些权重，以便隐藏单元捕捉数据中的结构。隐藏单元变成了捕捉数据中常见相关事物的特征检测器。然后你获取那些隐藏的活动模式，隐藏单元中的二元活动模式，你把它们当作数据。你只需将它们复制到另一个 RBM 中，它们就是另一个 RBM 的数据。第二个 RBM 查看这些已经捕捉到数据中相关性的特征，它捕捉这些特征之间的相关性，你可以像这样一直进行下去。所以你正在捕捉越来越复杂的相关性，所以你可以学习第二组权重 W2，你可以做任意多次。让我们学习第三组权重，所以现在我们有一堆独立的玻尔兹曼机，每一个都在寻找前一个玻尔兹曼机的隐藏单元之间的结构。

然后你可以做的是，你可以堆叠这些玻尔兹曼机，并把它们当作一个前馈网络。忽略连接是对称的事实，只使用一个方向的连接。现在因为你有一种方法进入你的第一个隐藏层，你已经提取了捕捉原始数据中相关性的特征；然后在你的第二个隐藏层中，你已经提取了捕捉第一个隐藏层中提取的特征之间相关性的特征，以此类推。所以你正在获得越来越抽象的特征，相关性之间的相关性。一旦你像那样堆叠它们，然后你就可以添加一个最终的隐藏层，像这样，你可以进行监督学习。也就是说，现在你可以开始告诉它关于事物名称的信息，比如猫和狗，那些是类别标签，你将不得不学习到那些类别标签的权重。但你从这个网络开始，你通过学习一堆玻尔兹曼机来初始化这个网络，会发生两件美好的事情。

第一件美好的事情是，如果你以这种方式初始化，网络学习的速度比用随机权重初始化要快得多，因为它已经学习了一大堆用于建模数据中结构的合理特征。它还没有学习任何关于事物名称的信息，但它已经学习了数据中的结构，然后学习事物名称就相对较快，就像小孩子一样，他们不需要被告知 2000 次那是一头牛，他们才能知道那是一头牛，他们自己弄清楚了牛的概念，然后他们的母亲说那是一头牛，他们就明白了，好吧，也许两次。

所以这使得学习速度快得多，例如识别图像中的物体，它还使网络泛化得更好，因为它们在不使用标签的情况下完成了大部分学习，他们现在不需要很多标签，他们不是从标签中提取所有信息，他们是从数据中的相关性中提取信息，这使得它们在需要少得多的标签的情况下泛化得更好。所以这一切都非常好。

在 2006 年到 2011 年之间，人们开始使用，特别是在我的实验室、约书亚·本吉奥的实验室和杨立昆的实验室，人们使用堆叠的 RBM 来预训练前馈神经网络，然后他们会应用反向传播。在 2009 年，我实验室的两位学生乔治·达尔和阿卜杜勒·拉赫曼·穆罕默德表明，这项技术比现有的识别语音中音素片段的最佳技术效果略好。

这改变了语音识别社区，我的研究生们去了各个领先的语音研究小组。在 2012 年，基于这种堆叠的受限玻尔兹曼机的东西被应用到了谷歌的产品中，他们突然获得了更好的语音识别，安卓上的语音识别变得好多了。不幸的是，对于玻尔兹曼机来说，一旦我们证明了这些深度神经网络真的很好用，如果你用堆叠的受限玻尔兹曼机预训练它们，人们就找到了初始化权重的其他方法，他们不再使用堆叠的玻尔兹曼机。但如果你是一位化学家，你知道酶是有用的东西，尽管 RBM 不再被使用，它们使我们能够从认为深度神经网络永远不会工作，转变为看到深度神经网络实际上可以很容易地工作，如果你以这种方式初始化它们。一旦你完成了转变，你就不再需要酶了，所以把它们看作是历史上的酶。

在睡眠期间使用反向学习来获得一种更符合生物学原理的算法，并避免反向传播的反向过程，我仍然认为这个想法有很多潜力。我仍然乐观地认为，当我们最终理解大脑如何学习时，它将涉及到使用睡眠来进行反向学习。所以我仍然很乐观，我想我就讲完了。

**结束致谢 (1:16:28 - 1:17:53)**

非常好，非常感谢。

那么，请大家和我一起欢迎两位获奖者上台，共同接受我们最热烈的掌声！

也许你们可以向前一步，站到前面，也许你们可以走到前面这里。
