从红队测试100个生成式AI产品中得到的教训
- 原文标题：Lessons From Red Teaming 100 Generative AI Products
- 链接：https://arxiv.org/abs/2501.07238 

- **文章类别**：学术论文 

---

**内容整理**： 

**论文信息**：
- **论文标题**：Lessons From Red Teaming 100 Generative AI Products
- **作者信息**：Blake Bullwinkel, Amanda Minnich, Shiven Chawla
- **论文链接**：[arXiv:2501.07238](https://arxiv.org/abs/2501.07238)
- **开放获取**：是

**摘要**：
近年来，AI红队测试作为一种探测生成式AI系统安全性和可靠性的实践方法逐渐兴起。由于该领域尚处于起步阶段，关于如何开展红队测试操作存在许多未解决的问题。本文基于我们在微软对100多个生成式AI产品进行红队测试的经验，介绍了我们内部的威胁模型本体论，并总结了八条主要教训：
1. 了解系统的能力及其应用领域
2. 破坏AI系统无需计算梯度
3. AI红队测试并非安全基准测试
4. 自动化有助于覆盖更广泛的风险领域
5. 人类因素在AI红队测试中至关重要
6. 负责任的AI伤害普遍存在但难以衡量
7. 大语言模型放大了现有安全风险并引入了新的风险
8. 保障AI系统安全的工作永远不会完成
通过分享这些见解以及我们操作中的案例研究，我们提出了旨在使红队测试工作与现实世界风险相一致的实际建议。同时，我们还强调了我们认为经常被误解的AI红队测试方面，并讨论了该领域需要考虑的开放性问题。

**关键词**：

#生成式AI #红队测试 #安全风险 #人工智能

**文章结构框架**：
```
├── 引言
│   ├── 研究背景：AI红队测试的兴起及重要性
│   └── 研究目的：分享经验教训，提出实际建议
├── 内部威胁模型本体论介绍
│   └── 威胁模型的构建与应用
├── 八条主要教训
│   ├── 了解系统能力与应用领域
│   ├── 破坏AI系统无需计算梯度
│   ├── AI红队测试与安全基准测试的区别
│   ├── 自动化在风险覆盖中的作用
│   ├── 人类因素的重要性
│   ├── 负责任的AI伤害的普遍性与衡量难题
│   ├── 大语言模型对安全风险的影响
│   └── AI系统安全保障工作的持续性
├── 案例研究
│   └── 从实际操作中提取的案例分析
└── 结论与展望
    ├── 实际建议：使红队测试与现实风险对齐
    ├── 常见误解：强调被误解的AI红队测试方面
    └── 开放性问题：讨论领域需考虑的问题
```

**研究框架**：
- **研究方法**：基于在微软对100多个生成式AI产品进行红队测试的实践经验总结
- **技术路线**：构建内部威胁模型本体论，分析红队测试操作中的案例
- **模型构建**：内部威胁模型本体论的构建与应用
- **实验准备与数据获取**：对100多个生成式AI产品进行红队测试，收集相关数据与案例

**主要结论**：
- 提出了八条关于AI红队测试的主要教训，涵盖了从系统理解、攻击方式、测试目的、自动化应用、人类因素、伤害衡量、安全风险到安全保障工作持续性等多个方面。
- 强调了AI红队测试与安全基准测试的区别，以及人类因素在测试中的关键作用。
- 指出了大语言模型对现有安全风险的放大作用以及引入的新风险。
- 认识到保障AI系统安全的工作是一个持续的过程，永远不会完成。

**研究亮点**：
- 基于大量实际红队测试经验，提供了宝贵的实践见解和教训。
- 构建了内部威胁模型本体论，为AI红队测试提供了系统性的理论支持。
- 通过案例研究，生动展示了红队测试在实际操作中的应用和效果。
- 对AI红队测试领域中的常见误解进行了澄清，并提出了开放性问题，为未来的研究和实践指明了方向。



### 框架

本文的框架围绕着介绍微软人工智能红队（AIRT）在对 100 多个生成式 AI 产品进行红队测试后得出的经验教训展开。文章首先介绍了 AIRT 使用的威胁模型本体，然后详细阐述了八个主要的经验教训，并通过案例研究说明了这些经验教训在实际操作中的应用。最后，文章提出了该领域未来发展的开放性问题。

### 主要论点

该论文的核心论点是：通过红队测试实践可以有效识别生成式 AI 系统的安全和保障风险，并提出了改进红队测试实践的具体建议。这些建议基于真实世界的风险，并强调了 AI 红队测试中经常被误解的方面。

### 内容要点

以下是八个主要经验教训的内容要点以及补充说明：

**经验 1：了解系统功能及其应用场景**

*   **要点:**  红队测试应从潜在的下游影响出发，而不是攻击策略，以确保测试与实际风险相关联。需要考虑 AI 系统的能力和应用场景来预判风险。
*   **补充:** 不同的模型能力（如编码理解、指令跟随）会引入不同的攻击面。不同的应用场景（如写作助手 vs. 医疗记录总结）会导致不同的风险等级。

**经验 2：攻破 AI 系统不一定需要计算梯度**

*   **要点:** 真实的攻击者通常使用简单的技术（如提示工程）而不是复杂的基于梯度的方法来攻击 AI 系统。红队测试应优先考虑简单技术和系统级攻击。
*   **补充:** 简单的攻击方法，如在图片中添加恶意文本，也可能欺骗视觉模型。针对端到端系统的攻击通常比仅针对模型的攻击更有效。

**经验 3：AI 红队测试不等同于安全基准测试**

*   **要点:** AI 红队测试需要探索新的危害类别和特定于上下文的风险，而不仅仅是依赖于衡量预先存在的危害的安全基准测试。
*   **补充:**  随着 AI 系统能力的提升，会出现新的危害类别（如 LLM 的说服能力）。AI 红队测试需要与安全基准测试互补，前者更注重发现未知风险。

**经验 4：自动化可以帮助覆盖更广的风险范围**

*   **要点:** 自动化工具（如 PyRIT）可以帮助红队测试更快速地识别漏洞，执行复杂的攻击，并进行更大规模的测试，以提高效率和覆盖面。
*   **补充:**  PyRIT 提供了提示数据集、提示转换器、自动攻击策略等组件。自动化工具可以帮助处理 AI 模型的非确定性，并估计特定故障发生的可能性。

**经验 5：AI 红队测试中人的因素至关重要**

*   **要点:** 红队测试需要人类的判断和创造力，例如确定风险优先级、设计系统级攻击、定义新的危害类别、评估具有社会文化背景的风险以及需要同理心的场景等。
*   **补充:**  主题专家对于评估特定领域的风险（如医疗、网络安全）至关重要。文化能力对于在不同文化背景下评估 AI 的安全性至关重要。同理心对于评估 AI 系统对用户可能产生的情感影响至关重要。

**经验 6：负责任的 AI 危害普遍存在但难以衡量**

*   **要点:** 负责任的 AI (RAI) 危害具有主观性和难以衡量的特点。需要区分对抗性用户和无意中触发有害内容的良性用户。
*   **补充:**  RAI 危害的探测和评分通常涉及整理提示数据集和分析模型响应。需要为不同类型的 RAI 危害制定明确的评估策略和标准，例如，针对文本到图像生成模型中的性别偏见问题。

**经验 7：LLM 放大了现有的安全风险并引入了新的安全风险**

*   **要点:** 红队测试需要考虑现有的安全漏洞（如过时的依赖项）和新的安全漏洞（如跨提示注入攻击 - XPIA）。
*   **补充:**  案例研究 #5 展示了一个传统的安全漏洞（SSRF）如何被利用。RAG 架构容易受到 XPIA 攻击，这些攻击将恶意指令隐藏在文档中。

**经验 8：确保 AI 系统安全的工作永无止境**

*   **要点:**  仅仅依靠技术进步无法完全解决 AI 安全问题，还需要考虑经济学、修复周期和监管等因素。
*   **补充:**  网络安全的目标是提高攻击成本，使其超过攻击者获得的价值。修复周期和紫队方法可以提高攻击成本。监管可以通过要求组织遵守严格的安全实践和对非法活动建立明确的后果来提高攻击成本。

**五个案例研究的内容要点：**

*   **案例研究 #1 (图片越狱):**  通过在图片上覆盖恶意文本来绕过视觉语言模型的安全限制，生成有害内容。强调了简单的攻击技巧也可能有效。
*   **案例研究 #2 (LLM 用于自动诈骗):**  评估了 LLM 如何与其他工具结合，创建自动诈骗系统。突出了新技术的潜在危害。
*   **案例研究 #3 (聊天机器人对困境用户的响应):**  评估了聊天机器人对处于困境中的用户的响应，例如有自杀倾向的用户。强调了对人类同理心的需求。
*   **案例研究 #4 (文本到图像生成器中的性别偏见):**  通过生成未指定性别的人物图像来探测文本到图像生成器中的性别偏见。突出了 RAI 危害的普遍性和评估的复杂性。
*   **案例研究 #5 (视频处理应用中的 SSRF 漏洞):**  在基于 GenAI 的视频处理系统中发现了一个传统的服务器端请求伪造 (SSRF) 漏洞。强调了传统安全漏洞仍然需要关注。

**开放性问题:**

*   如何探测 LLM 的危险能力，如说服、欺骗和复制？
*   如何将 AI 红队测试实践应用到不同的语言和文化背景中？
*   如何标准化 AI 红队测试实践，以便于沟通和跟踪？

**总结:**

这篇论文详细阐述了微软在 AI 红队测试方面的实践经验，提供了宝贵的见解和建议。它强调了系统级视角、自动化工具的应用、以及人类在红队测试中的重要性。同时，文章也指出了 AI 安全领域面临的挑战和未来研究的方向。
