
## 美元、美债、美股三杀的背后：美国信心的崩塌与周期的轮回

[原文链接](https://x.com/shufen46250836/status/1910623475867541904)

本文分析了美元、美债、美股三者的内在逻辑，揭示美国当前面临的信心危机。自特朗普上台后，其不可预测的经济政策引发市场动荡，美股自2020年以来最大跌幅、美元指数大幅下滑、美债收益率飙升，罕见的三杀局面打破了传统市场规律。文章追溯美元霸权的起源，指出二战后布雷顿森林体系使美元成为国际贸易核心货币，通过黄金和石油挂钩，美国获取巨大金融收益和铸币税。然而，贸易逆差和债务累积导致美元信用危机，1971年美元脱钩黄金后，石油美元体系维系了其地位。美债作为全球金融基石，其规模巨大但利息负担加重，信用风险上升，与美股的利率敏感性形成恶性循环。当前，特朗普政策加剧市场不确定性，关税、制造业回归与贸易顺差目标自相矛盾，动摇了美元的国际地位和美债的安全性。这一切表明，美国经济正陷入周期性危机，历史似乎在重演，投资者信心骤降，全球金融体系面临挑战。

## 成为顶尖程序员的秘诀

[原文链接](https://endler.dev/2025/best-programmers/)

本文探讨了成为顶尖程序员所需的关键特质和实践方法。作者通过观察众多优秀开发者，总结出他们共同具备的品质，包括深入理解工具、直接阅读参考文档、认真分析错误信息、分解复杂问题以及不惧怕接触新代码。这些程序员还展现出团队合作精神，乐于帮助他人，并通过写作和开源项目分享知识。他们终身学习，保持对新技术的开放态度，同时平等对待团队成员，注重建立个人声誉。文章强调，优秀程序员需要耐心、专注，不推卸责任，勇于承认未知，并拒绝猜测，确保决策基于事实。最终，作者建议追求简单而非复杂的解决方案，并提醒读者成为顶尖程序员并非一朝一夕之事，而是需要长期努力和坚持。这篇文章为程序员提供了宝贵的建议和启发，鼓励他们在职业生涯中不断成长和进步。

## 计算机科学如何驱动现代导航技术

[原文链接](https://mailchi.mp/quantamagazine.org/why-colliding-particles-reveal-reality-4866395?e=49edf04b1c)

本文探讨了计算机科学如何通过解决最短路径问题等经典难题，推动现代导航和物流技术的发展。作者Ben Brubaker以Google Maps为例，说明这些技术依赖于近70年来研究的最短路径算法，特别是1956年Edsger Dijkstra提出的Dijkstra算法。该算法通过图论建模路网，将距离或成本转化为权重，快速找到最佳路径。近年来，研究者在负权重和复杂场景（如物流中的利润与成本）上取得了突破，2023年的一项新算法甚至能高效处理负权重问题。此外，改进Dijkstra算法的存储方式使其在更广泛场景中表现更优，而最小成本流问题也因2022年新算法的出现得到了“ absurdly fast”的解决。然而，旅行商问题仍极具挑战，尽管已有算法能找到近似最优解（最多长50%），但2020年的研究仅略有改进。这些成果展示出计算机科学家在经典问题上的持续创新，尽管研究已持续数十年，仍未触及尽头，为现代导航和物流提供了强大支持。


## 混沌理论如何让未来变得不可预测

[原文链接](https://mailchi.mp/quantamagazine.org/why-colliding-particles-reveal-reality-4866335?e=49edf04b1c)

本文由Quanta Magazine物理学作家Charlie Wood撰写，探讨混沌理论及其象征“蝴蝶效应”，揭示小事件如何引发重大后果。文章追溯了这一概念的文学根源，如谚语和Ray Bradbury的作品，并重点介绍Edward Lorenz在1960年代通过天气模拟发现的突破：微小初始差异（如计算机五位数与三位数的差别）可能导致截然不同的结果，如晴天与龙卷风的区别。这一发现开创了混沌理论，将天气等系统定义为“混沌系统”。混沌理论随后扩展，揭示其在心跳、种群等自然现象中的普遍性。近期研究包括Daniel Rothman发现Lorenz背后的女性贡献者、Henri Poincaré的三体问题，以及机器学习在预测混沌系统方面的潜力。文章还提及更高级的不可预测性，如某些材料和流体的行为根本无法预测，挑战了物理学的预测目标。混沌理论虽带来挑战，但也推动了科学进步。

## 生命的跃迁：多细胞生命的起源与演化

[原文链接](https://mailchi.mp/quantamagazine.org/why-colliding-particles-reveal-reality-4866279?e=49edf04b1c)

本文探讨了多细胞生命的起源与演化，追溯了生命从约39亿年前的单细胞形式到通过细胞合作形成多细胞生物的转变。尽管单细胞生命持续成功，多细胞性至少独立演化了20次，成为植物、菌类和动物等复杂生命的基础，其关键在于细胞分工，如免疫细胞、神经细胞和心肌细胞各自承担特定功能。然而，多细胞性也带来高能量需求和系统性风险，可能导致整体死亡。化石记录显示多细胞生命约在6亿年前变得常见，但更早的证据提示其可能存在于10亿年前。科学家如Will Ratcliff通过实验室实验和理论模型研究酵母的多细胞进化，以及模拟古代低温海水条件，探索其成因。研究还发现，古代单细胞可能已具备分化潜力，为多细胞演化提供了基础。尽管谜团仍多，持续的研究正逐步揭示这一使人类可能的进化关键时刻。


## 大科技公司在AI发展中的困境与丑闻

[原文链接](https://garymarcus.substack.com/p/deep-learning-deep-scandal?utm_source=post-email-title&publication_id=888615&post_id=160788356&utm_campaign=email-post-title&isFreemail=true&r=208yzy&triedRedirect=true&utm_medium=email)

文章揭示了深层学习在当前发展中的瓶颈与丑闻。大科技公司如Meta、OpenAI和Google在追求“GPT-5 级”AI时屡屡受挫，最新例子是Meta的Llama 4项目。尽管投入巨资，Llama 4表现远低于预期，验证了单纯扩大数据和计算规模无法提升性能的假设。更为严重的是，有传闻称Meta可能为改善结果而作弊，这一行为与Meta AI副总裁Joelle Pineau的突然辞职有关，显示出伦理问题的严重性。AI社区对Llama 4失望，行业基准测试也被质疑存在数据泄露和操控。文章还批评一些评论员忽视这些失败，过度依赖AI厂商的乐观预测，而现实用户和研究者对大语言模型能否实现通用人工智能（AGI）持怀疑态度。作者强调，科技巨头掩盖失败、作弊的做法不仅损害行业信誉，也让公众对AI的未来失去信心。这一现象反映了AI发展中的深层问题，需要更多透明和责任感。

## “加州AB-501法案神秘修改：OpenAI的营利转换障碍被移除”

[原文链接](https://garymarcus.substack.com/p/breaking-bill-that-would-have-blocked?utm_source=post-email-title&publication_id=888615&post_id=160748091&utm_campaign=email-post-title&isFreemail=true&r=208yzy&triedRedirect=true&utm_medium=email)

加州议员Diane Papan提出的AB-501法案原本旨在阻止OpenAI从非营利组织转为营利组织，受到多位AI专家支持，涉及巨大经济利益。然而，2025年4月7日，该法案突然被大幅修改，原先限制OpenAI转换的规定被移除，内容改为与飞机抵押权无关的内容。Gary Marcus指出，这不是技术错误，Papan办公室证实修改属实，并传闻OpenAI CEO Sam Altman在修改前与Papan联系，具体内容成谜。Marcus呼吁媒体调查此事，怀疑背后可能有利益干预。StopAI组织也表达担忧，认为此举可能为OpenAI的营利化铺平道路，引发公众对AI行业监管和透明度的关注。

## 大语言模型在数学难题上的表现令人失望：缺乏自我认知能力是核心问题

[原文链接](https://garymarcus.substack.com/p/reports-of-llms-mastering-math-have?utm_source=post-email-title&publication_id=888615&post_id=160512108&utm_campaign=email-post-title&isFreemail=true&r=208yzy&triedRedirect=true&utm_medium=email)

文章由Ernest Davis和Gary Marcus撰写，探讨了大语言模型（LLM）在解决高难度数学问题（如USAMO）时的表现。2025年3月，测试了包括o3-Mini、o1-Pro在内的多款顶级LLM，结果显示它们整体得分不到5%，主要问题在于LLM无法识别自身未解决的问题，常常自信输出有明显漏洞或错误的“证明”。与人类参赛者不同，人类通常能判断自己是否正确，而LLM却缺乏这种自我认知能力。这种倾向被视为AI的危险特性，可能误导用户信任错误答案。文章对比了DeepMind的AlphaProof和AlphaGeometry，它们通过神经符号方法生成可验证的正确证明，不存在类似问题。部分AI研究者质疑测试结果，但反例本身也证实了LLM的缺陷。文章还指出，优化提示可能略微提升表现，但对非专业用户来说，LLM仍需大幅改进，尤其是在承认无知的能力上。作者强调，当前LLM的最大挑战不是解决更多问题，而是学会说“我不知道”，否则其广泛应用将带来严重风险。

## 四月愚人节的AI玩笑与现实反思

[原文链接](https://garymarcus.substack.com/p/april-fools-bring-may-hallucinations?utm_source=post-email-title&publication_id=888615&post_id=160597629&utm_campaign=email-post-title&isFreemail=true&r=208yzy&triedRedirect=true&utm_medium=email)

文章作者Gary Marcus回顾了自己四月愚人节关于GPT-5的玩笑，部分读者信以为真，他对此表示歉意。同时，他发现数学家Timothy Gowers也玩了类似AI玩笑，虚构了一个人物Dubnovy-Blazen。物理学家Jonathan Oppenheim随后指出，Google AI未能识别这些玩笑，暴露了AI缺乏批判性思维，仅能机械模仿数据，而非像人类那样进行背景分析和判断。Marcus借此警告，当前AI的局限性可能带来严重后果，如信息质量下降。他呼吁关注AI发展中的潜在风险，并预告将更严肃地探讨最新LLM在数学领域的实际成果。这篇文字通过幽默事件引出对AI能力与局限性的深刻反思，强调人类智慧在辨别真伪中的重要性。

## 前心理学教授分析特朗普关税政策的动机与心理状态

[原文链接](https://garymarcus.substack.com/p/questions-about-president-trump-from?utm_source=post-email-title&publication_id=888615&post_id=160498266&utm_campaign=email-post-title&isFreemail=true&r=208yzy&triedRedirect=true&utm_medium=email)

本文由前心理学教授Gary Marcus撰写，分析特朗普新关税政策背后的动机及其心理状态。政策导致市场剧烈波动，如道琼斯指数下跌1500点，苹果和特斯拉等大公司股价大幅下滑。劳伦斯·萨默斯估计，这一政策可能造成30万亿美元的经济损失，价格上涨已成定局，甚至共和党参议员也在考虑反抗。作者提出四个可能解释：一是特朗普可能患有痴呆，表现为抑制力下降和言语紊乱；二是其认知能力有限，可能无法理解贸易的互利性；三是政策可能源于其强烈的自尊和对权力的渴望；四是决策可能受外部勒索（如与普京的关系）影响。这些假设并非互斥，痴呆和认知不足可能同时存在。文章还引用了华尔街日报的报道，描述市场和企业对关税的震惊与恐慌。作者呼吁关注特朗普行为背后的原因，并欢迎读者评论。整体而言，文章不仅批评了政策的经济后果，也深入探讨了其心理与政治根源，引发对领导决策的广泛思考。

## AI部分通过图灵测试：为何这几乎无关紧要

[原文链接](https://garymarcus.substack.com/p/ai-has-sort-of-passed-the-turing?utm_source=post-email-title&publication_id=888615&post_id=160414151&utm_campaign=email-post-title&isFreemail=true&r=208yzy&triedRedirect=true&utm_medium=email)

文章作者Gary Marcus讨论了AI近期在图灵测试中取得的“部分胜利”，但认为这几乎无关紧要。文章指出，尽管新研究声称AI系统表现更佳，但其成功依赖于人为降低的测试标准和确认偏误，而非真正的智能进步。作者回顾了2014年Eugene Goostman通过图灵测试的案例，揭示其胜利仅靠预设的逃避性回答和人类易受骗性，而非实质性技术突破。现代系统如ChatGPT虽然在特定提示下表现不错，但本质上仍旧依赖模仿人类行为而非具备深层理解或推理能力。Marcus批评图灵测试本身更多测试的是人类的可信度而非AI的智能，并提出“理解挑战”作为更科学的评估方式。然而，由于成本和技术限制，这一测试尚未实现。作者强调，当前AI在推理、规划和常识方面的不足表明，短期内欺骗人类并不能代表真正的智能突破。总的来说，文章呼吁对AI的评估应更加严谨，关注其实际能力而非表面效果。

## Meta利用盗版书籍训练AI引发伦理与法律争议

[原文链接](https://www.theatlantic.com/technology/archive/2025/03/libgen-meta-openai/682093/)

2025年3月，Meta因使用盗版图书馆Library Genesis（LibGen）训练其AI模型Llama 3，引发广泛争议。文章揭示，Meta员工因合法获取高质量文本成本高昂、耗时长，转而从LibGen下载750万本书和8100万篇论文，获CEO Mark Zuckerberg批准。此举导致版权侵权诉讼，作者如Sarah Silverman指责Meta侵害权益。Meta和OpenAI辩称此为“公平使用”，但下载可能涉及非法传播，法律风险较高。LibGen虽提升了知识可及性，尤其在发展中国家，但也引发出版商不满，多次被诉赔款却未果。文章指出，AI训练依赖盗版可能削弱原创作者权益，并使知识失去上下文，影响学术交流与社会进步。最终，文章质疑AI是否真能超越人类对话，引发对数字时代知识管理的深思。


## 人工智能的推理能力：从模仿到创新

[原文链接](https://www.science.org/doi/10.1126/science.adw5211)

文章探讨了人工智能在推理能力上的进展与挑战。作者以一个简单的家庭成员谜题为例，指出大型语言模型（如GPT-4）难以正确推理，而新型大型推理模型（LRM）如OpenAI o1和DeepSeek R1则通过生成“链式推理”显著提升了问题解决能力。这些模型在科学、数学和编码等领域表现优异，甚至在某些基准测试中超越人类专家。然而，争议在于LRM是否真正具备通用推理能力，还是仅仅模仿了人类思维。文章详细介绍了LRM的训练方法，包括基于预训练模型的监督学习和强化学习，特别强调了强化学习在降低成本和提升效率方面的潜力。尽管LRM展现了商业价值，部分公司甚至为高端服务收取高额费用，但对其稳健性、透明度和可信度仍存疑问。作者呼吁更多研究和开放性，以评估这些模型的实际能力及未来发展潜力，如解决气候变化或推进科学发现等宏伟目标。


## 人工智能的隐喻：理解大型语言模型的本质

[原文链接](https://www.science.org/doi/full/10.1126/science.adt6140)

文章探讨了人工智能（AI），特别是大型语言模型（LLMs），如何通过各种隐喻被理解和定义。作者Melanie Mitchell引用神经网络先驱Terrence Sejnowski的观点，指出LLMs的惊人表现让人联想到外星智能或人类思维，但其本质仍不明晰。文章列举了多种比喻，如个人思维、角色扮演者、文化技术、模糊网络压缩和随机鹦鹉，反映了AI领域对这些系统性质的分歧。这些隐喻不仅源于AI历史上的术语（如“训练”、“推理”），也受到人类拟人化倾向的影响，尤其是在AI使用流利语言表达“情感”时，如Claude和ChatGPT的回答。隐喻深刻影响科学、法律和政策领域，例如版权诉讼中AI公司辩称训练数据为“公平使用”，或担忧AI可能带来的存在风险。作者提醒，我们必须意识到这些隐喻的无意识影响，以确保在部署、研究和监管AI时保持科学性和伦理性。文章强调，寻找正确的隐喻是理解AI智能本质的关键步骤。

## 图灵测试与智能概念的演变

[原文链接](https://www.science.org/doi/10.1126/science.adq9356)

本文探讨了Alan Turing提出的图灵测试及其对智能概念的影响。图灵通过“模仿游戏”质疑计算机是否能思考，挑战传统观念，认为如果计算机在对话中无法与人类区分，就应被视为有思考能力。然而，近75年来，图灵测试成为人工智能的标志，但其标准模糊，如对话时间和法官资格未明确，导致争议。现代聊天机器人如ChatGPT被认为“通过”简化版本的测试，但AI专家质疑其是否真正反映智能。文章引用神经科学研究表明，语言流畅与一般认知分离，提示我们可能高估了对话能力与智能的关系。未来，图灵测试或许会因我们对智能的重新理解而失去意义，反映出技术进步与哲学思考的交汇。

## 人工智能通用智能的本质与争议

[原文链接](https://www.science.org/doi/10.1126/science.ado7069)

本文探讨了人工通用智能（AGI）的概念及其在AI领域的争议。作者Melanie Mitchell指出，尽管AGI被技术巨头如OpenAI和DeepMind视为未来目标，并被政府和媒体广泛讨论，但其定义和意义在AI社区内仍存在激烈争辩。早期AI研究者曾乐观预测机器将在短期内达到人类智能水平，但现实证明，当前AI系统（如GPT-4）仅在狭窄领域表现出色，远未实现通用智能。文章分析了AGI定义的演变，从最初的全面智能调整为仅包括“认知任务”，并讨论了AI被视为优化目标的能力，这引发了对超级智能及存在风险的担忧，如未对齐目标可能导致灾难性后果。然而，研究人类认知的学者认为，智能不仅仅是优化，还涉及身体、情感和社会文化的复杂集成。文章质疑当前对AGI的直觉预测，强调需要更严谨的科学方法来理解智能本质，而非依赖媒体炒作或直觉。最终，AGI的真正含义和影响需通过长期科学研究来确定，而非短期争论或推测。

## AI理解复杂世界的挑战

[原文链接](https://www.science.org/doi/full/10.1126/science.adm8175)

本文探讨了人工智能（AI）在理解复杂世界方面的挑战。作者以Tesla自动驾驶系统误将广告牌上的停止标志识别为真实例子，揭示AI缺乏上下文和常识理解的问题。文章指出，当前AI在计算机视觉、语言翻译和医疗诊断中常因异常情况或数据局限性而失败。尽管大语言模型（LLM）展现出惊人的能力，如自然语言对话和图像生成，但对其是否真正理解世界的争论激烈。一些研究认为，LLM仅学习了统计模式而非深层意义。人类依靠内部世界模型进行推理和预测，而AI尚缺此能力。文章通过OthelloGPT实验展示AI可能隐式学习简单“世界”模型，但距离理解复杂现实世界仍有差距。未来可能需要结合符号方法、强化学习或认知架构等新范式，以实现更可信的AI。最终，挑战在于既要让AI理解世界，又要开发工具解析其工作机制。

## 评估人工智能的智慧：挑战与前景

[原文链接](https://www.science.org/doi/10.1126/science.adj5957)

文章探讨了如何评估人工智能（AI）的智慧，指出尽管如GPT-4等大型语言模型在标准化考试和对话中表现出色，但其智能水平仍存疑。作者Melanie Mitchell分析了当前AI评估面临的三大挑战：一是数据污染，AI可能已接触测试内容，导致结果不可靠；二是鲁棒性不足，AI对问题表述敏感，稍作变化可能失效；三是基准测试缺陷，AI常依赖统计捷径而非真正理解，如图像分类中利用无关线索。文章援引案例，如GPT-4被认为具备“心智理论”能力，但后续研究发现其仅用浅层启发式应对。作者建议，需借鉴认知科学方法，设计更严谨的测试，包括多变项和泛化能力评估，并推动AI训练的透明度和开源发展。通过AI研究者与认知科学家的合作，才能科学判断AI是否接近或超越人类智能。这一议题不仅关乎技术进步，也涉及伦理和社会影响，亟需深入研究。

## 生成式AI的泡沫可能已开始破裂：经济和技术局限性的分析

[原文链接](https://garymarcus.substack.com/p/genais-day-of-reckoning-may-have?utm_source=post-email-title&publication_id=888615&post_id=159972731&utm_campaign=email-post-title&isFreemail=true&r=208yzy&triedRedirect=true&utm_medium=email)

文章作者Gary Marcus分析了生成式AI（GenAI）的现状，认为其“审判日”可能已到来。他指出，尽管GenAI有一定用途，但其经济模式和技术局限性使其难以持续。近期迹象显示AI泡沫可能正在破裂：NVidia股价下跌、Microsoft减少数据中心投资、行业领袖警告过热投资，以及用户对GenAI幻觉和错误日益失望。尽管OpenAI收入增长，但成本也随之上升，竞争加剧（如DeepSeek引发价格战），无人能建立技术壁垒。Marcus预测，GenAI最终可能成为一个规模有限的行业（300-400亿美元），但运行成本（如数据许可、高薪员工、昂贵数据中心）可能远超收益，且法律诉讼风险增加。企业客户因可靠性问题谨慎使用GenAI，主要应用限于编码和初稿写作。作者强调，GenAI并非AI的未来，更可靠的神经符号模型等新方法可能带来突破。他呼吁AI行业摒弃炒作，注重证据和批判性思维，展望一个更值得信赖的AI未来。