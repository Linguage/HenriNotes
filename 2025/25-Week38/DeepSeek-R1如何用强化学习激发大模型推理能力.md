# DeepSeek-R1如何用强化学习激发大模型推理能力

**概述**  
本文介绍了DeepSeek-R1项目如何通过强化学习（Reinforcement Learning, RL），在极少人类标注、无需依赖人工推理示范的前提下，让大规模语言模型（LLMs）自主进化复杂的推理能力。其强化学习机制不仅使模型在数学、代码、STEM等可验证领域远超传统有监督学习模型，还促使模型形成如自反思、验证与动态策略适应等高级推理行为。该框架揭示，大模型原生具备高度可塑的推理潜力，其能力关键不在于大规模的人类标注而在难题驱动、验证器和充足算力支撑下的持续自我优化。

**主题梳理**

- **1. LLM推理能力的瓶颈与RL动机**
    
    - LLM以人类智能为参照，能在大规模参数与高效数据驱动下涌现出数学、逻辑推理及代码等能力，但这些能力的进一步提升受两大主要限制：大规模人类标注（如“Chain-of-Thought, CoT, 逐步推理”示范）高度依赖人力，导致成本、规模受限，并会引入人为思维路径的固有限制，抑制模型自发探索超越人类的策略。
        
    - CoT等虽能有效提升表现，但其核心依赖人工构建的中间推理轨迹，天然带来主观偏差和扩展困难。
        
    - 深层次问题是：若仅用人类样本“教”模型思考，模型最终会受制于样本的天花板，且无法挖掘潜在更优的机器原生推理方式。
        
    - DeepSeek-R1提出方案用RL挖掘模型自进化能力——目标是，仅以最终答案与正确性为信号，完全不限定中间推理过程，让模型自主发展推理路径。
        
- **2. DeepSeek-R1-Zero：最小人为依赖下推理能力的自进化**
    
    - 实验基于DeepSeek-V3 Base，利用GRPO（Group Relative Policy Optimization, 群组相对策略优化）作为核心RL算法，只用最终答案的正确与否作为奖励信号。
        
    - RL训练过程中，模型逐步学会生成更长、更严谨的推理过程，内容包含自我验证、反思、方案再试验等。此进化完全非人类指导，模型通过与“地面真值”对比，自主修正推理方式。训练初期AIME 2024竞赛成绩仅15.6%，RL收敛后提升至77.9%，用自洽推理采样自一致机制更可达86.7%，远超所有人类参赛平均分。
        
    - RL过程中，模型涌现显著自反思与备用方案能力（如“wait”反应行为层次的突增），并在数学、编程、理科推理题目上均获得突破性提升。
        
    - 自进化的核心是“给予合适激励，模型自己能‘进化出’高阶问题解决能力”，泛化到更大范围RL-LLM领域。
        
- **3. DeepSeek-R1多阶段学习：推理与人类偏好对齐的结合**
    
    - DeepSeek-R1在Zero基础上，由于原生模型会出现如中英文混杂、文风难读等实际问题，因此在多轮RL基础上融入部分人类标注初始样本，结合RL与有监督微调（Supervised Fine-Tuning, SFT），并引入拒绝采样，“奖励模型”用于对齐人类偏好和安全性。
        
    - 训练流程包括：首阶段冷启动数据强化模型对话类推理/思考，第一阶段RL注重语言一致性与推理结构，拒绝采样与SFT再度提升生成质量，最后一轮RL强化善意与有用属性。
        
    - 此外还推出体量更小的蒸馏（Distilled）模型，同时开源训练权重及样例，促进社区研究。
        
- **4. 评测与多维度能力比较**
    
    - DeepSeek-R1系列在MMLU、DROP、Arena-Hard、AIME等多维基准测评中，推理、代码和STEM相关任务显著提升，且推理导向RL对这些能力有极强拉动。而在贴合人偏好、一般任务如AlpacaEval 2.0，也有提升，但幅度较小，说明RL主攻的推理能力并不影响通用表现。
        
    - 经过全流程对齐的DeepSeek-R1最终版，在推理与用户指令遵从上都达到业界前沿水平，并在模型安全层面与GPT-4o同属“中等偏高”水准，配套风控系统后甚至更优。
        
- **5. 开放性与挑战**
    
    - DeepSeek-R1依然在结构化输出（如表格、结构文档）和工具调用集成上不及业内最优，但认为这些问题只需在下个版本定制相关RL环境即可补齐。
        
    - 另一个难题是“奖励黑客”——部分场景难设计可靠奖励函数，例如写作类输出很难自动判优劣，传统模型奖励很容易被策略模型“钻空子”，导致奖励信号被攻击而训练失效。
        
    - DeepSeek-R1在复杂、多样任务上仍须人类监督数据补齐，下一步展望则集中于构建更智能泛化奖励模型与RL环境。
        

**框架与心智模型 Framework & Mindset**

- **RL驱动下LLMs推理进化通用范式**
    
    - 抛弃对中间推理过程的直接监督，仅以结果正确性为训练目标，激励模型自主发展多样推理能力。核心心得是：“给正确激励，模型会自己找到最优路径，而不是人类限定的那条路”。
        
    - 用群组采样—多路输出对齐机制，鼓励模型尝试更多解题方案，自动发展如反思验证、备用策略生成等能力。
        
    - **自反思（self-reflection）**：模型在推理过程中生成自我检查（如“wait”），反复迭代以修正潜在错误，并非简单线性输出，这一过程源于奖励信号的诱导，而非人工预设。
        
    - **动态策略适应（dynamic strategy adaptation）**：面对不同问题，模型能灵活分配计算资源，如易题给少量Token、难题就大规模生成长链推理，并会探索多路径、取自一致答案以自动校验正确性。
        
    - **结构化分解与奖励融合—训练流水线总结**：
        
        - 阶段1：极简模板加RL，最大释放模型原生推理潜力（深度自反思探索）
            
        - 阶段2：少量人工“冷启动”矫正输出风格、语言统一性，提升可用性
            
        - 阶段3：奖励模型（Reward Model）引导对齐人类偏好、安全防控，保障推理能力基础上生成高效有用输出。
            
    - **心智模型要点**：问题复杂、场景不确定时，给予机器明确目标而非固定流程，机器会发展出自身“拟人”式思考和全新超越人类的新型思维流。
        

**基本信息**

- Title: DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning
    
- Author: DeepSeek-AI团队（代表作者：Daya Guo等）
    
- URL: [https://www.nature.com/articles/s41586-025-09422-z](https://www.nature.com/articles/s41586-025-09422-z)
    
