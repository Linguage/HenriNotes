# 为个人使用场景评估大型语言模型

## 文章概要

本文作者 Graham King 分享了他为个人使用场景评估各种大型语言模型（LLM）的过程和结果。他的核心观点是：对于大多数日常任务，所有主流模型的表现都足够出色，因此成本和延迟成为了选择模型的关键决定因素。

作者从自己的 bash 历史中收集了 130 个真实的提示（问题），并将其分为四类：编程、系统管理（Sysadmin）、技术解释、以及通用知识和创意任务。他使用这些提示在 OpenRouter 平台上对 11 个模型（包括 Claude Sonnet 4、DeepSeek 系列、Gemini 2.5 系列、Kimi、GPT-OSS-120B、Qwen3 系列、GLM 4.5 等）进行了盲测评估，记录了每个模型的成本、延迟和吞吐量。

评估的主要发现包括：
1.  **所有模型都很优秀**：绝大多数模型都能正确回答大部分问题，且答案质量普遍令人满意。
2.  **成本和延迟是关键**：在准确性相差无几的情况下，模型的调用成本和响应速度成为了主要的区分点。作者希望提问成本极低且响应迅速。
3.  **闭源模型并非更优**：Anthropic 的 Claude 和 Google 的 Gemini 2.5 Pro 在质量和性价比上并未胜出，甚至有时不如开源模型。
4.  **Gemini 2.5 Flash 表现突出**：它通常响应最快，有时答案质量也很好，性价比极高。而 Gemini 2.5 Pro 因其高价格和低效的 token 使用而显得性价比不高。
5.  **推理模式（Reasoning）作用有限**：对于作者的简单问题，开启推理模式通常只会增加延迟，帮助不大。唯一的例外是写诗任务，推理模式下的模型确实能产出更高质量的作品。
6.  **不同模型各有所长**：例如，Mercury Coder 这种扩散模型速度极快；DeepSeek 和 Qwen 在多项任务中表现出色；GLM 4.5 在某些场景下给出了优秀的答案。

最终，作者并没有选择单一的“最佳”模型，而是根据不同的需求组合使用多个模型：日常快速查询用 DeepSeek Chat v3.1 或 Qwen3；需要第二意见时会并行查询 Gemini Flash 和另一个模型；只有在极少数需要深度思考的任务时，才会启用 Qwen3 Thinking 或 Claude Sonnet 4 的推理模式。

文章强调了模型评估的主观性，并提醒读者结果可能因平台（如 OpenRouter 的提供商）和设置（如推理强度）而异。

## 文章标签

#LLM评估 #个人AI #成本效益 #延迟 #模型选择

## 文章链接

[https://darkcoding.net/software/personal-ai-evals-aug-2025/](https://darkcoding.net/software/personal-ai-evals-aug-2025/)