# Writing with LLM is not a shame. An essay about transparency on AI use.

## 链接
https://reflexions.florianernotte.be/post/ai-transparency/

## 概要
这篇文章探讨了在使用人工智能（AI）生成内容时是否需要进行免责声明的伦理问题。作者 Florian Ernotte 最初认为需要透明度，并公开了自己的AI使用情况，但后来受到“人们修图后发布也不声明，周围充斥着大量被编辑图片”的论点影响，转为不再常规声明。然而，当他看到一些倡议（如 Derek Sivers 的页面、notbyai.fyi、蒙特利尔大学的指南）鼓励或要求声明AI使用后，他开始重新思考这个问题。
作者将讨论范围缩小到包含主观观点和思想的文本（如散文、评论），而非纯事实性内容（如天气预报、商业文案）。他认为，简单的“透明度”并非唯一答案，需要更深入探讨。他质疑“AI辅助”的定义模糊性，并指出像 Grammarly 这样的工具在AI出现前也未被特别声明使用。
文章提出，真正需要关注的可能是“可信度”问题：当一篇内容很有价值时，如果作者声称是自己创作的，但实际上主要是AI生成的，这可能会构成一种“作弊”或误导。然而，作者也指出，很多时候有价值的想法是在与AI的互动中产生的，作者在其中起到了构思和筛选的关键作用。
作者进一步将问题引向“溯源”：就像学术写作需要注明引用来源一样，AI生成内容的“思想源头”也应该被追踪。但他承认，由于LLM的技术特性（统计模型），它天然难以像人类那样自然地引用来源，尽管一些新模型已具备此功能，但这仍未完全解决透明度的核心问题。
文章还讨论了“信任”因素。作者质疑是否应该因为使用AI就默认内容质量下降或作者不可信，并指出预先声明使用AI可能会在读者心中植入偏见，影响他们对内容的客观评判。
最后，作者得出结论：当前围绕AI透明度的讨论可能陷入了一种“道德恐慌”或“思维警察”的陷阱。他引用 Christophe Denis 的观点，认为当前关于AI伦理的讨论常常表现为“空洞的警惕、顺从机制或指责姿态，而非真正的辨别力”。作者认为，现在就为AI使用制定严格的伦理规范可能为时过早，因为我们仍处在探索阶段。他主张，问题的关键不在于是否使用AI，而在于那些试图以“伦理”之名随意批评他人作品的人。

## 标签
#人工智能 #AI伦理 #透明度 #可信度 #内容创作 #AI使用声明 #溯源 #偏见 #道德恐慌