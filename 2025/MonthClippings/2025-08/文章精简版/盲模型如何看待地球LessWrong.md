# How Does A Blind Model See The Earth? — LessWrong

## 文章概要

这篇文章的作者受到古代不完整地图的启发，想探究大型语言模型（LLM）如何“看待”地球。作者认为，虽然我们可以从太空中清晰地看到地球，但古代地图反映了制图者个人的认知和知识边界，这种“不完整”和“亲密感”非常有趣。因此，作者尝试通过一种实验方法，从LLM内部的知识网络中提取出一幅（不完美的）世界地图。

**实验方法**：
1.  **采样坐标**：在全球范围内均匀采样经纬度坐标点（分辨率取决于模型运行成本）。
2.  **提问模型**：对每个坐标点，向经过指令微调的模型提问，例如：“如果这个位置在陆地上，就说‘陆地’。如果这个位置在水上，就说‘水’。不要说别的。x° S, y° W”。
3.  **获取概率**：获取模型输出“陆地”和“水”的对数概率（logprobs），并通过softmax函数将其转换为总和为1的概率。
4.  **生成图像**：将所有坐标点的概率组合成一张等距柱状投影的世界地图图像。

作者强调，这种方法比直接要求模型生成SVG或ASCII地图更能反映其真实的地理知识，因为直接生成的图像可能与内部知识无关。同时，作者也避免将实验变成严格的基准测试，而是更关注于观察和发现。

**实验结果**：
作者对众多主流LLM进行了测试，包括Qwen系列、DeepSeek系列、Kimi、Mistral系列、LLaMA系列、Gemma系列、Grok系列、GPT系列和Claude系列。结果显示：
*   **模型规模与性能**：通常情况下，模型规模越大，生成的地图越准确、越精细。小模型（如0.5B参数）可能将整个地球都视为陆地，而大模型（如数百B参数）则能较好地勾勒出各大洲和海洋的轮廓。
*   **模型家族差异**：不同公司的模型（如Qwen、LLaMA、GPT）在地图细节和置信度分布上展现出不同的“指纹”特征。即使是同一公司不同版本或变体（如coder、hermes）的模型，表现也可能有显著差异。
*   **后训练影响**：指令微调或进一步的后训练（如chat fine-tune）可能会显著改变模型的表现，有时甚至比模型规模的影响更大（如LLaMA 405b与其Hermes变体、GPT-4与其ChatGPT变体的对比）。
*   **MoE模型与多模态模型**：MoE（Mixture of Experts）模型在不同专家的路由上可能存在问题，影响地图的统一性。早期的多模态模型（如Grok）似乎并未显著提升其地理知识的准确性。
*   **置信度表现**：不同模型在置信度的分布上差异很大，有些模型（如GPT-4）表现出极高的置信度和清晰的边界，而有些则显得模糊或不确定。

**观察与思考**：
*   **“理想原型”**：作者推测，可能存在一个LLM所理解的“理想地球原型表示”，随着模型规模和能力的提升，其生成的地图会逐渐趋近于这个原型。
*   **未解之谜**：作者提出了一系列值得进一步探究的问题，例如：训练过程中的哪些因素真正决定了模型在此任务上的表现？基础模型和指令微调模型的表现有何差异？模型内部的地理知识是如何结构化的？MoE模型的专家激活图是怎样的？

总的来说，这篇文章通过一个有趣且直观的实验，揭示了不同LLM在地理知识上的掌握程度和表现差异，并引发了关于模型内部知识结构和训练影响的深入思考。

## 文章标签

#大型语言模型 #LLM #地理知识 #实验 #模型比较 #Qwen #DeepSeek #Kimi #Mistral #LLaMA #Gemma #Grok #GPT #Claude

## 文章链接

[https://www.lesswrong.com/posts/xwdRzJxyqFqgXTWbH/how-does-a-blind-model-see-the-earth](https://www.lesswrong.com/posts/xwdRzJxyqFqgXTWbH/how-does-a-blind-model-see-the-earth)