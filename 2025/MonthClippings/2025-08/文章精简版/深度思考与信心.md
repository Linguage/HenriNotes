# Deep Think with Confidence

[https://arxiviq.substack.com/p/deep-think-with-confidence](https://arxiviq.substack.com/p/deep-think-with-confidence)

这篇文章介绍了一种名为“深度思考与信心”（Deep Think with Confidence, DeepConf）的新方法，旨在提升大型语言模型（LLM）在复杂推理任务中的表现和效率。DeepConf是一种测试时推理（test-time inference）方法，它不依赖于额外的模型训练，而是巧妙地利用模型内部生成的对数概率（log-probabilities）来评估和引导推理过程。

*   **核心思想**：与传统的“多数一致”（self-consistency）方法生成大量推理路径并简单投票不同，DeepConf为每个推理路径计算“局部置信度分数”（localized confidence scores）。它有两种操作模式：
    1.  **离线模式（Offline Mode）**：先生成所有推理路径，然后使用置信度分数过滤掉低质量的路径，并对剩余路径进行加权投票（置信度越高的路径，其答案权重越大）。
    2.  **在线模式（Online Mode）**：这是一种更高效的创新模式。它在生成推理路径的过程中实时监控置信度。一旦检测到某条路径的置信度低于动态设定的阈值，就立即停止生成该路径（Early Stopping），从而节省计算资源。此外，当某个答案获得足够高的共识度时，也会提前终止整个生成过程。
*   **置信度度量**：DeepConf没有使用单一的全局置信度，而是设计了几种更精细的局部置信度指标，如“组置信度”（Group Confidence）、“尾部置信度”（Tail Confidence）等，这些指标能更准确地识别出推理过程中的薄弱环节。
*   **显著优势**：
    1.  **高准确性**：在多个复杂推理基准测试（如AIME、GPQA）上，DeepConf显著超越了传统的多数一致方法。例如，使用GPT-OSS-120B模型在AIME 2025上达到了99.9%的准确率。
    2.  **高效率**：尤其在线模式下，DeepConf能大幅减少生成的token数量（最高减少84.7%），这意味着更低的计算成本和更快的响应速度，有时甚至在减少计算量的同时还能提高准确率。
*   **意义与影响**：DeepConf代表了“测试时缩放”（test-time scaling）的一种重要进步，它证明了可以通过更智能的推理策略，而非仅仅增加计算量，来提升LLM的智能水平。这使得高性能的LLM推理变得更加经济、可扩展和实用。
*   **未来方向**：作者指出的主要挑战是“自信的错误”（confidently wrong）问题，即模型对错误答案也很有信心。未来的研究将关注于改进置信度校准和将DeepConf框架应用于强化学习等领域。

#标签 #大型语言模型 #LLM #推理优化 #置信度 #测试时缩放 #效率 #准确性 #AI研究