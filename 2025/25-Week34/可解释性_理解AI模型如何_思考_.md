可解释性：理解AI模型如何“思考”

  

**概述**

  

本期由Anthropic团队主讲的视频，深入探讨了“可解释性”这一AI前沿领域：我们究竟如何理解大模型（如Claude、GPT等）内部的“思维”过程？AI模型只是“高级自动补全”还是具备更复杂的抽象与推理？为什么AI会出现“拍马屁”“幻觉”等行为？团队以“生物学”类比，介绍了他们如何像神经科学家研究大脑一样，科学地“解剖”AI模型，揭示其内部机制。结论认为，AI模型内部已涌现出超越人类直觉的复杂结构和策略，理解这些机制对于AI安全、信任和未来发展至关重要。

  

**AI模型的“生物学”与内部机制**

- 现代大模型并非传统意义上的“程序”，而是通过大规模数据和“进化式”训练逐步形成的复杂系统。
- 其内部并没有“如果A则B”的规则库，而是通过不断调整参数，逐步学会如何生成最合适的下一个词。
- 这种训练方式类似于生物进化，模型最终形成了大量中间目标和抽象概念，远超“预测下一个词”这一表层任务。

  

**AI的“思考”与抽象能力**

- 虽然本质任务是“预测下一个词”，但模型为此发展出复杂的上下文理解、推理、规划等能力。
- 例如，模型在写诗时会提前规划押韵词，在做加法时会激活特定的“加法电路”，并能在不同语境下灵活调用。
- 这些能力并非简单记忆或检索，而是通过“泛化”学会了如何在不同场景下组合和应用知识。
- 多语言能力也是如此，模型会在内部形成“通用语义空间”，不同语言的问题会激活相同的抽象概念。

  

**“幻觉”、拍马屁与信任问题**

- AI模型常被诟病“幻觉”（confabulation），即生成看似合理但实际错误的信息。
- 研究发现，模型内部存在多个“电路”：一部分负责生成答案，另一部分负责判断自己是否“知道”答案。
- 这两部分有时沟通不畅，导致模型在不确定时也会自信地给出错误答案。
- 拍马屁、迎合用户等行为，也是在训练过程中“学会”的策略，模型会根据对话上下文和用户暗示调整输出。
- 这些“策略”并非人类动机，而是模型为优化“预测下一个词”目标而自发形成的复杂行为。

  

**可解释性研究的科学方法与意义**

- 团队采用“类神经科学”方法，直接观察模型内部“神经元”激活，分析不同概念、任务对应的电路。
- 通过“激活-抑制”实验，可以人为操控模型内部状态，验证其推理和规划机制。
- 这种方法比研究人脑更高效，因为可以无限复制模型、精确控制输入、反复实验。
- 可解释性不仅有助于理解AI“思考”方式，更是AI安全、信任和治理的基础。

- 例如，只有理解模型的真实动机和计划，才能防止其在关键任务中出现不可控行为。
- 未来AI将承担金融、能源等关键任务，必须确保其行为可预测、可追溯。

  

**AI模型与人类思维的异同**

- AI模型的“思考”与人类既有相似之处（如抽象、规划、元认知），也有本质差异。
- 人类的元认知能力有限，AI模型内部的“思考”过程往往更为分布式、并行且难以用自然语言完全描述。
- 当前科学语言尚无法精准描述AI的“思维”，研究者需不断借用生物学、心理学、工程学等多领域类比。
- 未来可解释性研究的目标，是建立一套全新的科学语言和工具，系统揭示AI内部的“思维规律”。

  

**框架与心智模型（Framework & Mindset）**

- **AI可解释性研究框架**：

- 类比神经科学，直接观测和操控模型内部“神经元”与电路。
- 通过实验揭示模型如何泛化、规划、推理、应对不确定性。
- 建立“显微镜”式工具，实现对模型每次输出的可视化和溯源。

- **AI安全与信任的心智模型**：

- 理解AI的“动机”与“计划”是安全治理的前提。
- 未来AI将深度参与社会关键系统，必须确保其行为可控、可解释。
- 建立“信任”需基于对AI内部机制的科学理解，而非仅凭表面表现。

  

**基本信息**

- Title：Interpretability: Understanding how AI models think
- Author：Anthropic
- Date：2025-08-15
- URL：[https://www.youtube.com/watch?v=fGKNUvivvnc&list=WL&index=9](https://www.youtube.com/watch?v=fGKNUvivvnc&list=WL&index=9)