AI学习理论的意外革命

  

**概述**

  

本文讲述了人工智能领域一个颠覆性发现：大规模神经网络为何能成功学习，推翻了三百年来关于“模型过大必然过拟合”的理论。通过“彩票票据假说”（Lottery Ticket Hypothesis）和“双重下降”现象，研究者们发现，庞大的模型并非简单地记忆数据，而是在庞大的参数空间中寻找最优、最简洁的解决方案。这一发现不仅改变了AI模型的设计思路，也重新定义了我们对智能和学习本质的理解。

  

**机器学习的铁律与传统理论**

- 三百年来，机器学习领域一直遵循“偏差-方差权衡”（bias-variance tradeoff）原则。理论认为，模型太简单会遗漏重要规律（欠拟合），模型太复杂则会记忆噪声（过拟合），导致泛化能力丧失。
- 以学生学习算术为例：聪明的学生会掌握进位和位值的算法，愚蠢的学生则死记每一道题的答案。后者在作业上能拿满分，但考试时却一败涂地。
- 神经网络因参数众多，被认为极易陷入记忆陷阱。传统理论预测，过参数化网络只会在训练集上表现完美，却无法应对新数据。
- 这一理论主导了整个领域：研究者们专注于小型、受控模型，采用各种架构和正则化技巧，力求避免过拟合。扩大模型规模被视为“愚蠢且昂贵”的做法。
- 学界权威不断强化这一观点，“大模型只会过拟合”成为共识。会议论文关注效率而非规模，增加参数被视为学术异端。

  

**挑战权威的异端者**

- 2019年，一批研究者选择无视警告，继续扩大模型规模。他们没有在训练准确率达到完美时停止，而是继续推进到理论所称的“危险区”。
- 结果出乎意料：模型并未崩溃。虽然一开始确实出现了记忆训练数据的现象，但随后性能却再次大幅提升。
- 这一现象被称为“双重下降”（double descent）：误差先因过拟合上升，随后却意外下降，模型超越了过拟合的限制。Mikhail Belkin等人指出，这一发现“与偏差-方差分析的传统智慧相悖”。
- 影响迅速扩散。OpenAI等机构发现，这种效应在参数数量级上持续存在。大模型不仅积累更多知识，还展现出全新能力，如仅凭少量示例就能学习任务。
- 行业风向急转。Google、Microsoft、Meta、OpenAI等巨头投入数十亿美元，打造更大规模的模型。GPT系列从1.17亿参数扩展到1750亿，“越大越好”成为新信条。
- 但一个问题始终困扰研究者：为什么大模型能成功？

  

**彩票票据假说的诞生与理论救赎**

- 2018年，MIT的Jonathan Frankle和Michael Carbin在研究神经网络剪枝（pruning）时，意外发现了“彩票票据”现象。
- 在每个大网络中，隐藏着“中奖票据”——极小的子网络，能达到完整网络的性能。研究者发现，去除高达96%的参数后，准确率几乎不变。绝大多数参数其实是“无用负担”。
- 关键洞见在于：这些子网络只有在初始随机权重不变时才能成功。改变初始值，同样的稀疏结构就会失败。
- 彩票票据假说（Lottery Ticket Hypothesis）由此形成：大网络的成功不是因为学会了复杂解法，而是因为提供了更多机会去找到简单解法。每组权重都是一张彩票——一个潜在的优雅解决方案。大多数彩票都“落空”，但有了数十亿张彩票，中奖几乎是必然。
- 训练过程中，网络并不是在寻找完美架构，而是已经包含了无数小网络，每个都有不同的初始条件。训练就像一场庞大的彩票抽奖，最优初始化的小网络最终胜出，其余则被淘汰。
- 这一发现让经验与经典理论得以调和。大模型不是在记忆数据，而是在庞大参数空间中找到最简洁的解决方案。奥卡姆剃刀（Occam’s razor，简约原理）依然成立：最简单的解释仍然最佳。规模只是帮助我们更高效地找到这些简单解释。

  

**智能的真正样貌**

- 这一理论影响远超人工智能。若学习的本质是寻找最简模型解释数据，而更大的搜索空间能带来更简洁的解法，那么智能的定义也随之改变。
- 人脑拥有860亿神经元、数万亿连接，按任何标准都极度“过参数化”。但人类却能用极少的例子学习并泛化。彩票票据假说暗示，这种神经元的“冗余”正是为了提供更多潜在的简单解决方案。
- 智能不是记忆信息，而是发现优雅的模式来解释复杂现象。规模为这种搜索提供了空间，而不是存储复杂解法。
- 这一发现也揭示了科学进步的本质。几十年来，研究者因理论限制而不敢扩大模型规模。突破来自于勇于质疑假设、进行实证测试。
- 这种模式在科学史上屡见不鲜。大陆漂移理论曾被否定，直到板块构造学说出现；量子力学曾被视为荒谬，直到实验结果不可忽视。最重要的发现往往源于突破理论边界的勇气。
- 彩票票据假说并未推翻经典学习理论，而是揭示了其更复杂的运作机制。简单解法依然最优，我们只是找到了更高效的寻找方式。
- 对AI开发而言，这一理解既带来希望，也揭示了局限。规模化有效，是因为大模型提供了更多“彩票”，更多找到最优解的机会。但这一机制也暗示了自然界的边界。随着网络越来越擅长找到最简解，继续扩大规模的收益会递减。
- 这与专家对当前方法极限的担忧一致。Yann LeCun认为，根本的架构限制可能阻碍语言模型实现真正理解，无论规模多大。彩票票据机制解释了当前的成功，也暗示了未来的挑战。

  

**框架与心智模型**

- **偏差****-****方差权衡**（Bias-Variance Tradeoff）：模型需在拟合数据与保持简洁之间取得平衡，避免过度复杂导致记忆噪声。
- **双重下降现象**（Double Descent）：模型规模扩大后，误差曲线出现两次下降，第二次下降源于模型在庞大参数空间中找到更优解。
- **彩票票据假说**（Lottery Ticket Hypothesis）：大模型通过随机初始化，内含大量潜在子网络，每个都是一张“彩票”。训练过程就是筛选出最优初始化的子网络，其余被淘汰。
- **奥卡姆剃刀**（Occam’s razor）：最简单的解释最优，大模型只是更高效地找到这些解释。
- **科学进步的心智模型**：突破理论边界、勇于实证测试，往往能带来意想不到的发现。

  

**基本信息**

- Title: How AI researchers accidentally discovered that everything they thought about learning was wrong
- Author: NearlyRight
- Date: 18 Aug, 2025
- URL: https://nearlyright.com/how-ai-researchers-accidentally-discovered-that-everything-they-thought-about-learning-was-wrong/