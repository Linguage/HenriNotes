为什么我讨厌 AI，也许你也应该如此

  

**概述**

  

本文作者以技术专家的身份，系统阐述了自己对当前 AI（尤其是生成式大模型，LLM）热潮的批判态度。他认为，AI行业目前处于泡沫阶段，技术本身的实际价值远低于市场炒作和资本投入。作者并不否认 AI 的潜力，但坚信 LLM 并非通向 AGI（通用人工智能）的正确路径。文章详细分析了大公司为何押注 AI、行业的“炒作经济学”、LLM 的技术局限、对人类认知与技能的潜在负面影响，以及为何“早期入场”并非明智之举。最终，作者主张理性对待 AI，强调专业技能的重要性，并预测未来 AI 发展将更多依赖人类专家的参与，而非完全替代。

  

**AI** **泡沫与行业炒作**

- 作者指出，生成式 AI（Generative AI）目前处于明显的泡沫期。虽然技术本身有发展空间，但市场炒作远超实际价值。
- 他总结了三种主流观点：一是技术不可持续，泡沫很快会破裂；二是未来可能有突破，但目前主要靠营销维持热度；三是技术在少数领域有价值，其他大多是炒作。
- 作者以金融危机为例，说明即使行业巨头也会因短视和贪婪而犯错。AI 行业目前的疯狂投入，类似于历史上的“热土豆游戏”，短期看似繁荣，长期风险巨大。
- 他认为，大公司押注 AI 并非愚蠢，而是出于对 AGI 的“生存威胁”——一旦有公司实现 AGI，其他公司将被彻底淘汰。因此，烧钱押注 AI 是合理的“对冲”策略。
- 但在实际操作中，为了安抚投资者，公司不得不通过夸大宣传、制造“AI即将颠覆一切”的幻觉，维持资本流入。这种“假装已接近突破”的行业氛围，导致了大量虚假承诺和炒作。

  

**大公司与** **AI** **的“对冲博弈”**

- AGI（Artificial General Intelligence，通用人工智能）被视为科技公司的最大威胁。谁先实现 AGI，谁就能在几乎所有领域取得压倒性优势。
- 因此，科技巨头们不惜投入数百亿美元研发 AI，即使失败也无所谓，因为大家都在烧钱，没人会落后。
- 但如果 AGI 真被实现，市场格局将瞬间颠覆，未能跟上的公司将被淘汰。
- 这种“烧钱押注”的逻辑，导致公司在宣传上极力渲染 AI 的潜力，哪怕实际进展有限。
- 作者以特斯拉为例，指出“全自动驾驶明年就来”的承诺持续多年，实际上是为了维持投资者信心。大公司同样通过“AI即将觉醒”等言论，延续资本投入。
- 他认为，外部很难分辨哪些公司真的相信 AGI即将到来，哪些只是炒作。根据目前的行业表现，他更倾向于后者。

  

**炒作的下游效应：全民“** **AI** **焦虑”**

- 大公司不断发布夸张的 AI 宣传，导致中小企业和个人也开始焦虑：“我们该如何应对 AI？”
- 但作者认为，除非拥有巨额资金自建模型，否则大多数企业只能成为大公司 AI 服务的客户。
- LLM 的研发和部署成本极高，形成了“围墙花园”，大公司可以通过订阅模式垄断市场。
- 在炒作氛围下，各类产品都开始“AI化”，哪怕只是简单地嵌入一个聊天机器人。
- 同时，行业内出现了大量“投机者”，他们在每一波新技术热潮中迅速获利后离场。作者讽刺地指出，许多 AI 网红此前还在推销 NFT 或区块链产品。

  

**苹果的“冷静”策略**

- 作者对苹果公司的运营风格表示赞赏，认为其在 AI 领域表现出罕见的理性和克制。
- 苹果并未盲目跟风，而是通过内部研究和谨慎投资，保持技术探索的主动权。
- 苹果发布的两项 LLM 研究（2024年10月和2025年6月）均指出，LLM 并不具备真正的“推理”能力，只是统计模式匹配。
- 因此，苹果公开持保守态度，认为 LLM 不可能成为 AGI。
- 虽然苹果仍在进行相关研究，但并未像其他公司那样“all in” LLM 热潮。这种理性策略反而引发了投资者的不满，认为苹果“没有做AI”。
- 作者讽刺地指出，当前行业对“AI”的定义极为狭窄，只有将 LLM 强行嵌入各类产品才算“做AI”，而传统的语音识别、图像分类等都不被认可。

  

**LLM** **并非通向** **AGI** **的道路**

- 作者与苹果的结论一致：LLM 不是，也永远不会成为 AGI。
- 他强调，LLM 的“推理”能力一直备受争议。由于模型本身是“黑箱”，连开发者都无法解释其输出机制。
- LLM 与人脑类似，单个神经元可以解释，但整体行为难以理解。即使顶级神经科学家也无法完全解释人脑的工作原理。
- “Stochastic Parrot”（随机鹦鹉）和“Chinese room”（中文房间）等理论，质疑 LLM 是否真的具备推理能力。作者认为，现有 LLM 的所有能力都可以用更简单的统计现象解释，远未达到“思考”或“推理”。
- 他举例说明，LLM 能够完美复现人类语言，是因为其训练数据本身就是人类产出。就像复印机能复制文章，但没人会认为复印机会思考。

  

**真正的推理** **vs.** **统计模式匹配**

- 作者认为，真正的推理能力体现在面对全新问题时的表现。
- LLM 支持者常说模型“像人类一样学习”，但作者指出，自己作为研究者，能够在无人涉足的领域进行原创研究，而 LLM 无法做到。
- LLM 在有大量训练数据的问题上表现优异，但在没有现成答案的新问题上则完全失效。
- 他举例“狼、羊和白菜过河”问题，LLM 能轻松回答标准版本，但只要稍作变动（如将狼换成狮子），模型就会输出错误答案。
- 人类能灵活应对变体，而 LLM 只能机械匹配训练数据。即使开发者通过算法修复部分问题，本质上还是在为特定场景构建子模型，而非提升整体推理能力。
- 作者认为，这种做法实际上是回归传统的“问题特定模型”，而不是通用智能。

  

**LLM** **技术的瓶颈与“补丁式创新”**

- 作者认为，LLM 技术已经或即将触及天花板。增加数据、参数和 token 数量已无法带来显著提升。
- 近期的技术进展更像是“补丁”而非真正的创新。

  

**Chain-of-Thought Reasoning** **（思维链推理）**

- CoT 让 LLM 将问题拆解为多个步骤，理论上能减少“幻觉”现象。
- 但这种方法依赖模型能正确拆解问题，且每一步都不出错，计算成本极高。
- 对大多数用户而言，LLM 的吸引力在于快速获得答案，没人愿意等待模型“烧脑”数分钟。
- 作者认为，这只是用更多 LLM 解决 LLM 的根本缺陷，并非技术本质的进步。

  

**Retrieval-Augmented Generation** **（检索增强生成，** **RAG** **）**

- RAG 允许 LLM 实时检索互联网数据，解决训练数据过时的问题。
- 作者批评 RAG 本质上是“高级抄袭”，只是用 LLM 的语言能力对新闻、博客或论文进行重述，剥夺了原作者的流量和收入。
- RAG 依然无法解决“幻觉”问题，且在信息源稀缺或需要额外背景知识时表现不佳。
- 作者举例，自己曾用 LLM 自动交易股票，结果因模型未能检索最新数据而导致错误决策。

  

**AI** **替代恐惧与冲动决策**

  

**行业现状**

- 作者指出，当前科技行业的裁员潮前所未有，许多技术人才被迫转行，招聘市场极度紧张。
- 大众普遍将失业归咎于 AI 替代，虽然实际原因更复杂，但公司乐于利用这种恐惧炒作 AI 产品。
- 恐惧驱使人们做出非理性决策，成为 AI 采纳的主要动力。

  

**“** **早期入场”并非优势**

- 作者批判“先发优势”神话，认为新技术初期往往问题重重，早期入场风险极高。
- 他举例，Google 并非最早的搜索引擎，苹果 iPhone 也不是最早的智能手机，特斯拉更是电动车领域的“迟到者”，但都因后发优势取得成功。
- 作者主张“专业性迟到”，只有技术成熟、市场验证后再采纳，避免盲目跟风。

  

**个人选择**

- 作者自述，自己始终选择在技术成熟后再入场，避免因炒作而损失。
- 他仍然使用 LLM 并关注行业动态，但无意转型为“AI专家”。
- 认为当前 LLM 领域过度饱和，自己更愿意在现有技术的前沿深耕。

  

**LLM** **的“自我吞噬”与信息生态危机**

  

**源数据枯竭**

- 作者认为，LLM 依赖大量人类数据进行训练和检索，但当模型通过“抄袭”剥夺内容创作者收入时，内容生产者会转向付费墙，限制了模型未来的数据来源。
- 这不仅影响 LLM 的发展，也损害了普通用户的信息获取。
- 当前大公司利用开放互联网进行大规模版权侵权，但尚未承担信息生态被破坏的后果。

  

**AI** **低质内容泛滥**

- LLM 产出的“AI垃圾内容”充斥网络，降低了整体信息质量。
- 低质内容进入训练集，可能导致模型“崩溃”，形成恶性循环。

  

**虚假生产力与** **LLM** **成瘾**

- 作者指出，LLM 通过快速总结和处理他人工作，给用户带来“完成任务”的快感，类似于即时满足机制。
- 这种机制与药物成瘾类似，频繁的“多巴胺奖励”让人误以为自己变得更高效。
- 他引用 Adderall（治疗 ADHD 的药物）相关研究，说明药物能提升主观生产力，但客观效率反而下降。
- LLM 也有类似效果，用户感觉自己更高效，但实际产出质量下降。
- 作者怀疑，LLM 用户的“生产力提升”更多是多巴胺驱动的错觉，而非真实进步。

  

**“** **你不会被** **AI** **替代，而是被会用** **AI** **的员工替代”？**

- 作者批判“AI加速员工”论，认为“提示工程”并非真正的技能。
- 真正的优势在于专业领域知识，而不是“如何用 AI”。
- 过度依赖 AI 反而导致技能退化，成为最容易被替代的人群。
- 他强调，专业技能的积累无法速成，而“提示工程”随时都能学。

  

**LLM** **过度依赖与认知退化**

- 作者认为，LLM 类似于对互联网的“有损压缩”，丢失了大量细节和深度。
- 用户如果缺乏对任务的深入理解，无法分辨模型遗漏或捏造的信息。
- 他预测，未来会有大量研究证明过度使用 LLM 会导致认知能力显著下降。
- 大脑如同肌肉，需要不断锻炼。LLM 让人们在各类技能上都变得懒惰，导致知识和逻辑能力退化。
- 维持技能不仅需要定期接触，还要通过多种方式互动和应用。LLM 让人们把这些过程都外包出去，最终损害自身能力。

  

**框架与心智模型**

  

**1.** **理性技术采纳框架**

- 评估新技术时，优先关注实际价值而非市场炒作。
- 观察行业巨头的行为，但不盲目跟风，分析其背后的动机（如对 AGI 的“对冲”）。
- 技术成熟后再采纳，避免“早期入场”带来的高风险。
- 持续关注技术进展，但保持独立判断，避免被炒作和恐惧驱动。

  

**2. LLM** **技术局限心智模型**

- LLM 本质是统计模式匹配，缺乏真正推理能力。
- 依赖大量人类数据，无法进行原创研究或解决全新问题。
- 技术进步多为“补丁式创新”，难以突破核心瓶颈。
- 过度依赖 LLM 会导致认知和技能退化，需警惕“虚假生产力”陷阱。

  

**3.** **信息生态保护框架**

- 关注 LLM 对内容生产者和信息生态的负面影响。
- 支持高质量内容创作，警惕 AI 低质内容泛滥。
- 维护开放、健康的信息环境，避免模型“自我吞噬”。

  

**基本信息**

- Title: Every Reason Why I Hate AI and You Should Too
- Author: MalwareTech
- URL: https://malwaretech.com/2025/08/every-reason-why-i-hate-ai.html