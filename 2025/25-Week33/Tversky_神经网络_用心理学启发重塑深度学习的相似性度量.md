# Tversky 神经网络：用心理学启发重塑深度学习的相似性度量

## 概述

本文介绍了一种全新的神经网络架构——Tversky 神经网络（Tversky Neural Networks），其核心创新在于用 Tversky 相似性函数替代了传统的点积（dot product）或余弦相似度（cosine similarity）作为神经网络中衡量“相似性”的基本方式。作者通过将心理学中的 Tversky 特征匹配模型（feature matching model）转化为可微分的形式，使其能够与现代深度学习的梯度优化机制兼容。实验表明，这一新方法不仅提升了模型的表现，还带来了更强的可解释性，并在一定程度上印证了心理学理论的有效性。

## 主题梳理

### 1. 传统神经网络的相似性假设与局限

- 现代深度学习架构（如 CNN、Transformer）普遍假设“相似性”可以用几何方式度量，常用点积或余弦相似度。
    
- 这种度量方式虽然计算高效，但与人类的相似性判断存在本质差异。心理学家 Amos Tversky 在 1977 年指出，人类的相似性判断往往是非对称的（如“儿子像父亲”比“父亲像儿子”更常见），而几何模型无法表达这种非对称性。
    
- Tversky 提出了“特征匹配模型”，即相似性是对象间共有特征与各自独特特征的函数。但该模型依赖离散集合操作，难以直接应用于需要可微分优化的神经网络。
    

### 2. 可微分 Tversky 相似性函数的提出

- 作者创新性地将 Tversky 相似性转化为可微分形式。具体做法是：每个对象既是向量（R^d 维），又是特征集合（feature set）。
    
- 特征集合的定义：若对象向量与特征向量的点积为正，则该特征“存在”于该对象中。
    
- 这样，传统的集合交集（intersection）和差集（difference）操作被重写为可微分函数。
    
- Tversky 相似性函数公式如下：
    
    S(a, b) = θf(A ∩ B) − αf(A − B) − βf(B − A)
    
    - A、B 分别为对象 a、b 的特征集合
        
    - θ、α、β 为可学习参数
        
    - f(·) 表示对特征的聚合函数
        
- 具体实现细节：
    
    - **特征显著性（Salience）**：对象 A 的显著性为其所有“存在”特征的正向点积之和。显著性低的对象（如“儿子”）更容易被判定为与显著性高的对象（如“父亲”）相似。
        
    - **交集（Intersection）**：通过函数 Ψ 聚合两个对象共有的特征。Ψ 可选 min、max、product、mean、gmean、softmin 等。
        
    - **差集（Difference）**：有两种方式。`ignorematch` 只考虑 A 有而 B 没有的特征；`subtractmatch` 还考虑两者共有但在 A 中更显著的特征。
        

### 3. Tversky 神经网络的结构与表达能力

- Tversky 神经网络引入了两种新层：
    
    - **Tversky 相似性层（Tversky Similarity Layer）**：类似于传统的点积或余弦相似度层，但用上述 Tversky 相似性函数替代，输出为标量。
        
    - **Tversky 投影层（Tversky Projection Layer）**：类似于全连接层。输入向量与一组“原型向量”（prototypes）计算 Tversky 相似性，输出为 R^p 维向量（p 为原型数量）。
        
- 可学习参数包括：
    
    - 原型向量 Π
        
    - 特征向量 Ω
        
    - 权重 α、β、θ
        
    - Π 和 Ω 可在不同层间共享
        
- 表达能力提升：作者证明单个 Tversky 投影层可以拟合非线性 XOR 函数（传统单层线性网络无法实现），但参数数量较多（11 个），与传统两层网络（6-9 个参数）相比优势有限。
    
- 参数初始化和超参数选择对模型表现有影响，不同初始化结果差异较大，原型和特征向量数量需手动设定。
    

### 4. 实验结果与可解释性

- **图像识别**：在冻结 ResNet-50 的情况下，将输出层替换为 Tversky 投影层（Tversky-ResNet-50），NABirds 数据集准确率从 36.0% 提升到 44.9%，MNIST 从 57.4% 提升到 62.3%。在非冻结或从零训练时提升不明显或略有提升。
    
- **语言建模**：在 Penn Treebank 数据集上，从零训练 GPT-2 small，使用 Tversky 层（TverskyGPT-2）可同时降低 7.5% 的困惑度（perplexity），并减少 34.8% 的参数量。
    
- **可解释性**：Tversky 框架本身即具备可解释性。其基本操作基于“共有特征”和“独特特征”，天然适合解释模型决策。作者提出了新的可视化方法，能在输入空间中直接展示模型学到的原型和特征。例如在 MNIST 上，Tversky 层学到的原型和特征对应于可辨识的笔画和曲线，而传统线性层学到的则是难以解释的纹理模式。
    
- **心理学一致性**：实验发现，训练后模型普遍学到 α > β，即输入的独特特征（如“儿子”）权重高于原型的独特特征（如“父亲”），这与 Tversky 的“显著性假说”一致。
    

### 5. 局限与展望

- Tversky 神经网络的表达能力虽有提升，但参数量和初始化敏感性带来实际应用上的挑战。
    
- 目前仅在投影层进行了实验，尚未扩展到 Transformer 或注意力机制（attention）。
    
- 该方法为模型可解释性和心理学理论验证提供了新思路，值得进一步探索。
    

## 框架与心智模型（Framework & Mindset）

- **特征匹配心智模型**：将对象视为特征集合，相似性由共有特征和各自独特特征共同决定，且权重可调。
    
- **可微分集合操作**：用向量空间中的点积和聚合函数，将离散集合操作转化为可微分形式，兼容深度学习优化。
    
- **原型投影思路**：通过与一组可学习原型的相似性投影，实现更强的非线性表达能力。
    
- **可解释性优先**：模型结构本身即具备解释能力，决策过程可用“共有/独特特征”语言描述，便于人类理解和分析。
    
- **心理学与机器学习结合**：用心理学理论指导神经网络设计，并通过实验验证理论假设（如显著性假说）。
    

## 基本信息

- Title: Tversky Neural Networks
    
- Author: Moussa Koulako Bala Doumbouya, Dan Jurafsky, Christopher D. Manning
    
- Date: 2024 年 6 月（论文发布时间，原文未注明具体日期）
    
- URL: https://gonzoml.substack.com/p/tversky-neural-networks
    
- 论文链接: https://arxiv.org/abs/2506.11035