**人工智能时代下的科研诚信导航：透明与开放数据的责任**

## I. 双重挑战：人工智能驱动的虚假信息与持续存在的学术不端行为

学术界面临着双重挑战：新兴的人工智能（AI）技术带来了信息可靠性的新问题，而长期存在的学术不端行为持续侵蚀着科研的基石。用户对AI研究工具的担忧，以及对学术造假可能因AI而加剧的恐惧，凸显了对科研诚信的深切关注。这些问题的核心在于，如果构成科研基础的信息本身就不可靠，那么整个科学事业的可信度都将受到威胁。因此，开放科学实践，特别是数据和代码的透明化，正成为应对这些挑战的关键。

### A. 科研中的人工智能：前景与风险

人工智能在科研领域的应用展现出巨大潜力，例如加速数据分析、模式识别和文献检索。然而，这种潜力伴随着显著的风险，尤其是在信息准确性和可靠性方面。

#### 对AI研究工具的担忧：验证AI生成的引用和内容

用户对“深度研究”（Deep Research）等AI工具提供引用链接但无法验证其可靠性的担忧，反映了对AI生成信息可信度的普遍疑虑。尽管AI能够快速整合信息并提供文献索引，但其输出内容的质量和真实性是一个核心问题。生成式AI工具可能会“幻觉”或编造完全没有根据的信息，包括虚假的参考文献¹。这使得人工核查成为必要步骤，例如，将AI生成的引文与图书馆搜索引擎（如NUsearch）中的记录进行比对，以确认作者、标题、日期等信息的准确性³。这种挑战源于AI在当前发展阶段通常缺乏真正的理解和批判性评估能力，其性能高度依赖于训练数据的质量⁴。如果训练数据本身存在偏见或不准确（正如用户担心的那样，互联网上充斥着质量参差不齐的内容），AI的输出结果也可能继承这些缺陷。AI模型在生成文本时，可能无法可靠地区分有偏见和无偏见的材料⁷。因此，若不经过人类专家的严格审查，AI的输出内容可能并不可靠。

AI研究工具内部缺乏健全、普适的验证机制，加之AI生成内容（AIGC）易于传播，这为学术信息生态系统的污染提供了直接途径。用户担心AI工具提供不可靠的链接，而研究表明AI确实能够捏造信息，并且其训练数据来源于庞大且未经严格筛选的互联网。虽然可以进行人工验证³，但这非常耗时。如果AI工具没有内置可靠的验证功能，而用户（尤其是在追求效率的情况下）未能仔细核查，那么未经证实甚至虚假的信息就会轻易扩散。这直接回应了用户对于AI“胡编乱造”的内容若充斥互联网，将难以与真实研究区分的担忧。

目前对AI生成内容的人工核查方法，在规模化应用时是不可持续的。随着AI在研究中的应用日益广泛⁶，需要验证的AI生成内容量将爆炸性增长。研究人员在时间压力下，对AI工具提供的每一条引文或信息都进行人工核查是不切实际的。这意味着，如果没有更先进的自动化验证技术，错误和捏造的内容混入研究成果的风险将显著增加，这可能削弱依赖AI辅助的研究成果的可信度，并指向开发自动化、可靠的信任度量或AI研究工具验证层的迫切需求。

#### AI生成捏造内容与“幻觉”的幽灵

AI模型产生看似合理但完全错误或无意义信息的现象，即“幻觉”，构成了重大风险。AI生成的文本即使不正确，也可能显得非常自信和权威²，这使得辨别其真伪尤为困难。问题不仅限于文本，还可能扩展到伪造数据、误导性结论，甚至虚假的同行评审意见⁸。这直接触及了用户对AI可能纯粹“胡编乱造”的恐惧。一些AI模型如何得出结论的过程缺乏透明度（即“黑箱”特性），进一步增加了评估其可靠性的复杂性。AI生成内容（AIGC）的多种形式，如深度伪造的图像和视频、包含虚构数据和无法验证数据的AI生成虚假研究论文，以及AI文本中的“幻觉”事实，都对学术界构成了独特的挑战⁶。研究表明，当前的生成式AI，如ChatGPT，确实可能产生虚假和不准确的信息²。

AI生成文本的自信和说服力，即便其内容在事实上是错误的或纯属捏造，也对研究人员构成了显著的心理挑战。研究人员在压力之下，可能更容易相信那些听起来合理的输出，尤其是当他们期望通过AI加速工作进程时²。人类容易受到确认偏误的影响，倾向于信任符合自身预期或以权威方式呈现的信息。这暗示了培养针对AI输出的批判性数字素养的必要性。

AI生成完整虚假研究论文的潜力⁶，可能催生一种新的、更隐蔽的“论文工厂”形式，使得辨别真实研究与欺诈性的AI生成研究变得更加困难。这直接关联到用户的核心关切：当互联网充斥着AI生成（且可能是虚假的）内容时，如何信任研究？传统的论文工厂已然存在，若AI进一步赋能此类活动，其规模和复杂性可能急剧扩大，从而加剧用户所指出的问题。

### B. 军备竞赛：AI生成内容的检测与规避

学术界面临着一场持续的“军备竞赛”：一方面是开发旨在识别AI生成内容（AIGC）的工具，另一方面则是不断出现用以规避这些检测的方法。尽管存在多种检测AIGC的策略，如元数据标准、数字水印、AI注册系统⁸、同行评审以及AI文本分类器¹⁰，但它们的可靠性常常受到质疑。尤其是在面对复杂的规避技术时，例如文本改写或提示工程（prompt engineering），这些工具的效能大打折扣。

有报告指出，一些检测工具准确性不高，甚至会导致假阳性结果，将人类撰写的文本错误地标记为AI生成⁹。大型语言模型（LLM）的快速进步意味着它们生成的文本越来越接近人类写作风格，使得检测难度与日俱增⁶。例如，用于检测GPT-3.5的工具可能难以应对更高级的GPT-4模型⁶。

AIGC检测方法主要包括：基于水印的方法，在AIGC创建过程中嵌入难以察觉的签名；基于零样本学习的方法，利用预训练模型的语言模式识别能力；以及基于训练分类器的方法，通过机器学习模型区分AI生成文本和人类书写文本⁶。尽管某些工具在特定、简化的数据集（主要包含直接由ChatGPT生成的文本）上取得了较高的准确率（如Detect-GPT的AUROC达到0.95，Fast-DetectGPT达到0.98，ArguGPT准确率约90%）⁶，但这并不能完全反映真实世界中复杂AI和规避技术普遍存在的情况。

编辑和审稿人可以采用多种方法辅助识别AI撰写的论文，例如关注异常的语言模式、仔细审查参考文献和引文、将提交稿件与已知的AI文献进行比对、核查图表数据的准确性、验证作者身份以及请求提供AI模型代码和原始数据等¹⁰。然而，这些方法也面临挑战，包括审稿人对AI最新发展认知不足、审稿时间有限以及缺乏专业检测工具等¹⁰。

AIGC检测与规避技术之间的发展呈现出一种持续的“军备竞赛”态势。随着检测工具的改进，规避手段也在不断升级⁵。例如，文本改写、词句替换、优化对LLM的提示，甚至添加微小修改（如空格）都可能有效规避检测⁵。这表明，单纯依赖技术手段来识别AIGC可能始终是一种被动且不完善的策略。

AIGC检测工具的不可靠性和潜在的假阳性问题⁹，可能导致对学术不端行为的错误指控，损害个人声誉并制造不信任的氛围。这种情况尤其可能对非英语母语者或写作风格独特的学者产生不成比例的负面影响。如果机构或期刊过度依赖这些不完善的检测器，无辜的个体可能受到惩罚，这不仅涉及严重的伦理问题，还可能扼杀学术表达的多样性。

对AIGC检测的过度关注，可能会分散对研究质量和诚信等更根本问题的注意力。无论内容由AI生成还是人类撰写，其准确性、原创性以及对知识的贡献应是首要考量。AI生成的内容往往缺乏人类作者所能带来的创造性火花和独特视角⁶，这本身也是一种质量问题，超越了简单的来源检测。因此，在应对AIGC挑战时，培养批判性思维和严谨的研究文化，比仅仅关注内容来源更为重要。

### C. 学术不端：一个持续存在的祸害

学术不端行为，如同一个难以根除的顽疾，长期困扰着科学界。它不仅破坏了知识的根基，也侵蚀了公众对科学的信任。

#### 形式、普遍性与学科差异

学术不端行为表现为多种形式，传统的包括剽窃、篡改、伪造、不当署名、重复发表等¹¹。近年来，随着技术的发展，又出现了新的不端形式，如伪造同行评审、利用“论文工厂”批量生产虚假论文，以及滥用AI生成内容（AIGC）等¹²。

关于学术不端的普遍性，数据揭示了一个令人不安的画面。一项综合分析显示，约有1.97%的科学家承认至少有过一次伪造、篡改或修改数据或结果的严重不端行为，而高达33.7%的科学家承认有过其他可疑的研究实践¹³。这些自我报告的数据可能只是冰山一角，许多不端行为从未被发现¹⁵。

学术不端的发生率在不同学科之间存在显著差异。例如，在电气工程、电子与计算机科学领域（EE & Comp Sci），因学术不端导致的撤稿率（AMR）高达每万篇论文17.4篇，是物理学领域的10倍。临床与生命科学（Clin & Life Sci）领域虽然撤稿总数最高（12,565篇），但其AMR率为8.9¹²。几乎所有的细分研究主题中都存在学术不端行为，其中一些特定主题，如微小RNA和长链非编码RNA（mlncRNA）、免疫学、分子与细胞生物学（癌症、自噬和凋亡相关）以及电信等领域，是学术不端的高发区¹²。相比之下，数学、物理学和化学等学科因学术不端导致的撤稿比例则显著低于其发表量¹²。

学术不端并非一个同质化的问题；其表现形式和发生率高度依赖于具体情境，随学科乃至特定研究主题的不同而显著变化。这意味着，试图用“一刀切”的方法来打击学术不端行为是不太可能奏效的。不同学科面临的压力、存在的机会或规范的差异，可能导致了不端行为发生率的不同。因此，干预措施需要针对每个学科的具体脆弱性进行定制。

新兴的不端行为形式，如伪造同行评审和论文工厂¹²，通常借助数字技术实现。这表明学术不端的方法正随着技术进步而演变，给维护科研诚信带来了新的挑战。用户对AIGC的担忧，正是这种不断演变的威胁格局的一个典型例证。随着研究越来越依赖数字平台和工具（包括AI），新的不端行为途径将会出现，需要采取前瞻性策略来预测和减轻这些风险。

**表1：学术不端行为类型、普遍性及学科热点概述**

| 不端行为类型                                               | 描述                                                               | 估计的普遍性/撤稿贡献 (基于文献)                                     | 主要受影响的学科/主题 (基于文献)                                                                                                 |
| :--------------------------------------------------------- | :----------------------------------------------------------------- | :------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------- |
| **剽窃 (Plagiarism)**                                      | 将他人的工作、想法或文字据为己有，未进行适当署名¹¹                   | 占撤稿原因的16%（一项研究数据）¹²；普遍存在                              | 各学科均存在                                                                                                                     |
| **伪造 (Fabrication)**                                     | 编造数据或结果并记录或报告¹¹                                       | 约1.97%的科学家承认有过此类行为¹³                                    | 临床与生命科学，工程学等                                                                                                           |
| **篡改 (Falsification)**                                   | 操纵研究材料、设备或过程，或更改、删除数据或结果，导致研究记录不能准确反映研究情况¹¹ | 同上¹³                                                               | 临床与生命科学，工程学等                                                                                                           |
| **伪造同行评审 (Fake Peer Review)**                          | 提交虚假的审稿人信息或操控审稿过程                                   | 占所有不端行为原因的45.6%（一项研究数据）¹²                            | 电气工程、电子与计算机科学（如电信、安全系统、计算机视觉与图形学、人工智能与机器学习），教育与教育研究¹²                     |
| **论文工厂 (Paper Mills)**                                 | 以盈利为目的，有组织地生产和销售虚假或低质量论文                     | 占撤稿原因的8.5%（一项研究数据）¹²；2022年高达2%的出版物可能来自论文工厂¹⁵ | 电气工程、电子与计算机科学，临床与生命科学中的特定主题¹²                                                                             |
| **AI生成内容滥用 (AIGC Misuse)**                           | 不当使用AI工具生成学术内容，如未披露使用、将AI列为作者、或利用AI生成虚假内容 | 占不端行为原因的4.4%（一项研究数据）¹²；是一个新兴的担忧                 | 尚在观察，但可能影响所有依赖文本生成的领域                                                                                             |
| **其他可疑研究实践 (Questionable Research Practices, QRPs)** | 包括选择性报告、p值操纵等不符合最佳实践的行为                        | 高达33.7%的科学家承认有过QRPs¹³                                      | 各学科均存在，生物医学领域关注较多                                                                                                   |

#### 学术不端对科学进步和公众信任的冲击

学术不端行为的后果是深远的，它不仅损害了个体研究者的声誉，更动摇了科学知识的根基，破坏了整个科学界的公信力¹⁶。当含有缺陷或已被撤稿的论文继续被引用时，新的知识便可能建立在不可靠的基础之上¹⁴，这直接呼应了用户对于新发表文章在引用了造假论文后其可信度如何保证的担忧。如果新研究的基础本身就是虚假的，那么后续的科学大厦将摇摇欲坠。生成式AI能够轻易地创造文本、图像和数据，这进一步加剧了人们对科学文献可信度的担忧，担心充斥着难以辨别的虚假图表、手稿和结论²³。这种担忧并非空穴来风，因为AI生成的内容可能看似合理，实则充满了“幻觉”或捏造的事实²。

学术不端的影响远超个体学者或单篇撤稿论文的范畴。它对整个研究生态系统具有腐蚀作用，可能导致研究走向死胡同，或迫使科研人员花费大量精力去重复验证本应可靠的研究，从而延缓创新步伐¹⁴。如果研究者在不知情的情况下基于欺诈性工作进行研究，他们的努力可能付诸东流，科研资源也会被错误分配。这不仅减缓了真正的科学进步，也侵蚀了科学共同体内部的信任。

学术欺诈的蔓延，特别是如果被AIGC放大，会严重损害公众对科学的信任¹⁵。如果公众认为科学不可靠或充斥着欺骗，那么对科研经费的支持以及基于证据的政策制定都可能受到削弱。当备受瞩目的欺诈案件或广泛的AIGC滥用变得普遍时，公众可能会对科学主张持怀疑态度，这将影响从公共卫生决策到新技术接受度的方方面面。

#### 系统性驱动因素：“要么发表，要么出局”及其他

学术不端行为的产生并非仅仅是个人道德失范的结果，其背后往往存在着深刻的系统性驱动因素。其中，“要么发表，要么出局”（publish or perish）的学术评价体系被广泛认为是催生不端行为的重要推手¹⁷。在这种高压环境下，学者们面临着巨大的发表数量和速度的压力，因为发表记录直接关系到职称晋升、基金获取、学术声誉乃至机构排名¹⁸。这种对“量”的过度追求，往往牺牲了对“质”的关注，使得一些研究者可能为了满足指标而采取不正当手段，如数据造假、选择性报告，甚至求助于论文工厂¹⁷。有研究将发表压力视为一种心理应激源，可能导致研究者伦理决策能力下降，从而采取高风险行为，包括科学不端¹⁸。

除了发表压力，机构间的竞争、对科研经费的争夺，以及某些领域对抗性的学术文化，也可能助长不端行为的滋生¹⁶。在学生层面，无效的学习习惯和时间管理技能也可能成为学术不端的诱因¹⁹。

“要么发表，要么出局”的文化创造了一种高风险的环境，在这种环境中，不道德行为的潜在收益（如快速发表、获得晋升）可能被认为超过了被发现的风险，特别是当研究伦理和负责任研究行为的培训不足时。如果职业发展、资金和机构声望主要与发表指标挂钩，个体可能会感到被迫走捷径或实施欺诈以求生存和成功。

解决学术不端问题，不能仅仅依赖于惩罚个别违规者，更需要对科研评价体系、研究人员培训方式以及学术机构如何营造诚信文化而非单纯追求产出进行系统性改革¹⁸。例如，一些倡议（如《旧金山科研评估宣言》DORA）主张根据研究的质量和社会影响力，而非仅仅基于期刊指标来评估研究成果¹⁷。这表明问题不仅仅在于“坏苹果”，更在于可能无意中鼓励或未能充分阻止不端行为的“坏系统”。因此，解决方案必须针对这些系统性因素。

## II. 开放科学的回应：数据与代码透明化的指令

面对AI带来的新挑战和根深蒂固的学术不端问题，开放科学（Open Science）提供了一条有力的应对路径。通过倡导并实施数据和代码的透明化，开放科学旨在增强研究的可信度、可重复性和整体的科研诚信。

### A. 开放数据与代码的伦理与实践依据

开放数据与代码的核心伦理依据在于其能够促进透明度、可重复性和科学诚信²⁰。科学的本质特征之一是其自我修正能力，而这种能力依赖于研究过程和结果的可验证性。当研究数据和分析代码公开时，其他研究者便有机会审查、验证乃至复制原始研究的结果，从而发现潜在的错误或不端行为，例如p值操纵（p-hacking）²¹。这种开放性不仅有助于减少偏见，还能建立和维护公众对科学研究的信任²⁰。

从实践层面看，公共资助的研究数据通常被视为一种公共产品，理应在最大程度上向公众开放，并施加尽可能少的限制²²。许多出版机构也鼓励开放数据共享，认为这有助于提升透明度并促成研究结果的独立验证²³。

开放数据和代码的伦理责任源于科学的核心价值观——透明和可验证，这对于科学的自我纠错机制和维持公众信任至关重要。科学通过在前人工作的基础上进行建设和纠正错误来进步。如果数据和代码被隐藏，验证将变得困难，错误可能持续存在，欺诈行为也可能不被发现，从而破坏整个科学事业。这直接回应了用户对不可靠研究基础的担忧。

然而，在推动开放性的同时，必须审慎平衡其他伦理义务，特别是保护敏感数据和研究参与者的隐私²⁴。例如，在处理涉及人类受试者的数据时，必须获得适当的知情同意，并对数据进行匿名化处理，以确保共享的合乎伦理²⁰。对完全开放数据可能带来的患者隐私和数据安全风险，以及再识别风险（尤其是在弱势或小规模人群中）的担忧，都是合理的²⁵。这意味着“开放”并非绝对，它需要在具体的伦理框架和健全的数据治理下进行，尤其是在生物医学等敏感领域。

### B. 开放性的益处：增强可重复性、验证性与协作

开放数据与代码为科研带来了多方面的显著益处。首先，它极大地增强了研究结果的可重复性和可验证性²⁶。当其他研究者能够获取原始数据和分析代码时，他们可以尝试重现实验结果，或对数据进行再分析，从而独立验证研究结论的稳健性²¹。这不仅有助于发现和纠正错误，也是识别学术不端行为的有效途径。

其次，开放性可以显著提升科研效率，避免不必要的重复劳动和资源浪费²⁷。当研究数据和成果可以自由获取时，其他研究者可以更容易地在此基础上开展新的研究，而无需从头收集相同的数据。这加速了知识的积累和科学的进步²⁶。

此外，开放数据和代码能够促进更广泛的学术合作²⁵。不同机构、不同学科的研究者可以共享数据资源，共同解决复杂的科学问题。例如，Dryad等平台通过CC0许可豁免确保了数据对全球研究界的无限制访问和重用，从而推动创新与合作²⁵。

开放数据和代码的益处是相互关联的；例如，增强的可重复性直接支持更好的验证，这反过来又建立了信任，并促进了更稳固的合作。如果结果可以被重现（得益于开放数据/代码），它们就更有可能得到验证和信任。可信的研究随后成为合作和进一步发现的更坚实基础。

开放数据和代码直接解决了用户关于在不可靠基础上建立新研究的担忧。如果基础研究的数据和代码是开放的，那么在开展新工作之前，可以更彻底地评估其可靠性。如果“旧”论文的数据和代码可用，研究人员可以尝试复制或重新分析它们，在构建“新”研究之前识别缺陷或欺诈。这提供了一种目前在数据专有时所缺乏的质量控制机制。

### C. 直面共享障碍：迷思、现实与解决方案

尽管开放数据与代码的益处显而易见，但在实践中，研究人员在共享数据时仍面临诸多障碍和顾虑。这些障碍既有观念上的误区，也有现实中的挑战。

常见的顾虑包括担心研究成果被他人“抢先”（scooped）、因涉及敏感信息而产生的伦理困境、害怕失去对数据的控制权、共享过程耗时耗力以及可能产生的费用等²⁸。例如，研究人员可能担心，在他们有机会发表分析结果之前共享数据，会导致其他研究者捷足先登并窃取学术成果²⁸。

更深层次的障碍则源于学术界的文化和评价体系。许多研究人员感到，他们在数据共享方面所付出的努力没有得到应有的学术承认和回报²⁹。在当前的学术评价体系下，数据共享、软件开发或研究计划的预注册等开放科学实践，其价值往往低于传统的期刊论文发表，尤其是在职称评定和基金申请中²⁹。此外，缺乏相关的培训和技术支持、对数据服务不熟悉或不信任、以及对数据所有权和管理的担忧，也阻碍了数据共享的推广³⁰。

许多数据共享的障碍并非纯粹的技术或财务问题，而是深深植根于学术文化、激励结构和研究人员个体焦虑之中。克服这些障碍需要多管齐下的方法，包括政策变革、基础设施建设、培训以及改变对研究贡献的认可和奖励方式。如果机构和资助者强制要求开放数据，却没有提供必要的支持系统，研究人员将被迫独自应对复杂的技术和后勤挑战，这会成为一个主要的抑制因素。

区分关于数据共享的迷思/误解（可以通过信息和更好的工具来解决²⁸）和合法的伦理或实践限制（例如，保护高度敏感的患者数据²⁴）至关重要。政策必须具有足够的灵活性，以适应合理的限制，同时仍促进最大程度的适当开放。例如，对于“被抢先”的担忧，可以通过设置数据访问的禁发期（embargo period）或发表“数据说明”（data note）来缓解²⁸。对于伦理顾虑，则可以通过匿名化、假名化处理或建立受控访问机制来解决敏感数据的共享问题²⁴。

**表2：数据共享的常见障碍与潜在解决方案**

| 障碍/迷思                                      | 现实/解释 (基于文献)                                                | 潜在解决方案/反驳论点 (基于文献)                                                                                                 |
| :--------------------------------------------- | :------------------------------------------------------------------ | :----------------------------------------------------------------------------------------------------------------------------- |
| **担心被“抢先”(Fear of being scooped)**        | 研究者担心在自己发表分析前，他人会利用其数据抢先发表²⁸                  | 设置数据访问禁发期；发表“数据说明”以确立优先权；共享数据可增加引用²⁸                                                               |
| **伦理限制 (Ethical constraints)**             | 涉及人类受试者或敏感信息时，担心违反隐私或保密规定²⁴                  | 获得知情同意；数据匿名化/假名化；建立受控数据访问机制；解释为何不能共享²⁴                                                            |
| **失去数据控制权 (Loss of control)**           | 担心将数据存入存储库后失去对其的控制²⁸                              | 许多存储库允许设置访问限制或禁发期；可选择适当的共享许可协议（如CC BY-NC）²⁸                                                       |
| **耗时耗力 (Time-consuming)**                  | 准备和上传数据需要大量时间和精力²⁸                                    | 制定数据管理计划（DMP）可提前规划，提高效率；一些存储库提供策展服务；长期看可节省回应索取数据请求的时间²⁸                           |
| **费用高昂 (Expensive)**                       | 担心数据存储和共享产生额外费用²⁸                                      | 存在许多免费的数据存储库；可在项目早期预算数据保存费用；数据共享的长期效益值得投资²⁸                                                 |
| **缺乏学术承认/激励 (Lack of credit/incentive)** | 数据共享工作未得到学术评价体系的充分认可²⁹                            | 改革科研评价体系，将数据共享等开放实践纳入考量（如DORA倡议）；资助机构和期刊推广数据引用；提升数据作为一种学术成果的地位²⁹         |
| **缺乏技能/培训 (Lack of skills/training)**    | 研究人员可能缺乏数据管理、策展和发布的专业技能³⁰                      | 提供数据管理培训和支持服务（如图书馆咨询）；开发用户友好的数据共享工具和平台³⁰                                                       |
| **技术与基础设施障碍 (Technical & infrastructural barriers)** | 缺乏合适的存储库、数据标准不一、数据量过大等³⁰                          | 投资建设和维护强大的数据基础设施；推广FAIR原则和通用数据标准；提供可扩展的存储和计算资源³⁰                                         |
| **文化阻力/观念问题 (Cultural resistance/mindset issues)** | 学术界固有的保守文化，对开放共享持怀疑态度；研究者不熟悉“数据服务”概念，或因“学术英雄主义”不愿求助³⁰ | 加强开放科学理念的宣传和教育；领导层示范和推动文化变革；让数据服务更易于被研究者发现和使用³⁰                                   |

## III. 政策图景：开放研究的指令与指南

为了推动科研的透明化和可信度，全球范围内的主要科研利益相关方，包括期刊出版商、科研资助机构和学术机构，正逐步制定并实施关于开放数据和代码共享的政策。这些政策的出现，标志着科研范式正经历深刻变革。

### A. 期刊出版商关于数据和代码共享的政策

众多学术期刊出版商已经认识到开放数据和代码对于提升研究质量和可信度的重要性，并相继出台了相关政策。然而，这些政策在强制性、具体要求以及执行力度上存在差异。

*   **Springer Nature**：要求在其期刊发表的原创研究论文中包含“代码可用性声明”（Code Availability Statement），强烈鼓励公开新开发的代码，部分期刊甚至在同行评审阶段就要求共享代码。对于数据，则要求提供“数据可用性声明”（Data Availability Statement），强烈鼓励公开共享支持性数据集，并对某些社群认可的数据类型实行强制共享³¹ ³²。出版商也承认，在某些情况下（如涉及参与者隐私），可能无法公开共享数据或代码³²。
*   **Elsevier**：采取了相对温和的策略，鼓励和支持研究人员在适当情况下尽早共享数据，并为此提供了Mendeley Data存储库和《Data in Brief》数据期刊等工具。但其政策明确指出，共享研究数据并非强制性的³³。Elsevier也提供了金色开放获取（Gold OA）和绿色开放获取（Green OA）等多种出版选项，并有相应的文章共享政策³⁴。
*   **Wiley**：根据期刊的不同，Wiley实施了从“鼓励数据共享”到“强制数据共享”共四个不同层级的数据共享政策³⁷。他们支持数据引用，并与数据存储库Dryad建立了合作关系，在某些情况下，期刊会为作者通过Dryad共享数据支付费用³⁷。
*   **IEEE**：提倡共享数据和代码，并维护IEEE DataPort作为作者存储数据集的平台。同时，IEEE也鼓励使用Code Ocean、figshare、Zenodo和Dryad等第三方平台³⁶。值得注意的是，并非IEEE DataPort中的所有数据集都是开放获取的，部分可能需要付费订阅才能访问³⁸。
*   **ACM (Association for Computing Machinery)**：正在向完全开放获取的出版模式转型，计划于2025年底前实现³⁹。ACM推出了“ACM Open”计划，供机构参与，以支持其研究人员发表开放获取文章³⁹。作者可以选择合适的知识共享许可协议（Creative Commons licenses），包括CC-0许可用于研究产出物（如数据、代码）⁴⁰。对于其主办的会议（如CIKM），资源类论文必须使用提供DOI的数据共享服务发布数据集，并将代码在GitHub等平台公开⁴¹。ACM的特别兴趣小组SIGCHI也针对会议相关数据制定了详细的数据发布政策，强调透明度和在可能情况下的开放许可⁴²。
*   **Cell Press**：要求作者愿意共享论文中报告的所有数据和原始代码，除非存在法律或伦理上的禁止（例如，涉及机密的医疗记录）⁴³。其出版政策强制要求在研究论文中包含“资源可用性”（Resource Availability）部分，其中必须涵盖“数据和代码可用性”的详细说明⁴⁴。Cell Press强烈鼓励将研究成果数字化存档，并推荐使用合适的存储库⁴⁴。
*   **JAMA Network**：要求作者在其“致谢”或“方法”部分报告使用人工智能、语言模型、机器学习或类似技术创建内容或协助稿件撰写的情况⁴⁵。不鼓励提交和发表由AI创造的内容，除非这是正式研究设计或方法的一部分，并且必须明确说明所创建的内容以及模型/工具的名称；作者必须对这些模型和工具生成的内容的完整性负责⁴⁵。自2023年1月起，JAMA Network允许作者在稿件被接收并发表的当天，将其手稿存入自选的公共存储库⁴⁶。JAMA Network也制定了关于共享原创研究所依赖数据的政策⁴⁶。

尽管各大出版商在推动数据和代码共享方面展现出积极趋势，但政策的异质性依然显著。从“鼓励”到“强制”，从“声明”到“实际共享”，再到“数据同行评审”，不同出版商和期刊的要求各不相同，这无疑给研究人员带来了遵循上的复杂性。然而，一个共同的趋势是，“数据/代码可用性声明”正逐渐成为标准要求。这类声明即便在数据或代码因合理限制未能完全公开共享的情况下，也至少提供了一层透明度，指明了相关材料的获取状态。这标志着向更高透明度迈出的一步，尽管这并不等同于完全的开放。

一个持续存在的挑战是，尽管政策已经出台，但期刊在多大程度上能够或确实去核实共享数据和代码的实际可用性与质量，仍是一个悬而未决的问题。例如，Wiley的政策中，将“数据同行评审”列为最高层级，暗示了其他层级可能并不包含此项³⁷。如果数据仅仅被共享，但文档记录不佳或难以使用，那么政策的益处就会大打折扣。这再次凸显了对强大基础设施和专业数据策展服务的需求。

**表3：主要期刊出版商数据与代码共享政策比较**

| 出版商         | 总体立场                                 | 数据可用性声明(DAS)要求              | 代码可用性声明(CAS)要求              | 数据共享政策                                               | 代码共享政策                                               | 支持的数据/代码存储库                                           | 数据/代码同行评审                                     |
| :------------- | :--------------------------------------- | :----------------------------------- | :----------------------------------- | :--------------------------------------------------------- | :--------------------------------------------------------- | :---------------------------------------------------------- | :---------------------------------------------------- |
| Springer Nature | 强烈鼓励，部分强制³¹ ³²                  | 是³²                                 | 是³¹                                 | 强烈鼓励公开，部分类型强制存入公共存储库³²                 | 强烈鼓励公开，部分期刊强制³¹                               | 推荐Code Ocean, Zenodo等提供永久标识符的存储库³¹            | 部分期刊（如Nature系列）要求提交代码供同行评审；数据评审情况视期刊 |
| Elsevier       | 鼓励，非强制³³                           | 鼓励³³                               | 未明确为普适性要求，视期刊           | 鼓励在适当情况下尽早共享，提供Mendeley Data³³              | 未明确为普适性要求，视期刊                               | Mendeley Data, Data in Brief (数据期刊)³³                   | 否（作为普适性要求）                                      |
| Wiley          | 分四级：鼓励、期望、强制、强制且评审³⁷     | 鼓励/期望/强制（视政策级别）³⁷       | 未明确为普适性要求，但鼓励共享所有研究产物 | 鼓励/期望/强制存入公共存储库（视政策级别）³⁷               | 鼓励公开存档                                               | 支持Dryad等，鼓励使用提供永久链接的存储库³⁷                 | 最高级别政策包含数据同行评审³⁷                            |
| IEEE           | 提倡³⁶                                   | 鼓励（通过数据可用性声明）³⁶           | 提倡³⁶                               | 提倡共享，维护IEEE DataPort³⁶                              | 提倡共享                                                   | IEEE DataPort, Code Ocean, figshare, Zenodo, Dryad³⁶        | 未明确为普适性要求                                      |
| ACM            | 推动全面OA；鼓励³⁹ ⁴⁰                      | 鼓励（通过资源论文要求）⁴¹           | 鼓励（通过资源论文要求）⁴¹           | 资源论文要求使用提供DOI的共享服务发布数据集⁴¹              | 资源论文要求在GitHub等平台公开代码⁴¹                       | Zenodo, Dataverse, GitHub等⁴¹                               | 未明确为普适性要求（但资源论文本身会评审）                    |
| Cell Press     | 强制（除非法律/伦理禁止）⁴³                | 是（作为资源可用性一部分）⁴⁴         | 是（作为资源可用性一部分）⁴⁴         | 必须愿意共享所有数据，鼓励存档⁴⁴                           | 必须愿意共享所有原始代码，鼓励存档⁴⁴                       | 推荐领域特定和通用存储库⁴⁴                                  | 未明确为普适性要求                                      |
| JAMA Network   | 鼓励透明；对AI使用有特定披露要求⁴⁵        | 是（通过公共存储库自愿存放已接收手稿⁴⁶；对原创研究依赖数据有共享政策⁴⁶） | 未明确为普适性要求（但AI工具使用需披露⁴⁵） | 允许作者在发表当天将已接收手稿存入公共存储库⁴⁶；有数据共享政策⁴⁶ | 未明确为普适性要求                                       | 作者自选公共存储库⁴⁶                                      | 未明确为普适性要求                                      |

### B. 科研资助机构对数据管理与共享的要求

主要科研资助机构正日益成为推动开放科学实践的关键力量，它们通过将数据管理与共享计划（DMSP）纳入基金申请流程，对科研人员的研究行为产生深远影响。

*   **美国国立卫生研究院 (NIH)**：自2023年1月25日起，NIH要求所有产生科学数据的资助申请都必须包含一份DMSP⁴⁸。该政策期望研究人员在遵守伦理、法律或技术限制的前提下，最大限度地共享其数据，并明确鼓励将数据存入公共存储库作为标准研究流程的一部分⁴⁹ ⁵⁰。DMSP必须详细说明数据类型、相关工具/软件/代码、采用的标准、数据保存/访问及时间表、访问/分发/重用考量以及数据管理与共享的监督机制⁵⁰。
*   **美国国家科学基金会 (NSF)**：NSF期望其资助的研究人员能够与其他研究者共享原始数据、样本、物理馆藏以及其他在NSF资助工作中产生或收集的支持材料，且成本不应超过增量成本，并应在合理时间内共享⁵²。申请NSF资助时，必须提交一份不超过两页的DMSP，阐述项目将如何遵循NSF关于研究成果传播与共享的政策⁵²。其材料研究部（DMR）尤其强调，现代材料研究期望数据以数字形式存在，并符合FAIR原则（可发现、可访问、可互操作、可重用），同时附有充分的元数据⁵¹。
*   **欧盟“地平线欧洲” (Horizon Europe)**：该计划要求所有在“开放研究欧洲”（Open Research Europe）平台发表文章的作者共享与文章相关的基础数据⁵³。政策强调遵循FAIR原则，使用持久标识符（PID，如DOI），提供丰富的元数据，并将数据存储在可信的存储库中（如欧洲开放科学云EOSC）⁵⁴。
*   **惠康基金会 (Wellcome Trust)**：强制要求其资助的出版物（包括期刊文章、书籍章节和专著）在欧洲PubMed Central (Europe PMC)上即时开放获取，并采用CC BY许可协议⁵⁶。出版物必须包含数据可用性声明，并遵循基金会的数据、软件和材料管理与共享政策⁵⁵。惠康基金会鼓励遵循FAIR原则共享数据⁵⁵。

科研资助机构通过将资金与数据共享指令挂钩，正在成为推动开放科学实践的强大驱动力。这使得数据共享从许多研究人员的自愿行为转变为一项合规要求。DMSP的强制要求，迫使研究人员从项目一开始就考虑数据共享和保存问题，而非事后弥补，这有助于改善数据管理。许多资助机构明确引用或使其政策与FAIR数据原则保持一致，表明对数据质量和可重用性的期望正在趋向标准化，这有助于创建一个更一致的国际数据共享环境。

**表4：主要科研资助机构数据管理计划与共享指令概要**

| 资助机构        | DMSP要求? | DMSP关键要素                                                           | 数据共享指令 (范围, 时间)                                                         | 代码共享指令                                      | 推荐/要求的存储库                                 | FAIR原则立场                      | 违规处罚 (如有提及)                                       |
| :-------------- | :-------- | :--------------------------------------------------------------------- | :-------------------------------------------------------------------------------- | :---------------------------------------------- | :---------------------------------------------- | :-------------------------------- | :-------------------------------------------------------- |
| NIH (美国)      | 是⁵⁰      | 数据类型、工具/软件/代码、标准、保存/访问/时间表、重用考量、监督⁵⁰          | 最大限度共享科学数据；不晚于相关发表或资助期结束（以较早者为准）⁵⁰              | DMSP中需说明相关代码⁵⁰                            | 鼓励使用公共存储库⁵⁰                              | 强调数据共享以促进FAIR⁴⁸            | 未明确提及处罚，但合规性是资助条件                          |
| NSF (美国)      | 是 (最多2页)⁵² | 数据类型、标准、访问/共享政策、重用/再分发政策、存档计划⁵²                   | 期望共享原始数据、样本等支持材料，合理时间内，成本不超过增量⁵²                  | DMSP中需说明相关软件⁵²                            | 未指定特定存储库，但DMR强调数字数据和元数据⁵¹       | DMR期望数据符合FAIR原则⁵¹         | 未明确提及处罚，但DMSP是同行评审的一部分                    |
| Horizon Europe (欧盟) | 是 (作为项目要求的一部分)⁵³ | 遵循FAIR原则，使用PID，丰富元数据，可信存储库⁵³ ⁵⁴                           | Open Research Europe要求作者共享与文章相关的基础数据⁵³                          | 要求共享代码，使用PID，存储于可信存储库⁵³           | 欧洲开放科学云 (EOSC)，领域特定存储库⁵³ ⁵⁴         | 强制遵循FAIR原则⁵³ ⁵⁴             | 未明确提及处罚，但开放科学是项目要求                        |
| Wellcome Trust (英国) | 是 (通过数据、软件和材料管理与共享政策)⁵⁵ | 数据可用性声明；管理与共享计划⁵⁵                                           | 鼓励遵循FAIR原则共享；数据可用性声明必须包含在出版物中⁵⁵                          | 鼓励遵循FAIR原则共享⁵⁵                            | 鼓励使用符合FAIR原则的存储库；Europe PMC用于出版物⁵⁵ | 鼓励遵循FAIR原则⁵⁵                | 未明确提及处罚，但开放获取和数据共享是资助条件              |

### C. 学术机构的框架与支持

大学和研究机构在推动开放科学方面扮演着双重角色：它们不仅制定自身的开放科学政策，还提供必要的基础设施、培训和支持服务（如图书馆的数据管理计划咨询服务），以帮助研究人员遵守机构和资助者的指令。

*   **斯坦福大学 (Stanford University)**：其于2020年11月通过的开放获取政策，主要涉及教员授权斯坦福大学通过斯坦福数字存储库公开其学术文章，并要求教员获取ORCID iD⁴⁷。该政策本身未详细规定数据和代码共享的强制要求，但提及霍华德·休斯医学研究所（HHMI，常与斯坦福研究人员有关联）要求其资助的论文包含数据/代码可用性声明⁴⁷。
*   **剑桥大学 (University of Cambridge)**：制定了《研究数据管理政策框架》，为学术人员提供最佳实践指导。该校期望公共资助的研究数据作为公共产品，以尽可能少的限制开放获取²²。同时指出，一些资助机构已对不遵守其研究数据共享政策的行为规定了处罚措施²²。
*   **麻省理工学院 (MIT)**：在开放共享方面拥有悠久历史（如OpenCourseWare, DSpace）⁵⁷。2017年成立了“MIT研究开放获取特设工作组”，旨在更新针对文章、数据、代码和教育材料的开放获取政策⁵⁷。MIT图书馆的数据管理服务部门协助研究人员制定符合NIH政策的DMSP，并就研究期间的有效数据管理以及数据共享和发表选项提供咨询⁵⁰。其人类受试者使用委员会（COUHES）也制定了关于数据共享的指南，特别强调涉及人类受试者数据的情况，并可能要求签署数据使用协议（DUA）⁵⁸。
*   **牛津大学 (University of Oxford)**：其研究数据管理政策（2023年11月更新）适用于所有参与研究的大学成员⁶⁰。政策规定，研究数据应安全存储、可识别、可检索、准确、完整、可靠，并符合法律和伦理要求，且在可能的情况下能够提供给他人使用⁶⁰。研究人员负责制定DMP，在项目结束后保存并提供适当的数据访问⁵⁹。支持其他产出的数据应存入适当的数据存储库（如ORA或其他存储库，并在ORA中创建元数据记录）⁶⁰。该政策采纳了FAIR原则⁶⁰。

尽管总体趋势是鼓励更广泛的开放，但不同机构的政策侧重点可能有所不同。例如，斯坦福大学早期的开放获取政策更侧重于文章的自存档，而牛津大学则拥有一个与FAIR原则高度一致的全面研究数据管理政策。这反映了不同大学在其使命、资源和研究重点方面的差异。对于那些大量从事生物医学或社会科学研究的机构而言，健全的内部政策和审查委员会（如MIT的COUHES）对于处理共享人类参与者数据时复杂的伦理问题至关重要。这些机构的努力表明，它们正积极参与构建开放科学的生态系统，而不仅仅是被动等待外部指令。

### D. 指导原则与倡议：FAIR、TOP与更广泛的开放科学运动

除了具体的政策指令外，一系列具有指导意义的原则和倡议正在引领全球科研向开放科学转型。

*   **FAIR原则 (Findable, Accessible, Interoperable, Reusable)**：这套原则已成为数据管理与共享的基本准则，旨在最大限度地发挥数据的效用⁶¹。其核心要求是，数据及其元数据应易于被人类和计算机发现；可以通过标准化的通信协议被访问（在必要时进行身份验证和授权）；能够与其他数据集成，并与应用程序或工作流程互操作；以及得到充分描述，以便可以在不同环境中被复制和/或组合⁶¹。尽管FAIR原则的益处（如增强可重用性、支持AI/ML应用）已得到认可，但在实施过程中仍面临挑战，包括实际共享率低、执行力度不足以及数据策展困难等⁶²。
*   **TOP指南 (Transparency and Openness Promotion Guidelines)**：由开放科学中心（Center for Open Science）制定，为期刊和资助机构提供了一个包含八项标准（如引文标准、数据透明度、分析方法（代码）透明度、研究材料透明度、设计和分析透明度、预注册、分析计划预注册、复制）和三个实施层级（披露、要求、验证）的框架，以提升研究过程和成果的透明度⁶³ ⁶⁵。美国心理学会（APA）是众多签署并采纳TOP指南的机构之一⁶⁴。
*   **开放科学运动 (Open Science Movement)**：这是一项全球性的努力，旨在使科学研究及其产出（包括出版物、数据、软件和生物材料等）能够被自由获取、共享和重用，从而促进透明度、合作和包容性，加速科学进步，并确保知识惠及每一个人⁶⁶ ⁶⁷。联合国教科文组织（UNESCO）于2021年通过了《开放科学建议书》，为全球推动开放科学提供了框架⁶⁶。欧洲的“S计划”（Plan S）也是推动出版物即时开放获取的重要举措⁶⁷。

FAIR原则和TOP指南等框架为将开放科学的抽象理念转化为跨学科、跨机构、跨国家的具体实践提供了可操作的标准。它们帮助创建了一种共同语言和一套共同期望，而不仅仅是对“开放性”的模糊认可。与此同时，由UNESCO等国际组织和“S计划”等区域性倡议支持的开放科学运动，标志着国际社会在推动科研更高透明度和可及性方面已形成广泛共识和强劲势头，这给所有利益相关方带来了适应和变革的压力。

这些原则和倡议直接回应了用户对不可靠研究的担忧。通过提升数据、代码和研究方法的透明度（如TOP指南和FAIR原则所倡导的），它们使得科研工作更容易受到审视，从而有助于识别错误或欺诈行为，进而构建一个更可靠的科学记录体系。如果研究遵循这些原则，其他人就更容易核查其工作，这既是对不端行为的威慑，也是一种错误纠正机制。

## IV. 规划未来：在演变的研究生态中培育诚信

面对人工智能技术的飞速发展和开放科学浪潮的持续推进，维护和提升科研诚信需要前瞻性的思考和多方协同的努力。未来的研究生态系统将更加复杂，但也充满了提升透明度和可信度的机遇。

### A. 加强人工智能时代的验证机制

随着AI工具日益融入科研流程，建立和强化新的验证机制变得至关重要。这不仅包括检测AI生成内容（AIGC）的来源，更要验证AI研究所依赖的底层数据和模型的可靠性。有趣的是，AI本身也可以在同行评审过程中发挥积极作用，例如检查稿件的语法、结构、原创性，检测欺诈性研究（如剽窃、图像篡改、数据伪造），甚至识别潜在的利益冲突⁶⁸ ⁶⁹。然而，AI辅助同行评审也存在局限性，并可能引入新的偏见⁶⁹。

将开源原则应用于AI的开发过程，例如公开方法论、数据来源和决策过程，有助于培养信任和问责制⁷⁰。透明的AI系统使用户能够理解决策是如何做出的，从而增强其公平性和协作性⁷⁰。尽管开放源码AI在加速模型开发、促进跨平台集成和定制化方面潜力巨大（如Meta的Llama 2），但也面临着训练数据访问受限和商业使用限制等挑战，显示出“完全开放”的复杂性⁷¹。建立公私合作伙伴关系以进行AI模型验证，被认为是应对这些挑战的一个方向⁶⁹。

AI技术呈现出双重性：它既可能成为虚假信息和学术欺诈的源头，也能够作为增强验证过程（例如在同行评审和欺诈检测中）的有力工具。关键在于负责任地开发和部署用于维护科研诚信的AI工具。用户对AI生成谬误的担忧是合理的，但⁶⁸和⁶⁹的研究表明，AI有潜力发现欺诈行为并提高评审质量。这种二元性意味着科研界需要战略性地利用AI在验证方面的优势，同时努力减轻其风险。

AI模型开发的透明度，包括对训练数据和算法的访问权限（正如开源AI原则所倡导的那样⁷⁰），对于验证AI生成的研究成果和建立对科研AI工具的信任至关重要。用户最初的恐惧部分源于某些AI的“黑箱”特性及其输出的不可靠性。如果AI工具本身更加透明，其输出结果也就更容易得到验证，从而缓解用户的忧虑。

### B. 推动科研探索中合乎道德的AI开发与应用

除了验证机制，为在研究中使用AI制定明确的伦理指南也至关重要。这包括在稿件中清晰披露AI的使用情况⁷²，确保人类对AI生成内容进行监督并承担最终责任⁷²，以及解决AI模型中固有的偏见问题⁵。AI的开发必须优先考虑公平性、隐私保护和问责制⁷⁰。

多数期刊已制定严格的AI使用政策，要求作者披露所使用的工具、方法论以及如何评估生成数据的可信度²³。学术机构和期刊也正在建立相关指南，强调透明度和问责制，明确AI不能作为作者，因为它无法对科研工作的准确性或完整性负责²³。JAMA等出版机构的指南也要求作者报告AI的使用，并对AI生成内容的完整性负责⁴⁵。解决AI训练数据中可能存在的偏见，对于防止错误结论和维护科研公平性至关重要⁵。

无论AI工具多么先进，人类研究者必须始终对研究的诚信、准确性和伦理操守负最终责任，包括任何由AI生成或辅助生成的内容。AI日益强大，人们可能会试图推卸责任，但科研界必须坚持人类问责这一基本伦理原则。

在用于研究的AI工具中主动减轻算法偏见，不仅是一个技术问题，更是一个伦理责任，以确保公平性，防止科学发现和应用中系统性不平等的延续。如果用于医学研究或社会科学分析的AI工具带有偏见，其输出可能导致歧视性结果或强化现有差异的错误结论。这需要在AI开发和部署过程中，从多样化数据源获取、偏见检测和缓解策略等方面做出积极努力。

### C. 弥合实施差距：从开放科学政策到实践

#### 构建稳健且易于访问的数据/代码存储库与基础设施

开放科学的有效实施，高度依赖于稳健且用户友好的基础设施。这包括符合FAIR等原则的通用型、学科专用型和机构型数据存储库⁷³。像GitHub这样的平台对于代码开发非常有用，但为了长期保存和可引用性，需要与Zenodo等存档存储库结合使用³¹。此外，利用持久标识符（如DOI）加强出版物与存储库之间的数据链接也至关重要³²。

开放科学指令的成功在很大程度上取决于数据/代码存储库及相关网络基础设施的可用性、可访问性和易用性。没有这些，政策仍将停留在愿景层面。尽管存在各种存储库⁷³，但研究人员在使用现有数据服务时仍面临挑战³⁰，这表明需要对该基础设施进行持续投资和改进。

为了使数据真正可重用并有效整合知识，存储库及其使用的元数据需要遵守促进互操作性的通用标准。FAIR原则强调互操作性⁶¹。如果数据以各种格式存储在元数据不一致且互不连通的存储库中，其重用和大规模分析的潜力将受到限制。这呼吁加强协调和采用社群标准。

#### 改革科研评价以体现开放实践的价值

一项关键步骤是改革科研评价和激励机制。目前，与传统出版物相比，数据共享和其他开放科学贡献在职称评定和晋升决策中往往被低估²⁹。像DORA（《旧金山科研评估宣言》）这样的倡议主张根据研究的内在价值和更广泛的影响来评估研究，而不仅仅是依据发表指标¹⁷。

除非像数据/代码共享这样的开放科学实践在学术职业发展中得到有意义的认可和奖励，否则无论政策如何，其广泛采用都将是缓慢的。研究人员与大多数专业人士一样，会对激励做出反应。如果他们的职业生涯主要取决于高影响因子期刊的发表，那么被认为是次要的或没有回报的活动（如细致的数据策展和共享）自然会排在较低的优先级。

改革科研评价需要学术机构内部发生根本性的文化转变，并得到资助者和出版商的支持，以拓宽有价值学术成果的定义。这种转变不能仅靠研究人员来推动。机构必须修订其晋升和终身教职指南。资助者可以通过在基金申请中评估这些实践来表明其重要性。出版商可以增强共享数据和代码的可见性和可引用性。这需要所有主要利益相关者的共同努力。

### D. 对研究者、机构、资助者和出版商的建议与未来展望

科研诚信的未来，取决于所有利益相关方能否共同适应并塑造一个日益受到AI影响且向开放获取持续迈进的研究环境。

#### 对研究人员的建议：

*   **批判性评估AI工具**：在使用AI工具进行研究时，保持审慎和批判的态度，主动验证其输出结果的可靠性，尤其是引文和关键数据。
*   **负责任的数据与代码管理**：从项目一开始就规划并实施良好的数据和代码管理实践，遵循FAIR原则，确保研究产出的透明度和可重用性。
*   **积极参与开放科学社群**：加入相关的开放科学社群，学习最佳实践，分享经验，并倡导有利于开放和诚信的机构政策。
*   **伦理先行**：在研究设计和执行中，始终将伦理考量置于首位，特别是在处理敏感数据和使用AI技术时，确保合规并保护参与者权益。

#### 对学术机构的建议：

*   **制定明确的开放科学政策**：出台清晰、可操作的开放科学政策，涵盖数据、代码、出版物和AI使用等方面，并确保政策得到有效传达和执行。
*   **投资基础设施与培训**：投入资源建设和维护稳健的数据/代码存储库及相关基础设施，并为研究人员提供必要的数据管理、开放科学实践和AI伦理方面的培训与支持²²。
*   **改革科研评价体系**：修订职称评定、晋升和奖励标准，将数据共享、代码共享、预注册等开放科学贡献以及在维护科研诚信方面的努力纳入评价范围，给予实质性认可。
*   **培育科研诚信文化**：通过教育、宣传和制度建设，在机构内部营造一种重视科研诚信、鼓励透明公开、容忍建设性质疑的学术文化。

#### 对科研资助机构的建议：

*   **强化并资助DMSP**：继续将数据管理与共享计划（DMSP）作为资助申请的必要条件，并为其实施提供充足的经费支持。
*   **支持开放基础设施发展**：资助开放数据存储库、共享平台以及相关工具和标准的开发与维护。
*   **协调政策，推动标准**：与其他资助机构和国际组织合作，尽可能协调开放科学政策，推动形成国际通用的数据和元数据标准。
*   **倡导科研评价改革**：利用自身影响力，推动学术机构和期刊改革科研评价方法，更全面地衡量研究的影响力和价值。

#### 对期刊出版商的建议：

*   **实施清晰一致的共享政策**：制定并执行明确、一致且易于理解的数据和代码共享政策，并为作者提供清晰的指导。
*   **支持数据/代码的同行评审**：逐步将数据和代码的可用性、质量和文档完整性纳入同行评审流程。
*   **投资检测与预防工具**：继续投资开发和使用能够检测AIGC、图像篡改、剽窃等学术不端行为的工具，同时清醒认识到这些工具的局限性，避免过度依赖。
*   **提升非传统成果的可见性**：确保共享的数据集、代码和其他非传统研究成果能够被方便地发现、获取和引用，例如通过分配DOI和建立与论文的清晰链接。
*   **推广合乎伦理的AI使用**：通过作者指南和编辑政策，鼓励在学术出版中合乎伦理、负责任地使用AI技术，强调透明披露和人类最终责任⁷²。

#### 未来展望：

未来科研无疑将与AI更深度地交织在一起，这使得透明度和开放性变得比以往任何时候都更加关键。持续的对话、适应性的策略以及跨学科的合作，对于应对新兴挑战至关重要。开放科学运动预计将继续发展壮大，但其目标的完全实现，取决于能否有效克服前文所述的文化、结构和技术障碍。

最终，回应用户最初的深层忧虑——当AI捏造的内容充斥互联网时，如何区分AI辅助的整理引用与AI的直接编造——其答案在于一个多层面的体系：人类的批判性监督、稳健的验证流程，以及一个以透明度（开放数据、开放代码、开放方法）为常态的研究生态系统。在这样的生态系统中，无论是AI还是人类产生的虚假信息都更难遁形，而真实的研究则更容易得到验证和传播。关注点不应仅仅是区分内容的来源（AI或人类），而应是区分可靠与不可靠，无论其来源如何。通过共同努力，科研界可以驾驭AI带来的变革，维护科学知识的完整性和可信度，确保其继续为社会进步贡献力量。

---

**参考文献**

- 1: [The Rise of AI-Generated Misinformation and Its Impact on Academic Publishing](https://www.econtentpro.com/blog/the-rise-of-ai-generated-misinformation-and-its-impact-on-academic-publishing/414)
- 2: [Generative AI Reliability and Validity - AI Tools and Resources - LibGuides at University of South Florida Libraries](https://guides.lib.usf.edu/c.php?g=1315087&p=9678779)
- 3: [libguides.northwestern.edu](https://libguides.northwestern.edu/ai-tools-research/evaluatingaigeneratedcontent#:~:text=Specifically%2C%20to%20verify%20AI%2Dgenerated,and%20other%20information%20is%20correct.)
- 4: [Four key ingredients to accurate and reliable AI | Thomson Reuters](https://www.thomsonreuters.com/en/insights/articles/four-key-ingredients-to-accurate-and-reliable-ai)
- 5: [Detecting AIGC in Academic Journals: Methods and Consequences](https://www.cwauthors.com/article/Use-of-AIGC-in-papers)
- 6: [(PDF) Survey on AI-Generated Plagiarism Detection: The Impact of ...](https://www.researchgate.net/publication/385521932_Survey_on_AI-Generated_Plagiarism_Detection_The_Impact_of_Large_Language_Models_on_Academic_Integrity)
- 7: [Risks of AI - Artificial Intelligence (AI) Resources](https://laboure.libguides.com/c.php?g=1365116&p=10126993)
- 8: [Measures to Authenticate AI-Generated Scholarly Output - Enago](https://www.enago.com/academy/authentication-of-ai-generated-scholarly-output/)
- 9: [Research coming out as 100% AI generated : r/ArtificialInteligence - Reddit](https://www.reddit.com/r/ArtificialInteligence/comments/1g5bise/research_coming_out_as_100_ai_generated/)
- 10: [Detecting manuscripts written by generative AI and AI-assisted ...](https://pmc.ncbi.nlm.nih.gov/articles/PMC10791078/)
- 11: [What is academic misconduct? | Student Administration - Registry Services](https://registryservices.ed.ac.uk/academic-services/students/conduct/academic-misconduct/what-is-academic-misconduct)
- 12: [Science map of academic misconduct - PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC10912691/)
- 13: [How Many Scientists Fabricate and Falsify Research? A Systematic Review and Meta-Analysis of Survey Data | PLOS One](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0005738)
- 14: [Repercussions of Plagiarism in Scientific Research: Threats to Integrity & Innovation](https://www.proofig.com/post/repercussions-of-plagiarism-in-scientific-research-a-threat-to-integrity-and-innovation)
- 15: [Academic fraud, data and dishonesty - Psych Safety](https://psychsafety.com/academic-fraud-data-and-dishonesty/)
- 16: [Research Misconduct News](https://research.uky.edu/research-misconduct/news)
- 17: [The 'publish or perish' mentality is undermining science - University of Technology Sydney](https://www.uts.edu.au/news/2024/09/publish-or-perish-mentality-undermining-science)
- 18: [Finding a Good Balance between Pressure to Publish and Scientific Integrity and How to Overcome Temptation of Scientific Misconduct - PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC9326544/)
- 19: [Factors that Can Contribute to Academic Misconduct | GSI Teaching & Resource Center](https://gsi.berkeley.edu/gsi-guide-contents/academic-misconduct-intro/factors/)
- 20: [Balancing ethical data sharing and open science for reproducible ...](https://pmc.ncbi.nlm.nih.gov/articles/PMC12047515/)
- 21: [Esteemed Colleagues: A Model of the Effect of Open Data on Selective Reporting of Scientific Results - Frontiers](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2021.761168/full)
- 22: [Open Data | Research Operations Office](https://www.research-operations.admin.cam.ac.uk/policies/open-data)
- 23: [Addressing the threat of generative AI tools to research integrity: Actions by the publishing industry - Kriyadocs](https://www.kriyadocs.com/blogs/addressing-the-threat-of-generative-ai-tools-to-research-integrity-actions-by-the-publishing-industry)
- 24: [Research ethics and data protection - University of Reading](https://www.reading.ac.uk/research-services/research-data-management/data-management-planning/research-ethics-and-data-protection)
- 25: [For researchers: Upholding scientific research integrity with Dryad](https://blog.datadryad.org/2025/02/05/for-researchers-upholding-scientific-research-integrity-with-dryad/)
- 26: [What is open science? - Center for Open Science](https://www.cos.io/open-science)
- 27: [guides.lib.virginia.edu](https://guides.lib.virginia.edu/RDM/open-science-and-data#:~:text=Enhance%20efficiency%3A%20Greater%20access%20to,done%20using%20the%20same%20data.)
- 28: [Data sharing myths - Author Services](https://authorservices.taylorandfrancis.com/data-sharing/data-sharing-myths/)
- 29: [Supporting open science: Exploring barriers and solutions to data ...](https://www.springernature.com/gp/librarians/the-link/open-science-blogpost/supporting-open-science-barriers-to-data-sharing/27737232)
- 30: [Researcher Challenges and Experiences with Data Services ...](https://sr.ithaka.org/publications/researcher-challenges-and-experiences-with-data-services/)
- 31: [Code Policy | Open science | Springer Nature](https://www.springernature.com/gp/open-science/code-policy)
- 32: [Research data policy | Publish your research - Springer Nature](https://www.springernature.com/gp/authors/research-data-policy)
- 33: [Elsevier, Springer Nature, and AAAS: Publisher Research Data ...](https://update.lib.berkeley.edu/2017/05/04/elsevier-springer-nature-aaas-publisher-research-data-policies/)
- 34: [Publishing Open Access with Elsevier](https://researcheracademy.elsevier.com/uploads/2017-11/Brochure_OpenAccess_2_web.pdf)
- 35: [Open Access - Wiley Author Services](https://authorservices.wiley.com/open-research/open-access/index.html)
- 36: [Funder and Journal Requirements - Data Management @ ODU ...](https://guides.lib.odu.edu/c.php?g=1353047&p=9990112)
- 37: [Data Sharing Policy - Wiley Author Services](https://authorservices.wiley.com/author-resources/Journal-Authors/open-access/data-sharing-citation/data-sharing-policy.html)
- 38: [Open Access Preprint Guide and FAQ - IEEE VIS](https://ieeevis.org/year/2025/info/open-practices/open-practices-faq)
- 39: [Open Access Publication & ACM](https://www.acm.org/publications/openaccess)
- 40: [Publication Rights & Licensing Policy - Association for Computing Machinery](https://www.acm.org/publications/policies/publication-rights-and-licensing-policy)
- 41: [Call for Resource Papers - CIKM 2025](https://cikm2025.org/calls/resource-papers)
- 42: [SIGCHI Data Collection, Access, and Use Policy](https://archive.sigchi.org/about/sigchi-policies/conference-policies/data-release/)
- 43: [Cell Press - Patterns - FAIRsharing](https://fairsharing.org/5834)
- 44: [Cell Press - Author's guide to data sharing - FAIRsharing](https://fairsharing.org/5838)
- 45: [Artificial Intelligence (AI) Guide: JAMA Network Journals - LibGuides](https://utsouthwestern.libguides.com/artificial-intelligence/ai-publishing-jama)
- 46: [JAMA leads way in enabling access to medical journal research](https://www.ama-assn.org/about/publications-newsletters/jama-leads-way-enabling-access-medical-journal-research)
- 47: [Open Access Policies - Understanding Open Access - Research ...](https://laneguides.stanford.edu/openaccess/policies)
- 48: [NIH Data Management & Sharing Policy (Effective 2023) - Data ...](https://hsls.libguides.com/data/nihdmsp)
- 49: [NIH Data Management & Sharing Plan (DMSP) Research Guide: Home](https://guides.himmelfarb.gwu.edu/NIHDMSPolicy)
- 50: [NIH Data Management and Sharing | MIT Research Administration Services](https://ras.mit.edu/grant-and-contract-administration/sponsor-information/nih/nih-data-management-and-sharing)
- 51: [DMR Data Management and Sharing Plan Guidance - Division of Materials Research (MPS/DMR) | NSF - National Science Foundation](https://www.nsf.gov/mps/dmr/data-management-sharing-plans)
- 52: [Preparing Your Data Management and Sharing Plan - Funding at NSF](https://www.nsf.gov/funding/data-management-plan)
- 53: [Horizon Europe: Open Data, Software and Code Guidelines - OECD](https://www.oecd.org/en/publications/access-to-public-research-data-toolkit_a12e8998-en/horizon-europe-open-data-software-and-code-guidelines_1b08e856-en.html)
- 54: [Horizon Europe Open Science Requirements: Understanding Compliance in Practice](https://opusproject.eu/openscience-news/horizon-europe-open-science-requirements-understanding-compliance-in-practice/)
- 55: [Open Research | Wellcome](https://wellcome.org/our-priorities/diversity-and-inclusion/open-research)
- 56: [Open Access Policy - Grant Funding | Wellcome](https://wellcome.org/grant-funding/guidance/open-access-guidance/open-access-policy)
- 57: [Open Access at MIT and Beyond](https://mitoataskforce.pubpub.org/pub/whitepaper)
- 58: [Data Sharing | Committee on the Use of Humans as Experimental Subjects - COUHES](https://couhes.mit.edu/guidelines/data-sharing)
- 59: [Sharing data | Research Data Oxford](https://researchdata.ox.ac.uk/sharing-data)
- 60: [University of Oxford research data policy](https://researchdata.ox.ac.uk/university-oxford-data-management-policy)
- 61: [FAIR Data: What It Is and How We Can Support Its Principles ...](https://www.csescienceeditor.org/article/fair-data-what-it-is/)
- 62: [The one piece R&D companies are missing in their FAIR data strategy - Benchling](https://www.benchling.com/blog/fair-data-strategy)
- 63: [Transparency and Openness Promotion (TOP) Guidelines - OSF](https://osf.io/9f6gx/)
- 64: [Transparency and Openness Promotion Guidelines: What are they?](https://www.apa.org/pubs/journals/resources/publishing-tips/transparency-openness-promotion-guidelines)
- 65: [TOP Guidelines - Center for Open Science](https://www.cos.io/initiatives/top-guidelines)
- 66: [Open science - UNESCO](https://www.unesco.org/en/open-science)
- 67: [Science is Under Siege: The Open Science Community Must Act and Lead by Example](https://upstream.force11.org/open-science-call-to-act/)
- 68: [AI in Peer Review: Enhancing Efficiency and Quality in a Changing Research Landscape](https://www.straive.com/blogs/ai-in-peer-review-enhancing-efficiency-and-quality/)
- 69: [Artificial Intelligence is the Future of Peer Review - Boston University Medical Campus](https://www.bumc.bu.edu/camed/2025/03/28/artificial-intelligence-is-the-future-of-peer-review/)
- 70: [The ethics of open and public AI: Balancing transparency and safety - Red Hat](https://www.redhat.com/en/blog/ethics-open-and-public-ai-balancing-transparency-and-safety)
- 71: [Mapping the Open-Source AI Debate: Cybersecurity Implications and Policy Priorities](https://www.rstreet.org/research/mapping-the-open-source-ai-debate-cybersecurity-implications-and-policy-priorities/)
- 72: [Practical Considerations and Ethical Implications of Using Artificial Intelligence in Writing Scientific Manuscripts - PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC11838153/)
- 73: [Open Research Data: Which data repositories can be used?](https://www.snf.ch/en/WtezJ6qxuTRnSYgF/topic/open-research-data-which-data-repositories-can-be-used)