
## 导演

本次对话整理自 Eye on AI 对 Richard Sutton 教授的访谈，聚焦于通过强化学习途径探索通用人工智能（AGI）的可能性。Sutton 教授，作为阿尔伯塔大学计算机科学教授及 Keen Technologies 的研究科学家，拥有超过 45 年的人工智能研究经验，被誉为“强化学习之父”。

访谈围绕 Sutton 教授与 John Carmack 合作的“阿尔伯塔计划”展开，这是一个为期五年的研究项目，目标是在 2030 年实现 AGI。该计划的核心在于构建“具身智能体”，即通过与环境的持续互动进行学习和规划，而非依赖预先标记的数据集。

对话内容涵盖了计算能力指数级增长（摩尔定律）对人工智能研究的深远影响、对 Transformer 模型兴起的评价、以及对“世界模型”作为通往 AGI 更优路径的论证。Sutton 教授详细阐述了阿尔伯塔计划如何借鉴并区别于 Yann LeCun 的相关研究，并深入介绍了强化学习中的关键概念，包括离策略学习、Horde 架构、动态学习网络以及体验式人工智能。

此外，访谈还涉及了 AI 伦理、未来潜在应用，以及对 2030 年实现 AGI 这一目标的展望。本次对话不仅提供了对强化学习前沿研究的深入了解，也引发了对人工智能发展方向和心智本质的深层思考。建议相关领域的研究者和对人工智能感兴趣的读者详细阅读本次访谈记录。



### 内容纲要

```
├── (00:00) 预览与介绍
│   ├── Craig Smith (主持人) 介绍
│   └── Richard Sutton 开场白：计算能力的重要性
├── (02:15) AI的演进：理查德·萨顿的洞见
│   ├── 自我介绍
│   ├── 《苦涩的教训》与计算能力
│   ├── 摩尔定律与“奇点”
│   └── Transformer模型的局限
├── (07:08) 分解AI：从算法到AGI
│    └── 与Yann LeCun世界模型方法的比较：目标与世界模型的重要性
├── (10:50) 阿尔伯塔实验：一种新的AI学习方法
│   ├── 从监督学习开始的原因
│   ├── 监督学习与强化学习的关系
│   ├── AI工具与AI智能体的区别
│   └── 阿尔伯塔计划的核心原则和12个步骤
├── (18:27) Horde架构详解
│    └── 离策略学习、子问题与Horde架构
├── (21:23) 强强联合：卡马克、Keen与AI的未来
│    ├── 与John Carmack合作的原因
│    └── Keen Technologies与阿尔伯塔计划
├── (25:04) 扩展AI的学习能力
│   ├── 大型语言模型的局限性与“语言最后”方法
│   ├── 阿尔伯塔计划的当前进展
│   └── 新算法思想：离策略学习与动态学习网络
├── (31:34) AI是技术的未来吗？
│   ├── 动态学习网络的细节
│   └── 体验式AI：数据来自生活经验
├── (35:29) AI的下一步：体验式学习与具身化
│   ├── 具身系统的重要性
│   ├── 模拟环境与真实世界的比较
	└── 开发正确的算法
├── (40:00) AI的构建模块：更智能未来的算法
│   ├── 自动驾驶的世界模型：抽象而非物理模型
│   └── 抽象模型构建：时间差分学习与动态规划
├── (45:59) AI的策略：规划与表示
│    ├── 阿尔伯塔计划的最终目标：理解并创造心智
│    └── AI的潜在应用：不仅仅是“奴隶”
├── (49:27) 学习方法对决：强化学习 vs. 监督学习
│    ├── 强化学习的目标导向 vs. 监督学习的模式识别
│    └── AI助手需要理解目标
├── (53:10) 驾驭AI伦理与安全辩论
│    └── 对AI威胁论的看法
├── (54:53) 2030年愿景：瞄准真正的AI智能？
│    └── 2030年实现真正AI的可能性
└── (56:39) 总结：反思与展望

```


# 理查德·萨顿论通过强化学习追求AGI 

**发布平台:** [Eye on AI (YouTube)](https://www.youtube.com/watch?v=zZuh8YUBeDY)
**发布时间:** 2024年2月23日

## (00:00) 预览与介绍

**Craig Smith (主持人):** 大家好，我是 Craig Smith，欢迎收看 Eye on AI。在今天的节目中，我将与强化学习之父、阿尔伯塔大学教授 Richard Sutton 对话。我们将讨论他与约翰·卡马克在 Keen 公司的合作，这家初创公司立志在2030年实现通用人工智能。Richard 还将谈到阿尔伯塔计划，他雄心勃勃的五年研究议程，专注于构建能够通过与环境互动来学习和计划的具身智能体。Sutton 将深入探讨当前进展、新的算法发展以及模拟和物理环境在训练中的权衡，以及创造 AGI 的最终目标。我相信这次对话会非常精彩，希望您也能喜欢。

首先，让我们先听一段 Richard 的开场白。

**Richard Sutton:**  科学领域没有哪个不受大规模计算能力可用性的深刻影响，更不用说更普遍的计算能力了。这不仅仅是人工智能的故事，这是我们这个时代的故事。关键在于利用计算能力来创造有用的事物，并理解心智。这些都需要大量的计算。计算能力变得越来越便宜，呈指数级增长，这种情况已经持续了大约100年，并且预计还会继续下去。看起来现在是每18个月翻一番，而且这种情况不断发生，这意味着每十年都会翻倍再翻倍，事物会发生质的变化。这种情况已经持续了很多年，而且未来会更加如此。我认为这就是我们真正应该理解的“奇点”的含义。奇点就是我们拥有这种缓慢但爆炸式的计算机能力增长，这正在从根本上改变事物。

**Craig Smith:**  大家好，我是 Craig Smith，欢迎收看 Eye on AI。在今天的节目中，我将与强化学习之父 Richard Sutton 展开对话，他目前是阿尔伯塔大学的教授。我们将讨论他与约翰·卡马克在 Keen 公司的合作，这家初创公司誓言要在 2030 年实现通用人工智能。Richard 还会谈到阿尔伯塔计划，他雄心勃勃的五年研究议程，专注于构建能够通过与环境互动来学习和计划的具身智能体。Sutton 将深入探讨当前进展、新的算法发展，以及在模拟和物理环境训练之间的权衡，以及创造 AGI 的最终目标。希望您能像我一样觉得这次对话非常精彩。那么，Richard，您能否先介绍一下自己？我想听众可能已经认识您了，我之前也邀请您参加过播客。但是对于新听众，请您介绍一下您是谁，您在哪里，然后我们将讨论我非常感兴趣的阿尔伯塔计划。

## (02:15) AI的演进：理查德·萨顿的洞见

**Richard Sutton:**  谢谢你，Craig。嗯，我是 Richard Sutton，我是一名科学家，研究人工智能已经大约45年了，时间很长了。我现在在加拿大的北方，阿尔伯塔大学，担任计算机科学系的教授，同时我也是 Keen Technologies 的一名研究科学家。我有很多头衔和兼职角色，但基本上我只是想弄清楚心智是如何运作的。我一直试图以一种非常广泛的跨学科方式进行研究，阅读所有关于这个主题的不同思想家的著作，并从心理学和大脑可能如何运作的角度来探讨这个问题。

**Craig Smith:**  我阅读了您最近的一些论文，我看到了这个思路的发展。我不知道是因为您写得更多了，所以这些想法在印刷品中更加完善，还是它们在您的脑海中不断发展。但从2019年您写《苦涩的教训》开始，您就谈到了一个观点，即真正推动事物发展的是不断增长的计算能力，这驱动了很多事情的进步。这在时间上与 OpenAI Transformer 模型的扩展大致吻合。我曾问过 Ilya Sutskever，您的文章是否引发了他们对扩展规模的兴趣，他说那只是巧合。但是，首先，我们能否谈谈规模扩展以及计算资源和摩尔定律的可用性是如何驱动人工智能研究领域的许多进展的，甚至超过了新颖的算法？

**Richard Sutton:**  我认为首先要意识到的是，它不仅驱动着人工智能领域，它还驱动着所有的科学和世界上所有的工程发展。几乎没有哪个科学领域没有受到大规模计算能力可用性的深刻影响，以及更普遍的计算能力。这是我们这个时代的故事，不仅仅是人工智能的故事。人工智能一直都知道它需要计算能力。关键在于利用计算能力来创造有用的事物并理解心智。是的，现在那些对连接主义系统或分布式网络感兴趣的人，现在通常被称为神经网络，虽然我不太喜欢这个术语，每次用的时候都会有点不舒服。但是，我们这些一直在做这件事的人，那些一直在做学习的人，我认为学习对于智能来说很重要。所有这些都需要大量的计算，所以它们会受到当时可用计算能力的限制。

**Craig Smith:**  好的，让我们明确一下，摩尔定律是什么？让我们称之为摩尔定律。摩尔定律是指计算能力正变得越来越强大和便宜，呈指数级增长，这种情况已经持续了大约一百年，并且预计还会继续下去。指数级增长看起来像是每两年翻一番，现在是每18个月翻一番，而且这种情况不断发生，18个月后又18个月，这意味着你翻倍再翻倍，每十年事物都会发生质的变化。这种情况已经持续了很长时间，几十年了，而且未来还会更加如此。所以我们有理由期待未来，它将继续对所做的一切产生巨大的影响。另一方面，这也很正常，这只是你所期望的，我们这些长期从事人工智能研究的人都对此有所预期和计划。现在它正在到来，但它是指数级的，所以指数是自相似的，这意味着它们在每个时间点看起来都一样。每年都在以一年半的时间翻一番，所以这是一种爆炸，因为每个指数都是一种爆炸，这有点像，我认为这就是我们真正应该理解的“奇点”的含义。奇点就是我们拥有这种缓慢但爆炸式的计算机能力增长，这正在从根本上改变事物。

**Craig Smith:**  是的，大约一年前，我与 Aiden Gomez 进行了一次非常有趣的对话，他参与了 Google Transformer 算法的设计团队。他现在创办了一家名为 Cohere 的初创公司，他也是加拿大人。他说了一件有趣的事情，他认为几乎可以是任何算法，不一定是 Transformer，只是社区选择了 Transformer，投入了资源，并持续扩展它。Transformer 是可扩展的，可扩展架构非常重要，但这不一定非得是 Transformer。这让我想到了您，因为 Transformer 的核心，正如 Aiden 所描述的，它本质上是一个堆叠的多层感知器，带有注意力机制。你扩展它，输入数据，它就能学会理解语言，或者至少看起来理解语言。但它也有很多明显的局限性。在过去的几年里，我一直在与 Yann LeCun 讨论世界模型，对我来说，世界模型听起来是通往通用智能的更令人兴奋的方向，因为并非所有的智能都包含在语言中，或者至少大部分甚至更少的部分包含在人类文本中。然后我看到你们提出了阿尔伯塔计划，这听起来甚至更令人兴奋。那么，您如何看待阿尔伯塔计划？你们的目标是构建一个智能体，最终是一个具身智能体，它拥有世界模型，或者可以通过与环境的互动来创建世界模型。这与 Yann LeCun 的方法在基本层面上有什么不同？

## (07:08) 分解AI：从算法到AGI

**Richard Sutton:**  在非常基本的层面上，一个好的观点是，它们非常相似。你看看他的架构的各个部分，以及阿尔伯塔计划中提出的架构的各个部分，它们几乎一一对应。是的，我们试图做同样的事情。我们采取的方法略有不同，我们可以讨论一下，但我认为，仅仅关注差异可能会分散对核心信息的注意力。核心信息是，你必须有一个目标，你必须有一个世界模型，然后一切都由使用该模型采取行动和规划行动来驱动，在不同的抽象层次上进行规划，以便实现目标。对我来说，这才是真正的智能。理解世界，利用你的理解来实现你的目标。我喜欢将目标定义为奖励，我对此非常满意。有些人勉强接受奖励，尽管这看起来有点低级，但它是一种自然的方法。我认为这对那些不熟悉深度学习和监督学习的人来说，更容易理解。

## (10:50) 阿尔伯塔实验：一种新的AI学习方法

**Craig Smith:**  我在阿尔伯塔计划的路线图中发现一件有趣的事情，你们从监督学习开始，为什么？仅仅是因为它比较容易吗？

**Richard Sutton:**  是的，我想在某种意义上是这样的。因为我们想专注于持续学习，持续学习是一件很明显的事情，几乎是学习本身的定义，学习应该是一个持续进行的过程。但是，第一步，实现非线性网络的持续学习仍然具有挑战性，即使对于监督学习也是如此。因此，从最简单的可能情况开始是很自然的，这种情况涉及最少的其他因素，那就是监督学习的情况。

**Craig Smith:**  是的，这很有趣。

**Richard Sutton:**  让我简单说几句，因为在过去的几十年里，监督学习和强化学习之间一直存在着某种斗争。你知道，学习方法能获得的关注是有限的，所有对监督学习的关注在某种程度上都分散了对强化学习的注意力。所以，存在着一种友好的竞争。监督学习总是赢得竞争，因为监督学习更容易付诸实践，更容易被人们使用。它在某种程度上没有那么雄心勃勃，但它非常重要。我们这些从事强化学习或试图构建完整智能体架构的人，都是监督学习成果的消费者。我们将使用它们作为我们整体架构的组件。所以我们需要它们，我们可以研究它们，我们需要为了我们的目的构建它们。

**Craig Smith:**  我在您的一次演讲中看到，您区分了 AI 工具和 AI 智能体，监督学习属于工具类别。您能否大致介绍一下阿尔伯塔计划的演变，然后以最简单的形式向听众介绍阿尔伯塔计划是什么？这将为我提供一个提问的框架。

**Richard Sutton:**  阿尔伯塔计划试图将智能理解为一种主要学习现象，一种通过学习来理解环境，然后驱动环境以实现目标的现象。因此，阿尔伯塔计划的第一步是构建智能体、环境及其互动之间的结构。在互动中，你不是交换状态，而是交换观察，比如传感器、视觉、触觉、听觉。这些都是对具体细节的抽象，但必须是真实的观察，而不是状态，因为我们无法直接访问状态。所以，你知道，第一个原则，我试图一边说一边记住它们，但第一个原则是，智能体与环境的互动是神圣不可侵犯的。第二个原则是，学习是持续的，我们可以说时间上均匀的，在阿尔伯塔计划中我们称之为时间上对称的，这意味着没有特殊的阶段，比如训练和测试，生活只是不断地继续。你获得奖励，或者没有获得你想要的奖励，你获得观察，除了奖励、痛苦和快乐之外，没有其他老师。也许我没有完全记住四个原则，但另一个重要的点是，你将形成一个模型，所以你将进行规划。既有直接从经验中试错学习，也有学习模型，然后使用模型进行规划。这两者都是智能的重要组成部分。好的，这就是背景。然后我们概述了我们的12个步骤，这12个步骤实际上是从以下方面开始的：让学习在时间上是均匀的，让元学习成为可能。也许我应该停下来解释一下元学习。元学习意味着学习如何学习，而不仅仅是学习一个函数。一旦你持续学习，你既学习这个，又学习那个，你获得了很多学习经验，你可以变得更擅长学习。你可以利用重复的学习经验，来提高未来学习 эпизодов 的效率。作为其中的一部分，你学习表示，你学习特征，你学习步长。好的，持续学习，然后是所有算法。一旦我们加入了元学习和持续学习，并且是监督学习，那么我们将其扩展到强化学习，这涉及到它自己的一系列问题，更有趣的时间关系。我想，前六个步骤就像是构建强化学习的基本算法，再次经历它们，使其具有持续性和元学习能力。然后我们开始引入具有挑战性的问题，比如离策略学习和学习世界模型，然后是规划。最后，跳到最后一步，最后一步是关于智能增强（IIA），与人工智能（AI）相对。智能增强是指我们将计算机 AI 与我们自己的心智结合起来，使我们自己的心智更强大。

## (18:27) Horde架构详解

**Craig Smith:**  其中一个关键步骤是离策略学习和学习世界模型。离策略学习意味着你希望能够学习你没有做的事情，或者你没有完全完成的事情。例如，即使是识别一个物体，你看着这个物体，你说，你必须以某种客观的方式定义它，最好的方法是将其定义为一个子问题。是的，也许我应该就此打住。阿尔伯塔计划最有趣、最独特的策略是，心智通过为自己提出子问题，然后努力解决这些子问题来运作。当然，它有一个主要问题，那就是获得奖励，但它也有成千上万个子问题同时在解决。由于它不能同时为所有成千上万个问题采取行动，它必须选择一个问题，比如主要问题，并根据该问题采取行动。因此，所有其他的事情都必须能够从并非完全按照它们会做的方式产生的数据中学习，这被称为离策略学习，它是学习实现辅助子问题的关键，也是高效学习世界模型的关键。

**Craig Smith:**  你们有一个叫做 Horde 架构的东西，是在哪里引入的？当你们把一个问题分解成多个子任务时，你们会学习这些子任务吗？

**Richard Sutton:**  那是一篇我们研究这个想法的论文，我们发展了这个想法。Horde 就是子问题群，Horde 中的每个“恶魔”，几乎可以看作是神经网络中的一个神经元，都在努力完成不同的任务，试图预测不同的事情，或者试图实现不同的目标。这是将心智视为去中心化的观点，有一个最终目标，一切最终都朝着一个目标前进，但拥有不同的部分朝着其他目标努力，仍然是一个有用的结构。

## (21:23) 强强联合：卡马克、Keen与AI的未来

**Craig Smith:**  您是如何与约翰·卡马克走到一起的？主要是因为您需要资金，而且这为您提供了一个筹集资金的渠道吗？不，我是认真的，我的意思是，Yann LeCun 背后有 Meta 支持，虽然这真的不具有可比性。约翰的公司很棒，但它仍然像是一家2000万美元的公司，这对于我们现在想做的事情来说已经足够多了。约翰和我走到一起，是因为我们对实现 AI 或 AGI 所需要的东西，以及不需要的东西，有相似的想法。是的，我读到一篇报纸文章，一篇约翰在德克萨斯州接受的采访，我能看出他在思考问题的方式与我相似，尽管我们的背景截然不同。你认为智能，必须有一些原则需要解决，而不是一个庞大的程序要编写。我们需要弄清楚一些原则，可能不多，也许是1万行代码，而不是1000万行代码。

**Richard Sutton:**  所以很容易获得，相对来说，虽然在世界上获得基础研究资金仍然很困难，但很容易获得用于人工智能应用的资金，特别是大型语言模型的应用。无论如何，我真的很享受在 Keen 工作，能够专注于想法。这是一家平静的公司，我们有很多思考，很多沉思，也有很多实验，我们正在努力让工程方面变得非常重要，但对我来说，能够重新整理我的思路，非常仔细地思考它们，并推动它们向前发展，真的太棒了。

**Craig Smith:**  Keen 正在实施阿尔伯塔计划，是这样吗？我的意思是，这是项目吗？

**Richard Sutton:**  阿尔伯塔计划是一个研究计划，它像一个五年研究计划，所以研究是你进行的，而不是你实施的，而且结果并不总是如你所愿。是的，我不会说“实施”是合适的词，至少目前还不是。

**Craig Smith:**  但是，您在 Keen 所做的工作是以阿尔伯塔计划为指导的，对吗？

**Richard Sutton:**  绝对是这样，我正在研究阿尔伯塔计划，Keen 的最终目标是创造阿尔伯塔计划描述的具身智能。

**Craig Smith:**  您听起来不是很自信。

**Richard Sutton:**  计划只是计划，你知道，我认为很有可能它会按计划进行，但你知道，五年计划，你会在四三年后制定另一个计划。是的，所以我不会妄自断言我知道它会如何发展，但与此同时，我们必须做出我们的赌注，我们必须认真思考它。只是知道，我们很可能走对了方向，但你知道，您的工作主要集中在强化学习，您写了关于强化学习的书，时间差分学习和 Lambda，以及所有这些。这似乎是一个更雄心勃勃的项目。Transformer 扩展的成功是否让您觉得，好吧，让我们用 RL 来做，为什么这些人都在庆祝他们所做的事情，但还有更多的事情要做？

**Richard Sutton:**  不，不，你看到阿尔伯塔计划可能比这本书更大，但这一直是我们的计划。我们在人工智能领域一直试图理解整个心智，并在计算机中重现它。这是一个巨大的野心，一直以来都是这样。

## (25:04) 扩展AI的学习能力

**Richard Sutton:**  大型语言模型在某种意义上有点令人失望。我的意思是，人们感到兴奋，人们想要了解它，这真的很好，但是，我不认为这是最有效率的追求方向。现在，谁知道呢？但我知道的是，这不是对我来说最有用的方向。我对行动和目标更感兴趣，以及智能体如何判断什么是真的，什么不是真的，所有这些都是大型语言模型所缺乏的。不，它们真的不是。它们所做的重要事情是，它们展示了你可以用计算和网络做什么，以及你可以获得多么复杂的东西，你可以处理大量数据。它只是向那些需要被展示的人展示了力量。而且，它可以成为人类和你最终创造的智能体之间的界面。你仍然需要一个语言界面来交流。

**Craig Smith:**  是的，但我怀疑我们今天使用大型语言模型所做的事情是否会对界面有所贡献。

**Craig Smith:**  是这样吗？换句话说，您想要构建的模型，您想要构建的智能体，会作为学习过程的一部分来学习语言吗？

**Richard Sutton:**  是的，就像我们说的“语言最后”，而不是“语言优先”。大型语言模型是“语言优先”，我们只是说“语言最后”，就像 Yann LeCun 说我们需要做的那样，先解决鼠类水平的智能，然后再解决猫类水平的智能，我们必须先弄清楚这些，然后再尝试制造人类水平的智能。

**Craig Smith:**  那么，您在计划中处于什么阶段？我的意思是，您已经弄清楚了强化学习，您可以构建智能体，并且有各种架构可以从各种感觉输入中创建表示。在表示之后，您就可以有效地进行规划。那么，在所有这些研究中，您目前处于哪个阶段？

**Richard Sutton:**  这有点难以用非技术性的语言解释，但你可以说一些事情。当然，你可以说，各个步骤并不是完全按顺序完成的。你总是在寻找可以取得进展的领域，这些领域可能在第10步，也可能在第3步。但我也想大致地说，我们现在大约处于第4步。我们仍在做一些事情，我们正在改变基本的底层强化学习算法。我们还没有完成这项工作，我们需要更高效的算法，我对我们最近正在开发的一些关于如何实现这一目标的新想法感到兴奋。

**Craig Smith:**  您能谈谈这些新想法吗？

**Richard Sutton:**  好的，其中一件大事是高效的离策略学习和重要性采样的使用。重要性采样是指你观察你在目标策略和行为策略下做事的可能性，并根据这两者的比率调整回报。长期以来，我认为这是调整回报的唯一方法。但是，现在我认为可以通过改变你的期望来完成回报的前向修正。例如，如果你期望好事发生，期望采取好的行动，但实际上采取了不同的行动，一个更具探索性的行动。这与你的目标策略（更贪婪的策略）有所偏差。考虑到与目标策略的偏差的一种方法是，直接说“哦，好吧，我现在做了一些不是最好的事情，所以我现在要调整我的水平，我要期望稍微少一点”。有一种系统的方法可以做到这一点，这为我们提供了一种处理回报离策略性的新方法。这产生了一整套新的算法，这令人兴奋。

## (31:34) AI是技术的未来吗？

**Richard Sutton:**  现在，对于令人兴奋的事情，也许对我来说最容易理解的新颖性方向是持续学习。我要说很多事情，对我来说，它们都将有相同的解决方案：持续学习、元学习、表示学习、学习如何学习、学习如何泛化状态、如何构建状态表示、特征发现，所有这些都在到来，这将是一种新的方法，一种在深度网络中进行学习的新方法。我称之为动态学习网络（Dynamic Learning Nets）。动态学习网络在三个层次上进行学习，而通常我们的神经网络只在一个层次上学习，它们在权重的层次上学习。但是，除了权重之外，我们还想在步长的层次上学习。所以，在你网络中每一个有权重的地方，你也会有一个步长。步长有时被称为学习率，最好称之为步长，因为学习率会受到许多其他因素的影响。如果我们想象一个完整的网络，每个权重旁边都有一个步长，这个步长是通过一个自适应过程来调整的，这个自适应过程以元学习的方式，以元梯度的方式进行调整，目的是使系统学习得更好，而不是仅仅在瞬时时刻表现得更好。学习率或步长不影响函数，它们不影响在特定时间点实现的函数，它们不影响网络所做的事情，它们影响网络如何学习。所以，如果你能调整步长，你也能获得学习如何学习和学习如何良好泛化的能力，以及诸如此类的东西。我们希望具有自适应性的最后一个元素是连接模式，即谁与谁连接。这将通过一个创造性的过程来完成。例如，假设你从一个线性单元开始，它学习，比如，一个价值函数或一个策略，它尽其所能利用可用的特征。然后它需要诱导创建新特征，因为你需要学习原始信号的非线性函数，所以你需要创建新的特征，这些特征可以被该线性单元使用。通过这种方式，你以一种有机的，有点像增长的方式，构建一个可以学习非线性函数的系统。这只是一种以不同方式最终得到深度网络的方法，包括所有学习到的特征，动态学习网络。

**Craig Smith:**  动态学习网络，输入数据来自哪里？

## (35:29) AI的下一步：体验式学习与具身化

**Richard Sutton:**  输入数据，在强化学习中，只是来自生活，来自做事，来自观察事物。这里没有标记的数据集。也许我应该从一开始就说，我称之为体验式 AI 的整个想法是，没有人为你制造数据，你像婴儿一样长大，你玩弄东西，你看到东西，你做事，这就是数据。强化学习的诀窍在于，如何将这种数据转化为你可以从中学习并从中发展心智的东西。监督学习的美妙之处和局限性在于，他们说，好吧，我们暂时不用担心这个问题，让我们假设我们以某种方式拥有一个带有标记事物的数据集，让我们研究这个子问题。这是一个好主意，研究一个子问题，弄清楚它，然后继续下一步。但实际上，我们必须继续下一步，我们必须担心数据集（所谓的“数据集”）是如何从训练信息中自动创建的。从来没有数据集，数据集是一个非常具有误导性的术语，它暗示这个东西很容易拥有、存储和管理。实际上，生活充满了你做事、事情发生，然后它们就消失了。你知道，一切都是转瞬即逝的。你没有它的记录，而且拥有它的记录将是极其复杂且没有太大价值的。强化学习和监督学习，特别是我处理它的方式，感觉完全不同。你知道，很多人通过创建一个缓冲区或记录所有已经发生过的经验（至少在一段时间内）来进行强化学习。现在，我认为这很有吸引力，但这不是答案所在。答案是拥抱数据的转瞬即逝的本质，并在它发生时充分利用它，然后让它过去。

**Craig Smith:**  这就是为什么你想制造一个具身系统，这样你就拥有了所有的五种感官或更多。你需要，不仅仅是说具身系统，而是一个交互式系统，它可以影响它的输入流，它的感觉流。然后你就可以获得这种互动。在很长一段时间内，你可以在模拟环境中做到这一点，或者你可以在机器人技术中做到这一点。我仍然不知道哪种方式最好，或者最好的方式是两者都做，或者先做一种再做另一种。约翰对从视频中学习很感兴趣，他喜欢他自己对体验的看法，即你拥有大量的视频流，比如你在观看 500 个电视频道，然后你可以切换观看这个，观看另一个。在 Keen，我的亲密同事 Joseph Modayil 对机器人技术很感兴趣，他认为获得适当输入数据流的最佳方式是实际构建机器人硬件。重要的是世界要足够大且复杂，因为我们想要处理的世界是庞大而复杂的。所以你需要像视频这样的东西，你需要大型数据流。现在你可以使用模拟来生成，甚至是模拟视频流，但不可避免地，这些模拟世界实际上非常简单，它们具有潜在的简单性，它们可能具有三维直线结构中的物体，也许它们是刚性物体，视觉是一种非常特殊的几何形式。它们是被生成的，它们是虚构的世界，它们是被生成的，所以这些世界实际上比智能体更简单。他们的目标是花费大部分计算机能力来研究心智，只用一小部分来创建模拟数据，这与真实情况相反。每个人都可能有一个复杂的大脑，但他们的世界要复杂得多，不仅因为世界由所有的物理和物质组成，还因为它由其他心智、其他大脑和外界的其他心智组成，以及它们心智中发生的事情也很重要。因此，世界本质上比智能体复杂得多，而我们在模拟世界中颠倒了这一点，这总是令人担忧的。无论如何，这些都是在模拟世界或物理世界中工作之间权衡的一些问题。尽管如此，您需要在担心数据流之前开发架构和算法，我认为是这样。

**Richard Sutton:**  是的，但你想要开发正确的算法，如果你在一个不代表你的目标世界的世界上工作，并且在重要方面不具有代表性，那可能会产生误导。但你是对的，你是对的，这就是我们努力做的事情。你知道，我不知道你是否知道，但我几乎总是把自己的工作看作是想关注一些问题，所以我制作了一个非常简单的该问题的实例，比如有限状态世界，我深入研究它，但我不想利用它的渺小，你知道，我研究的算法在某种意义上甚至比这个简单的世界还要简单，我强调这些算法，看看它们的能力是什么。所以我们总是，你知道，这始终是研究的一部分，我们简化世界，充分理解它，就像物理学家可能会简化世界，让一个小球从斜坡上滚下来一样，这是一个非常简单的世界，你试图消除摩擦，你消除其他奇怪的影响，只是以最简单的形式看待事物。

**Craig Smith:**  是的，您是否关注过 Alex Kendall 在 Wayve AI 的工作？您知道那家公司吗？那是一家自动驾驶公司，他们有一个名为 Gaia-1 的世界模型。它与 Yann LeCun 正在做的事情类似，它从实时视频中编码表示，然后基于这些表示进行规划，并且可以从表示空间控制汽车。这实际上非常了不起。那么，让我们来谈谈世界模型，以及哪种世界模型适合自动驾驶？

## (40:00) AI的构建模块：更智能未来的算法

**Richard Sutton:**  让我说一些在我看来是错误的事情，这些错误似乎很自然，但实际上是错误。一个错误是制作像世界物理模型这样的东西，或者试图制作可以模拟世界并生成视频帧的东西。你不需要未来的视频帧，这不是你思考的方式。相反，你会想，“哦，我可以去市场，也许那里会有草莓”。你不是在创建一个视觉视频，你是在说，你就像跳到市场，然后你的草莓可能有不同的尺寸和位置，但仍然，那里没有视频，只有一个想法，如果你去市场就会发生。所以，人们已经意识到这一点，比如 Yann LeCun 过去常常谈论生成未来的视频，然后你意识到它会是模糊的，现在他意识到你需要产生你的模型的输出，这些输出根本不像视频流，它们根本不像观察，它们就像，它们就像构建的状态，那是行动的结果。好的，这与世界的偏微分方程模型非常不同，与自动驾驶汽车公司最初的做法非常不同。自动驾驶汽车公司从物理学和几何学开始，以及经过人类理解校准的东西，工程师对世界和驾驶的理解。但我怀疑这将是，我的意思是，我能知道什么？我不搞自动驾驶，我不做自动驾驶汽车，但我知道像特斯拉和埃隆·马斯克这样的人在做。所以，他们的目标是制造一些东西，你知道，他们像其他人一样从工程模型开始，但我现在理解的是，他们正在构建某种更概念性的模型，这些模型是基于人工神经网络的。因此，他们没有从几何学和已理解的事物开始，而是只是获取大量数据并训练它来制作模型。我们需要一个处于高级结果水平的模型，而不是处于像像素和视频这样的低级事物水平的模型。实现这一点的一种方法是拥有更高级别的状态特征，你说“这是一辆汽车”，而不是“这是一个视频帧”。基本上，基本上它就像你需要在状态和时间上都进行抽象一样简单。状态的抽象就像说“当我到达市场时会有草莓”，时间的抽象就像说“哦，我可以去市场，大约20分钟后我可能会到达那里，其他事情也会保持不变或以自然的方式相关”。我们希望能够思考“我可以去市场”，你也想思考“哦，我可以拿起可乐，我可以移动手指，这将产生某些后果”。我们知道你所思考的所有这些事情都处于截然不同的尺度。去市场大约需要 20 分钟，找到一份新工作可能需要一年，决定学习一个主题也可能需要一段时间。我们思考并分析后果，比如您今天想与我会面，我们安排了，您安排了它，您的计划在几周甚至几个月内进行。我们通过规划所有这些并交换高级消息来组装这次采访活动。所有这些，你知道，认为这是在想象我们可能会用眼睛看到的视频或我们可能会在这里听到的音频信号的层面上完成的，这很愚蠢。是的，所以我们需要在时间和状态上都是抽象的模型。作为一名强化学习人员，我自然而然地转向一套特定的技术来做到这一点。预测是基于时间差分学习的多步预测。规划本质上是通过动态规划来完成的，价值迭代，但其中的步骤不是低级动作，而是被称为选项，它们是高级的行为方式，会终止。所以有一些事情，比如去市场，当你到达市场时它们就会终止。所以在一定的概念层面上，我们想要去哪里对我来说很清楚，使用时间和状态的抽象模型，用选项和特征构建。我不知道，我们最近确实写了一篇论文，发表在 AI 杂志上，关于使用子问题进行规划的概念，在 STOMP 进展中。STOMP 代表子任务、选项、模型和规划，将所有这些东西放在一起，你就可以完成从数据流到抽象规划的完整进展，这就是我们试图组合在一起的东西。

**Craig Smith:**  是的，是的。我之前在谈论 Gaia-1 模型时有点说错了，我的意思是，它们的输入是视频，它创建了一个表示，并在表示空间中进行规划和采取行动。然后你可以将表示解码成视频，以了解它在做什么，但你不是在视频空间中进行规划。那么，您对这一切的雄心是什么？您将找出并改进强化学习算法，它们需要是可扩展的。一旦您拥有了这些，您就继续前进，开始用计算能力来扩展它们，并遵循您的路线图，或者我是否把它想得太简单了？

## (45:59) AI的策略：规划与表示

**Richard Sutton:**  你知道，我们想要理解心智是如何运作的，然后我们将制造一个心智，或者一些心智，或者一些心智的量，它将在各方面都有用，在各种方面都有经济价值，它也将对我们有用，扩展我们自己心智的能力，如果我们能理解我们心智是如何运作的，我们就可以增强它们，以便它们可以更好地工作。是的，我们将要做的关键步骤是理解，然后就会有数百万种用途。我不认为它会像制造工人那样简单，有点像奴隶供我们指挥，我不认为它会那么简单。这也许给出了潜在效用的下限。我们在 Keen 的故事是，我们说，好吧，假设你可以制造一个虚拟工人，这将非常有用。我们所有人每天所做的大部分工作都不需要亲身在场，不需要机器人。我们所做的很多事情只是在处理信息，我们可以通过视频界面完成大部分事情。那么，为什么我们不能制造出非常有用的工人，在许多情况下扮演人们所扮演的角色呢？这有点像一个下限，可以做的事情。我认为可以做更多的事情，会有更多有趣的事情要做。然后就出现了应该做什么的问题。是的，这些都是丰富的哲学问题和经济实践问题。

**Craig Smith:**  是的，我看到了您，嗯，关于强化学习和监督学习的一件事，监督学习曾经占据主导地位一段时间，现在是基于 Transformer 的生成式 AI。但在监督学习阶段，当时的论点是，高级知识都是监督学习，并且它仍然是监督学习，在生成式 AI 和大型语言模型中，训练信息是下一个 token，下一个词，这被认为是正确的行动。您给我的类比是，因为总是给出的类比是，你知道，一个孩子看到一头大象，母亲说那是大象，孩子很快就能泛化并识别其他大象。也许孩子会犯错，母亲会纠正他，说“不，那是奶牛”。这总是被作为监督学习的例子给出，但也许这是强化学习，也许是孩子因为记住标签而从母亲那里获得的奖励。

**Richard Sutton:**  重点是，孩子在母亲说“那是一头大象”之前，就已经有了充分发展的概念类别，概念。孩子对空间是什么、物体是什么、以及这个被标记的事物是什么，已经有了广泛的理解。标签只是其中最不有趣的部分，孩子已经学会了所有其他最有趣的部分，关于拥有动物、移动物体和世界中的物体意味着什么。标签是最不有趣的部分。

**Craig Smith:**  好吧，首先，您谈论的是已经可以使用强化学习的虚拟工人。人们正在构建智能体，并使用大型语言模型和知识库来执行知识型任务。那么，您所谈论的不仅仅是语言任务或基于知识的任务，您谈论的是物理规划和物理任务，是这样吗？

## (49:27) 学习方法对决：强化学习 vs. 监督学习

**Richard Sutton:**  关键在于拥有目标。例如，如果你有一个助手来帮助你计划你的一天，组织你的一天，或者为你完成任务，我认为非常重要的是，系统能够拥有目标，并且能够理解你的目标。我认为，助手最重要的部分可能是理解所涉及的目的。大型语言模型并不真正理解所涉及的目的。它们似乎有点理解，但极端情况总是会出现，一旦你花一点时间，你总是会遇到极端情况。因此，一个 AI 系统是一个在一段时间后会做傻事的系统，这些傻事不尊重你拥有或已赋予它的目标。这样的系统不会是一个有用的助手。Ian，我不想批评大型语言模型，它们非常非常有用。但说它们同时也有相当重要的局限性，不应该被视为批评，从这个意义上来说，这不是一场竞争。

**Craig Smith:**  您是否担心，或者您是否认同威胁论的辩论？

## (53:10) 驾驭AI伦理与安全辩论

**Richard Sutton:**  不，我不担心，我不认同。我认为末日论者不仅是错的，我认为他们被偏见蒙蔽了双眼。偏见蒙蔽了他们对正在发生的事情的视线。基本上，AI 是一种广泛适用的技术，它不像核武器，不像生物武器，它可以用于各种用途，它不是，它是，我们处理这类事物的方式是我们尝试好好利用它们。会有人为了坏事而使用它们，这很正常，这很正常的技术，它可以被好人或坏人使用。末日论者只是说，哦，不知何故，它会像核武器一样糟糕，他们被这个比喻蒙蔽了双眼，认为 AI 会出来杀死他们，这太傻了，我不认为，末日论者实际上并没有给出他们所相信的连贯理由，所以很难与他们争论。也许公平地说，他们是有偏见的和盲目的。我不接受一个论点，除非它是一个适当的论点。

## (54:53) 2030年愿景：瞄准真正的AI智能？

**Craig Smith:**  您说您可能处于研究的第四阶段，卡马克说 2030 年，你知道，这足够遥远，也许人们不会记得他在 2030 年说过 2030 年。

**Richard Sutton:**  2030 年一直都在那里，它并没有消退，对于计算能力达到人类规模的数量来说，它一直是 2030 年。无论如何，2030 年对我们来说是一个合理的目标，理解我们需要的一切，以便制造一个真正的心智。我对此表示赞同。是的，你必须有雄心壮志。我一直说，2030 年有 25% 的机会实现真正的智能，真正的人类水平的智能，25% 的机会，所以可能不会成功，但这是一个足够大的概率，一个有抱负的人应该为此努力，并努力使其成为现实。这取决于我们做什么，而不仅仅是宇宙的展开。我们应该努力做到这一点。现在正在发生的一件大事是，公众正在逐渐认识到，对于理解心智并有能力创造有心智的事物意味着什么。因此，这是一个巨大的转变，我们世界观的巨大变化。因此，我们绝对需要各种各样的人来帮助我们，帮助我们变得容易，并对正在发生的事情有所了解，因为我们实现了人类水平的设计智能。

## (56:39) 总结：反思与展望

**Craig Smith:**  本周的节目就到这里。感谢 Richard 抽出时间。如果您想阅读今天对话的文字稿，可以在我们的网站 I on AI 上找到。网址是 eyeyenon.ai。同时，请记住，奇点可能越来越近，但 AI 已经在改变您的世界，所以请保持关注。