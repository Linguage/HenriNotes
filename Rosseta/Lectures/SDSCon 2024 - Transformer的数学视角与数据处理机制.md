SDSCon 2024 - Transformer的数学视角与数据处理机制  
- 原文标题：SDSCon 2024 - Philippe Rigollett - YouTube  
- 链接：https://www.youtube.com/watch?v=3McmEtA3t_0  

- **文章类别**：学术会议演讲  

---

**内容整理**：

### 文章信息
- **演讲者**：Philippe Rigollett  
- **会议名称**：SDSCon 2024  
- **演讲主题**：Transformers的数学视角与数据处理机制  
- **演讲时间**：2024年（具体日期未提及）  
- **演讲地点**：MIT（推测）  
- **演讲视频链接**：[YouTube链接](https://www.youtube.com/watch?v=3McmEtA3t_0)  

### 演讲框架
```
├── 开场与背景介绍
│   ├── 演讲者介绍
│   ├── 演讲主题引入
│   └── 合作者致谢
├── Transformers的背景与历史
│   ├── 神经网络的历史回顾
│   ├── ResNets与Transformer架构的对比
│   └── Transformer架构的兴起
├── Transformer的工作原理
│   ├── 输入与输出的数学表示
│   ├── Self-Attention机制
│   ├── Layer Normalization
│   └── 多头注意力机制
├── Transformer的数学建模
│   ├── 流映射（Flow Map）的概念
│   ├── 连续时间过程的视角
│   ├── Self-Attention的动态方程
│   └── 聚类现象的理论分析
├── 实验与结果
│   ├── 使用ALBERT模型的实验
│   ├── 内积分布的变化
│   └── 聚类现象的可视化
└── 结论与展望
    ├── Transformers的聚类特性
    ├── 研究的局限性
    └── 未来研究方向
```

### 演讲主要内容
#### 开场与背景介绍
Philippe Rigollett在SDSCon 2024上发表演讲，主题是关于Transformer的数学视角。他提到自己很久没有参加SDSCon了，并感谢了与他合作的团队成员，包括Borjan、Cyril和Yury等。

#### Transformers的背景与历史
- **神经网络的历史回顾**：Rigollett回顾了神经网络的发展历程，从Hinton团队的AlexNet开始，到ResNets的出现，再到2017年Transformer架构的诞生。  
- **ResNets与Transformer架构的对比**：ResNets主要用于图像处理，而Transformer架构则在自然语言处理（NLP）领域取得了巨大成功。  
- **Transformer架构的兴起**：Transformer架构的核心是“注意力机制”（Attention），这一机制极大地改变了神经网络的处理方式。

#### Transformer的工作原理
- **输入与输出的数学表示**：Transformer的输入是一个文本序列（如句子），经过编码后转化为高维空间中的向量集合。输出是一个概率分布，表示下一个可能的单词或标记。  
- **Self-Attention机制**：Transformer通过Self-Attention机制，让每个标记（如单词）根据其与其他标记的相关性动态调整权重，从而更好地捕捉上下文信息。  
- **Layer Normalization**：为了避免数值不稳定，Transformer在每一层后应用Layer Normalization，将数据投影到单位球面上。  
- **多头注意力机制**：Transformer通常会运行多个注意力头（Heads），并将它们的输出进行平均，以增强模型的表达能力。

#### Transformer的数学建模
- **流映射（Flow Map）的概念**：Rigollett将Transformer视为一种流映射，即将输入的分布映射到输出的分布。  
- **连续时间过程的视角**：他将Transformer的每一层视为连续时间过程的一个离散化步骤，从而可以利用动态系统的理论来分析其行为。  
- **Self-Attention的动态方程**：Self-Attention机制可以通过一个特殊的动态方程来描述，其中每个标记的移动方向取决于其他标记的加权平均值。  
- **聚类现象的理论分析**：通过数学建模，Rigollett发现Transformer倾向于将输入标记聚类到少数几个簇中，这一现象可以通过实验数据得到验证。

#### 实验与结果
- **使用ALBERT模型的实验**：Rigollett展示了使用ALBERT模型进行的实验结果，分析了不同层数下标记之间的内积分布。  
- **内积分布的变化**：随着层数的增加，标记之间的内积分布逐渐向右偏移，并在某些值上形成峰值，表明标记逐渐聚集。  
- **聚类现象的可视化**：实验结果表明，Transformer在处理文本时会将标记聚类到不同的簇中，这一现象与理论分析一致。

#### 结论与展望
- **Transformers的聚类特性**：Transformer通过其独特的注意力机制和动态系统特性，能够将输入标记有效地聚类，从而更好地捕捉上下文信息。  
- **研究的局限性**：当前的研究主要集中在理论分析上，对于实际应用中的复杂情况（如多头注意力机制、解码器结构等）还需要进一步研究。  
- **未来研究方向**：Rigollett提出未来需要更好地理解Transformer的压缩机制，以及如何利用其聚类特性来优化模型性能。

### 文章标签
#Transformer ， #NeuralNetworks ， #MachineLearning ， #NaturalLanguageProcessing

---

## 时间线与要点

**一、引言与背景 (0:00 - 2:05)**

*   **主讲人介绍及主题**: 主讲人很高兴回到SDSCon，将讨论Transformer，并介绍了合作者。
*   **Transformer的非统计性**: 强调本次演讲内容偏向数据科学而非统计学。
*   **Transformer的数学视角**: 提到了一篇关于Transformer的数学视角的综述预印本。
*   **SDSCon 2020的趣事**: 分享了为SDSCon 2020设计的T恤。
*   **Transformer的定义**: 从MIT、LIDS和深度神经网络的角度，引出Transformer的数学公式（交互粒子系统）。

**二、Transformer的历史与重要性 (2:06 - 5:37)**

*   **神经网络历史回顾**: 从AlexNet (2012) 开始，简述神经网络的发展历程，重点提及ResNets (2015) 和 Transformer (2017)。
*   **Transformer的应用转变**: 强调Transformer使自然语言处理 (NLP) 成为可能，并影响了计算机视觉。
*   **“注意力”机制 (Attention)**: 强调Vaswani等人在2017年提出的“注意力”机制是Transformer的核心，并影响了论文标题的命名。

**三、Transformer的工作原理 (5:38 - 9:22)**

*   **研究视角**: 关注已训练好的Transformer如何处理数据，而非训练过程。
*   **ResNets的回顾**: 介绍ResNets的结构和工作原理，即学习图像的表示，然后进行分类。
    *   **层 (Layers)**: 数据通过多个层进行处理。
    *   **参数 (Parameters)**: 每层都有训练好的参数。
    *   **映射 (Map)**: 将输入x0映射到输出xL。
    *   **公式**: xk+1 = xk + σ(Wkxk + bk)
*   **连续时间视角**: 将ResNets视为连续时间过程的离散化，方便后续分析。
    *   **动态系统 (Dynamical Systems)**: x'(t) = σ(Wt x(t) + bt)
    *   **流映射 (Flow Map)**: 将初始点映射到最终点。

**四、Transformer与ResNets的区别 (9:23 - 12:53)**
* **重点强调**: 理解ResNet和Transformer的差别.
* **符号简化**: 强调了xt dot的符号简化
* **训练动态**: 区分了数据动态和训练动态, 强调本文不是讨论训练过程
* **流映射**: 强调了流映射的重要性，将神经网络视为一种特殊的流映射。

**五、Transformer的输入与输出 (12:54 - 19:15)**

*   **从图像到文本**: 切换到更常见的文本应用场景 (ChatGPT)。
*   **输入处理**:
    *   **分词 (Tokens)**: 将句子分解为单词或子词。
    *   **位置编码 (Positional Encoding)**: 添加单词在句子中的位置信息。
    *   **嵌入 (Embedding)**: 将单词映射到高维向量。
    *   **输入表示**: 将输入表示为概率测度 (empirical measure)。
*   **输出处理**:
    *   **预测下一个token**: Transformer的目标是预测下一个token。
    *   **输出表示**: 输出为token上的概率分布，表示下一个token的可能性。
*   **Transformer作为流映射**: 将Transformer建模为概率测度空间上的流映射。

**六、Transformer的自注意力机制 (19:16 - 26:30)**

*   **动态 (Dynamics)**: 描述概率测度如何随时间变化。
*   **平均场交互粒子系统 (Mean Field Interacting Particle System)**:
    *   **粒子 (Particle)**: 每个token都是一个粒子。
    *   **速度 (Velocity)**: 每个token的速度取决于自身位置和其他token的聚合分布。
*   **连续性方程 (Continuity Equation)**: 描述分布如何随时间变化。
*   **自注意力动态 (Self-Attention Dynamics)**:
    *   **参数**: 值矩阵 (V), 查询矩阵 (Q), 键矩阵 (K)。
    *   **加权平均**: token的移动方向是其他token的加权平均，权重由Q和K决定的相似度决定。
*   **其他机制**:
    *   **层归一化 (Layer Norm)**: 将token投影回单位球面。
    *   **多层感知机 (MLP)**: 与自注意力层交替使用。
    *   **多头 (Multi-Head)**: 并行运行多个自注意力层，然后平均。

**七、Transformer的集群行为 (26:31 - 30:09)**

*   **研究问题**: 仅包含注意力机制的Transformer的长期行为是什么？
*   **实验观察 (ALBERT)**: 观察到token之间的内积分布向1移动，表明token聚集。
*   **类比**: 将Transformer与理想气体模型进行类比，简化模型以捕捉关键特征。

**八、简化模型与理论分析 (30:10 - 36:58)**

*   **模型简化**:
    *   仅保留自注意力层和层归一化。
    *   V, Q, K 简化为单位矩阵乘以参数β。
*   **简化后的动态**: token根据与其他token的距离加权平均进行移动，并投影回球面。
*   **反向梯度流 (Reverse Gradient Flow)**:
    *   **能量函数**: 类似于高斯核的排斥能量。
    *   **全局最大值**: 所有token位于同一点 (单集群)。
    *   **局部最大值**: 梯度流可能陷入局部最大值。
*   **理论结果 (部分)**: 当β非常大或非常小时，所有token会收敛到单集群。
*   **亚稳态 (Metastable States)**: 实际中，Transformer通常处于亚稳态，形成多个集群。
*   **集群数量**: 集群数量与β的平方根成正比。

**九、数据、熵与压缩 (36:59 - 39:14)**

*   **数据**:
    *   **输入数据**: 不同的prompt会映射到不同的集群状态。
    *   **训练数据**: 训练数据体现在映射的参数中。
*   **熵**: Transformer的动态以线性速率减少熵。
*   **压缩**: Transformer可能在压缩输入prompt，需要更好的理解。

**十、总结与展望 (39:15 - 43:58)**

*   **主要信息**: Transformers 使 tokens 聚集.
*   **亚稳态**: 亚稳态很重要，需要进一步研究。
*   **压缩**: 需要更好的理解Transformer的压缩机制。
*   **未来方向**:
    *   研究更复杂的K, Q, V。
    *   研究MLP层的影响。
    *   研究掩码注意力 (masked attention)。
    *   研究多头的影响。
* **Q&A**: 讨论了玻璃材料和transformer之间的相似性.

**要点总结:**

1.  **Transformer的核心是“注意力”机制**，它使模型能够处理变长序列数据（如文本），并在自然语言处理领域取得了巨大成功。
2.  **Transformer可以被视为概率测度空间上的流映射**，其动态由平均场交互粒子系统描述。
3.  **自注意力机制使token根据相似度进行加权平均移动**，导致token聚集。
4.  **简化模型显示，Transformer的动态类似于反向梯度流**，倾向于将token聚集到单集群或多个集群的亚稳态。
5.  **Transformer可能在执行某种形式的压缩**，需要进一步研究其与熵的关系。
6.  **未来的研究方向包括理解更复杂的机制、亚稳态和压缩机制**。

---

## 视频脚本

**一、引言与背景**

大家好，非常高兴能再次回到SDSCon。这次，我将和大家探讨一个偏向数据科学而非传统统计学的话题——Transformer。首先，我要感谢我的合作者们，Borjan、Cyril和Yury，我们曾在MIT共同研究这个课题。

今天的内容不会涉及太多技术细节。如果大家对技术细节感兴趣，可以参考我们去年12月发布的一篇综述性预印本论文：《Transformer的数学视角》。目前，Transformer是我的研究小组非常活跃的研究领域，我们也取得了一些新的成果。

在正式开始之前，我想分享一件趣事。我上次参加SDSCon是在2020年，但实际上那年并没有举办SDSCon。我非常热衷于T恤设计，所以当时我设计了一件SDSCon 2020的T恤，现在展示给大家看看，以免它永远被遗忘。

我上次缺席SDSCon是因为我当时正在休假。因此，我已经很久没有和MIT的统计与数据科学社区交流了，很高兴能再次回来。

今天，我将讨论Transformer。在MIT，大家可能会想到机器人；或者因为我与LIDS有关联，你们可能会想到一些不那么炫酷的应用。但今天是MIT统计与数据科学日，所以我将讨论的是深度神经网络Transformer。而且，由于我与数学也有关联，对我来说，Transformer可以用这个公式来表示，我们会把它解析为一个交互粒子系统。

**二、Transformer的历史与重要性**

在深入探讨之前，我想花点时间解释一下为什么我们应该关注Transformer，以及它与其他神经网络的区别。我将从一个不完整的历史视角来介绍Transformer和神经网络的发展。

神经网络在人工智能的“核冬天”中幸存下来。但真正让它复兴的是Hinton小组在2012年提出的AlexNet架构，它在ImageNet竞赛中取得了压倒性的胜利。从那时起，神经网络和机器学习就成了热门话题。

多年来，神经网络逐渐主导了机器学习领域。当然，这期间有大量的相关论文发表，但我将重点关注几个关键节点。首先是2023年（因为2024年会有更多），其中对我来说很重要的一项是ResNets（残差网络），它是由我们EECS的新同事何恺明提出的。

然后，在2017年，Transformer架构被提出。从那时起，出现了新的缩写词，比如大家熟悉的ChatGPT、GPT-4、Claude等等。

一个重要的转变是，我们从视觉应用（如图像分类、边缘检测）转向了自然语言处理（NLP）。Transformer架构使得这种转变成为可能，并对我们的生活产生了广泛的影响。

我们将尝试提炼出Transformer与以往不同的地方，并分析其特殊之处。Transformer对图像领域也产生了变革性的影响，但它在自然语言处理领域的应用最为广泛。它使模型从不可扩展变为高度可扩展，并带来了我们日常所见的各种应用。此外，Transformer的元素也被应用回计算机视觉领域，甚至传统的AlexNet也融入了Transformer的元素。

我一直提到的这个元素就是“注意力”（Attention）。2017年，Vaswani等人在谷歌发表了一篇论文，提出了“注意力”机制。这个机制对我们所做的一切都产生了巨大的影响，它成为了Transformer架构中的关键模块。

“注意力”机制的影响之一甚至体现在论文标题上。如你所见，在2024年，“is all you need”似乎成了一个流行语。如果你想使用这个短语，但又没有真正充分的条件，没关系，你可以使用“is not all you need”或“is all you need”。

**三、Transformer的工作原理**

在本次演讲中，我想从一个与以往理论研究略有不同的角度来理解Transformer。我不会试图理解如何训练神经网络，而是假设一个非学术实验室已经为我训练好了一个Transformer，我想知道它是如何处理数据的。

就像我使用ChatGPT一样，它已经训练好了。我输入一个句子，它在后台进行处理。我想了解这个过程中发生了什么。

可以把它看作一个函数，我们会看到它是什么类型的函数，以及它是如何为我们处理数据的。

我将从更经典的ResNets架构开始。ResNets是在2015年提出的，通常用于图像处理。

我们需要记住的是，神经网络首先会学习一个有效的表示。例如，图像的表示可以是一个包含像素强度的大向量。然后，神经网络会将这个图像转换为另一个向量，这个向量更适合进行图像分类。神经网络会进行所有这些计算，将输入x0转换为xL，xL就是学习到的表示。

在这个表示的基础上，我会应用一个经典的机器学习步骤，比如逻辑回归，来进行二元分类。也可以使用多项式回归。神经网络的最后一层通常是一个非常经典的机器学习方法。

在2012年之前，人们通常直接将输入图像xL放入这个分类器中。也有一些手工设计的特征，比如傅里叶系数，然后将它们放入经典的机器学习步骤中。

经典的机器学习步骤已经被充分理解，我将重点关注从输入图像到学习表示的这一部分。

这部分通过多个“层”进行处理，这里有L层。每一层都使用一些训练好的参数。这些参数决定了如何从一层到另一层，通过应用一个参数化的函数，将xK-1转换为xK。

将所有这些参数放在一起，我希望将这种表示视为一个映射，它将输入x0转换为xL。

这里的目标很明确，对吧？可以将它们视为d维向量，这里不再有标签，因为我只关心表示。

这个函数是以组合形式创建的。

你取第K层的值，

然后加上一个仿射变换，再对这个表示进行逐元素的非线性处理。例如，可以取向量每个元素的正数部分，然后加上前一个元素，再继续到下一层。

这个过程会重复多次，参数Wk和bk在每一层都会改变。

这就是经典的前馈神经网络，也称为多层感知机。

它定义了一个从x0到xL的映射，这就是我所说的神经网络。

为了简化符号，我将采用一个连续时间的视角，但这不会有太大影响。我认为可以将它视为一个连续时间过程的离散化，其中xk+1 - xk可以看作变化的方向，我将其表示为xt dot。

这里，t就是k，然后我将其表示为σ(Wt x(t) + bt)。

就好像我有一个由时间索引的连续层。这种视角已经被广泛应用，它可以与动态系统和控制建立联系。许多热门论文都利用了这种连续方法，关键词是“神经ODE”。

但这并不重要，只是用xt dot代替xk+1 - xk对我来说更方便。

我有一个动态系统，输入图像在这个系统中演化。

如果你听说过与神经网络相关的动态，那可能不是我们这里讨论的动态。你可能听说过的是训练动态。

训练动态通常用于只有一个隐藏层的情况，你只需要学习一个表示，然后进行机器学习步骤。相关的名字有Mei、Montanari、Vanden-Eijnden、Jacot、Chizat和Bach等。

这里的动态不是关于x，而是关于θ，也就是神经网络的参数。当你试图最小化关于θ的损失时，就会应用梯度下降，这就是训练动态。

但这不是我们这里讨论的内容。如果你从未听说过，不用担心。如果你听说过，请记住，这不是我们讨论的内容。我们考虑的是一个已经训练好的神经网络，θ是给定的。

**四、Transformer与ResNets的区别**

我们把重点放到 `xt dot = σ(Wt x(t) + bt)` 这个公式。

现在，我希望将神经网络视为一个告诉我如何从x0到达xt的工具，这被称为流映射。我昨天在讲授最优传输时也提到了流映射。

流映射就是当你对一个常微分方程进行积分时得到的结果。我给你动态，你想知道从给定的初始点出发，我的ODE会到达哪里，在时间t，它会到达某个位置。

我希望将神经网络视为一个流映射。然后，我对所有点应用相同的函数，一次一个。它只是一个以特殊方式参数化的函数。

从统计学的角度来看，我认为这很有趣。我们有一些从Rd到Rd的函数，如果让我提出一类从Rd到Rd的函数，我可能会使用张量化的方法，比如再生核希尔伯特空间、Sobolev函数等。我会说输出向量的每个坐标都是平滑的，比如平滑类是经典的。

我认为神经网络的主要优势之一在于，它们将这些映射参数化为流映射。它们不是直接参数化映射本身，而是参数化曲线，这条曲线允许我们使用这些动态从一个点到达另一个点。

**五、Transformer的输入与输出**

现在是2015年，我刚刚描述的是残差网络。但今天，我们要讨论的是Transformer。Transformer是最近新闻的焦点。

它们有什么不同？正如我提到的，从图像切换到文本可能更容易理解，因为这是你更常见的应用。我相信每个人都在使用ChatGPT来提高社交技能，我也是这样做的。

你会在ChatGPT中输入一个提示，它只是一个句子。Transformer首先会对句子进行分词，得到输入向量。它会将句子分解为“token”，这些token可以是单词或单词的一部分（如果单词太长）。

然后，你会添加一个编码，告诉模型单词在句子中的位置。例如，单词“quick”，你可能想知道它是句子中的第二个单词，因为它的含义可能取决于它在句子中的位置。

一种简单的方法是获取单词“quick”的嵌入，可以将其视为高维向量，然后在末尾添加数字2，表示位置。有很多复杂的方法可以做到这一点，但原则上，它们只是告诉你单词在原始句子中的位置。

一旦我将所有内容（单词嵌入和位置编码）组合在一起，我可以学习它，或者直接使用一个随机编码，只要它能以一致的方式将单词映射到向量即可。

每次我看到这个单词，我都会将它映射到相同的位置，只是后缀可能会根据它在句子中的位置而有所不同。

然后，我就得到了一个字典。现在，我有了token，假设它们在Rd中，它们就是这些带有位置信息的单词。

这就是我的提示，它的长度是n，所以我在Rd中有n个向量。

为了给大家一个数量级的概念，这些数字当然在增长。这可能已经过时了。我听说人们的目标是数十亿，但我认为n和d是相当的，并且n略大于d。

当然，你也可以对图像进行同样的处理。我提到Transformer不仅限于文本，你也可以将它应用于图像。

怎么做呢？你可以将图像分割成小块，而不是将图像视为一个长向量。然后，每个小块都会被发送到一个特定的编码向量。

你只需记录它的位置。例如，如果你使用字典序来读取图像，你就添加它在图像中的位置。

这是一种不同的方式。你不是将图像视为一个长向量，而是将每个小块视为一个单元，你通常会将其舍入到一个小块字典中，因为小块太多了。然后，一旦你有了每个小块，你就可以处理它。

你将获得一些关于它的邻居的信息，并将其作为单元，而不是将像素作为这里的单元。这在图像处理中非常成功。

现在，让我们忘掉应用，思考一下Transformer在做什么。Transformer的输入是什么？输入是一个提示，但从数学上来说是什么？

我有x1到xn，但由于我知道位置编码，我可以忽略顺序。我不再需要x1到xn的序列，我只需要x1到xn的集合。

这些是我的token，它们在Rd中，我不关心它们的顺序。

我将把这个集合转换为一个经验测度，它就是xi位置的经验测度。

这些是完全等价的。如果你给我左边的，我可以构建右边的；如果你给我右边的，我可以构建左边的。

它只是一个多重集合。我应该写成多重集合，然后我只有经验测度。

如果我有多个相同的元素，我会在前面加上系数，比如2/n。但这种情况不会发生，因为没有两个单词会出现在相同的位置。

在我的故事中，Transformer的输入将是token上的概率测度，它告诉我输入的经验分布。

输出是什么？Transformer实际上是训练来预测下一个token的。

这也是他们的主要工程见解之一。他们说：“没有人会告诉我这个句子是浪漫的、美丽的还是具有攻击性的。”文本没有标签。你能想到的标签是什么？

下一个出现的单词可以提供关于前面内容的信息。

因此，你将使用下一个token作为句子的标签。你将进行经典的机器学习，根据一个句子预测下一个token。

你应该输出的是下一个token。但我如何输出下一个token？我很可能无法100%确定下一个token是什么。

因此，我将给你一个token上的概率测度，它告诉我下一个token的可能性。

我的输入是token上的概率测度，我的输出也是token上的概率测度。

它们的含义不同。一个是经验分布，另一个是下一个token的可能性。

但这使我能够像ResNets一样，将从token上的概率测度到token上的概率测度的映射建模为流映射。我只需要将token的概率分布从一个位置移动到另一个位置。

这就是我可以使用神经网络的原因，它们非常灵活，可以将函数从一个集合（比如抽象空间x）建模到相同的抽象空间x，只要它可以在这个空间中连续移动。

现在，我希望将Transformer视为参数化的流映射。仍然会有一些参数，这些参数是由OpenAI训练的数十亿个参数，它们将初始分布映射到最终分布。现在，P(Rd)是Rd上的概率分布空间。

从图像上看，我从一个图像开始，这就是我的提示，它是token的经验分布，我将移动整个分布。希望它会集中在我认为是下一个token的位置。

这就是Transformer为我做的事情。这个观点已经在2022年的一篇关于Sinkformers的论文中提出，但可能没有得到足够的强调。

**六、Transformer的自注意力机制**

现在，我需要告诉大家这些测度的动态是什么。我告诉过大家x dot是什么，它有一个很好的公式。

这里，我必须告诉大家μt dot是什么。如果你不想被偏微分方程吓到，你不能写μt dot，你必须写dtμt，但它们是相同的。

所以我们写dtμt。我知道μ0是token的经验分布。我如何移动一个分布？

你给我token，我想移动它们的分布。如果你看看我刚刚展示的图像，我移动分布的方式是移动点的位置，这是一种移动分布的方法。有很多方法可以移动分布，我可以改变权重，但我可以移动位置，这就是Transformer实际上在做的。它们实际上实现了一个平均场交互粒子系统。

这里，粒子就是token，token i的速度由一个向量场给出。大写XT表示一个向量场。它取决于XT在哪里，Xi在哪里，也就是这个token在哪里，Xi(t)，但它也取决于所有其他token，但不是以任何方式。它只通过它们的聚合分布来依赖于所有其他token。这就是平均场的意思。它以相同的方式看待所有其他token。

它不会说：“我只根据我的最近邻移动一半的步长，然后根据我的次近邻移动三步，然后token 25把我带到那里。”你不能对token进行排序并区别对待它们。你只能以聚合的方式处理它们。

我们会看到具体如何做到这一点，但从高层次来看，这就是它的实际工作方式。

现在，我告诉了大家每个token如何移动。你想知道分布如何移动，这是一个标准的练习。你只需将这个速度场代入一个连续性方程。

它会告诉你dtμt根据负的μt乘以这个速度场的散度来移动。

Transformer在做什么？它们只是对这个速度场做了一个特殊的选择。

我将要讨论的自注意力动态是Xtμt的一个特殊选择。这个特殊选择有点拗口，所以我们需要花点时间一起解析它。

有三个参数，还记得吗，这是一个参数化的流映射，所以有一些参数是在OpenAI实验室训练的。这些参数在每一层（时间t）都是三个矩阵：Vt（值矩阵）、Qt（查询矩阵）和Kt（键矩阵）。

使用这些矩阵的方式如下。我希望大家看看右边，而不是解析这些积分。

它的作用是，实际上，一个token的方向看起来像是其他token的平均值，相对于一个分布，这个分布是其他token分布的一个倾斜版本。我稍后会提到这个分布是什么，然后我有一个预处理项Vt，因为，为什么不呢？你总是可以学习一些关于动态的预处理，所以你只需这样做。

关于Vt，我没有太多要说的。也许可以把它看作单位矩阵。在接下来的内容中，它将是单位矩阵。

但它只是你学习的一些预处理。

现在，什么是这个倾斜的测度？我认为这才是真正重要的。

如果没有倾斜，那将只是所有其他token的平均值，对吧？它会说，只需沿着所有其他token的平均方向移动。

但是在这里，你没有以相同的方式对所有其他token进行加权。这是Y的平均值，但我实际上重新加权其他token的分布的方式是，对那些远离我的token赋予较小的权重，也就是内积较小的token。

我测量接近度的几何结构由Qt和Kt决定，实际上只有Qt的转置乘以Kt起作用。

基本上，我说的是，我沿着token的加权平均方向移动，我对那些在某种意义上靠近我的token赋予更多的权重，这种接近度是根据Kt和Qt来衡量的。

这就是你如何做到这一点。

我把它写成积分的形式，因为这对我来说更方便。但是，如果你从μt开始，它只是经验测度，对吧？当你有n个token并且你确定性地移动它们时，它们将保持n个token。在这种情况下，所有这些积分实际上都是求和。

你可以看到，token i（如果你忽略这里的Vt）是根据这个移动的。对于每个i，Pij的集合是矩阵的一行。这个矩阵是随机的，所以这些元素是非负的，并且和为1。它实际上只是每个j的加权平均，但权重会根据你在这种几何结构中的邻居而从token到token发生变化。

这就是自注意力层的核心，它是Transformer中的一个重大进步。

当然，在我一开始展示的图中还有其他内容。在神经网络中，你会做很多事情。

我会提到它们。其中一个非常重要的是层归一化（Layer Norm）。

层归一化是指，当我进行这些动态时，它们可能会趋向于无穷大。为了防止它们趋向于无穷大，每次我移动token时，我都会将它们投影回球面。

这被称为层归一化。在连续时间中，它等同于说，无论自注意力层产生什么速度场，我都会将其投影到球面的切空间上，这迫使我的所有粒子都留在球面上。

我不允许沿任何法线方向移动。现在，我的动态在球面上演化。

实际上，还有一些非常重要的东西，但它们增加了复杂性，所以我们不会讨论它们。一个是多层感知机（MLP）。

在进行自注意力之后，通常会穿插一些多层感知机，就像我们之前做的ResNets一样。因为，为什么不呢？如果你可以添加更多参数，你总是可以这样做。

人们做的另一件事是并行运行一堆这些东西，然后将它们平均在一起。这些被称为“头”（Heads）。当你并行运行K个时，假设你并行运行20个，然后在每层之后将它们平均，所以你首先进行注意力层，然后进行MLP层，然后进行归一化层。

实际上，这可能应该在另一边。所以你进行归一化层，然后平均你看到的一切，然后继续。

这里，投影应该在另一边，因为我想留在球面上。抱歉。

我们不会提及这两件事，因为我试图提取关于自注意力的基本内容，这里有太多的东西了。

**七、Transformer的集群行为**

我要问的问题如下。现在，我有一些只包含这种新机制（称为注意力）的动态。我想了解，这会走向何方？

我们将其视为你获取一个初始提示并将其发送到一个最终的可能性。

正如标题所示，我希望……当然，每个初始提示都会将我发送到不同的μt。大的渐近行为将是不同的。显然，它是一个函数。它不会将所有人都发送到同一个地方，但我想看看我得到的限制分布是否有共同的特征。

我们将展示的是，它们倾向于成为集群分布。

在进行一些理论分析之前，让我们看看这在实践中是否有意义。

这里，我们下载了一个名为ALBERT的东西，它是一个轻量级的BERT，所以它是一个带有权重共享的BERT，你可以在Hugging Face上获得它。

我们在这里绘制的是每个层之后token之间内积的直方图。或者实际上，是在一定数量的层之后。我不想永远这样做。

这是第0层。这是维基百科中一个特定句子的嵌入，你可以看到内积的分布。这里，它不是很对称。这里有一些训练嵌入。

现在，我想看看当我沿着层移动时，它是如何变化的。

如你所见，它开始向右移动。这意味着所有这些家伙可能正在某种程度上靠近。

向右移动，你可以看到，在1处形成了一个点质量。对于两个token来说，在球面上内积为1意味着什么？这意味着它们在相同的位置。

当我继续前进时，这些家伙开始出现尖峰，这意味着我实际上有一些集群。

这是最终的结果。在这里，你实际上可以计算峰的数量。

看起来是3选2。这意味着，基本上，应该有三个集群，三个成对距离。但如果有相同的成对距离，可能会更多。

这实际上是一个有趣的问题，类似于我从Michel那里学到的部分摘要问题或Turnpike问题。

从成对距离中恢复元素的问题通常是在整数上研究的。但我们可以在高维球面


好的，继续输出：

但我们可以在高维球面上进行这项研究，并提出类似的问题。 无论如何，这是一个完全题外话。

正如你所看到的，从初始分布到最终分布，我们显然从一个非集群配置转变为一个集群配置。我们的目标是解释这里发生了什么。

Transformer是复杂的机器，对吧？请记住，有很多事情正在发生。有训练参数、注意力层、多层感知机、归一化层。我是说，还有多头。有很多事情正在发生。

这是一个非常复杂的对象。我的目标是提取一个模型，这个模型能够提供足够的信息，让我能够重现这个特定的属性。

这只是一张图片，但实际上，你可能想到的正确的类比是理想气体，玻尔兹曼的思想是说，你从一些复杂的气体开始，这些气体具有一些复杂的分子，以不同的方式碰撞，然后你说，好吧，这些只是以某种方式碰撞的硬球模型。然后你想从中恢复热力学第二定律之类的东西。

只是某些宏观特征。因此，我们将显著简化我们的模型。

首先，我们只关注注意力和层归一化。

在我们最初的尝试中，我们没有研究层归一化。我们确实看到了一些集群，但由于事物趋向于无穷大，它与实际情况相差甚远。实际上，球面上的动态更有趣。

所以我们将保留层归一化。

但这相当复杂，对吧？

我的意思是，有Kt、Qt，它们在每个时间都会变化。那里还有Vt。这些都是非常复杂的东西。

我应该选择什么？好吧，我们将让我们的生活变得非常轻松。我们将基本上将它们全部设置为等于单位矩阵。

Vt将是单位矩阵，Qt、Kt将是一个参数β。我仍然希望在这里有一个调节旋钮，一个温度参数β乘以单位矩阵，这基本上将我的动态变为如下形式。

它仍然是token的加权平均，但权重只是根据它们在经典几何（欧几里得几何）中的内积进行指数衰减的。

大家有时间理解这个东西吗？这就是我们将要研究的动态。

你根据邻居的距离对它们进行加权平均。β基本上是一个尺度参数，它告诉你实际上你想将多远的人视为邻居。

它只是给你一些范围。然后我们将其投影到球面上，以便我们留在球面上。

事实证明，如果你看看这个东西，你实际上可以……

我会讲得快一点，但在某种意义上，我会说，它是球面上某个排斥能量的梯度流。

这是我们正在研究的能量。

这类似于高斯核，你基本上是在观察e的负距离平方次方。如果你要最小化这个能量，这是Henry Cohn和Kumar在一篇论文中发现的能量。当你最小化它时，你会得到一些最优配置。

这是一个非常复杂的景观。你会得到不同的最优配置，这取决于点的数量和维度。

但我们不是在最小化这个能量。如果你看，这实际上不是某个东西的负梯度，而是某个东西的梯度。所以我们不是在做梯度下降，而是在做梯度上升。

我将称之为反向梯度流，就像我们谈论反向热流一样。在这种情况下，很容易说服自己，全局最大值就是当所有点都位于同一个地方时，对吧？我试图最大化这个东西。

这是正的，所以我想最小化里面的东西。我最小化它的方式是使所有这些指数都等于0。我使所有这些指数等于0的方式是将所有点放在相同的位置。

这些是全局最大值。它们都是一个单一的集群。

如果我的动态试图最大化这个东西，并且达到全局最大值，我将不会有一个一般的集群，我将只有一个集群。

这里的问题是，梯度流是否会陷入局部最大值？

答案是……我们有一个非常部分的答案。如果你有兴趣解决一些技术问题，我们可以证明以下内容。

如果β大于n的平方，其中n是粒子的数量，所以温度非常低……或者β小于1/n，所以温度非常高，这意味着什么？当β非常小时，这意味着我基本上只根据我的最近邻移动。当β非常小时，我基本上是根据所有人的平均值移动，而没有真正进行任何加权。

我们可以证明一些东西，这已经让我们很高兴了，但是如果你让n趋向于无穷大，你可以看到这是完全空洞的。这些东西会消失。我们实际上认为对于所有β，这都是正确的，所有这些粒子实际上都会碰撞到一个点，无论你运行什么β，但这需要很长时间才能到达那里。

这很好。但实际上，如果你考虑我给你的ALBERT的例子，它不仅仅是一个单一的集群，对吧？有很多集群点，但我们说可能存在三个集群，如果这些集群之间存在相同的距离，则可能更多。

事实证明，如果我们运行……即使我们在球面上使用简单的动态，对于合理的β选择，我们实际上会看到这个。

如果β非常小或非常大，它将变成一个单一的集群，就像我们可以证明的那样。但在这种情况下，我们实际上最终会处于这种状态。

但是，如果你实际上盯着这个看一会儿，你会发现点仍在移动，尽管非常缓慢。

我们实际上处于一个亚稳态。我们处于这个能量函数的鞍点，我们正在寻找方向，我们只是非常缓慢地移动。梯度非常小，但我们仍在移动。

如果我们有耐心，我们会等待，两个集群会合并成一个。三个集群会合并成两个集群。然后在另一个时间尺度上，两个集群会合并成一个集群。

这些东西……因此，亚稳态显然在实践中很重要，对吧？我的意思是，我们没有无限数量的层。

我们有有限数量的层，这才是真正重要的。目前，我们几乎没有工具来理解如何研究确定性动力系统的亚稳态。我知道如何研究随机系统的亚稳态，对吧？因为你有布朗运动来激发粒子，并且存在一个逃逸时间，在某个时刻，我会从随机噪声中获得足够的偏差，从而逃离局部最小值。

但是这里，我没有局部最小值。我实际上只想逃离这个。我们唯一知道的是对一个名为Allen-Cahn或cannular的偏微分方程的研究，其中出现了类似的现象，存在一些完全确定性演化中的亚稳态，但我们现在不知道如何应用它们。

但我们正在研究这个问题，我们可以证明某些状态是亚稳态的，但我们不能证明无论我们从哪里开始，我们都会最终处于亚稳态。

你可能会问的另一个问题是，有多少个集群？

这里只有一个参数，β。我说过，如果我把β取得很小或很大，我基本上只会得到一个集群。

事实证明，我们可以证明或者我们可以得到一些关于β如何控制我们实际获得的集群数量的见解，它的数量级是β的平方根。你可以说服自己，这就是将会发生的事情。这是因为1/√β……假设一切都在圆上。

1/√β是你观察邻居的范围，对吧？所以，基本上，你在覆盖。因此，它们就像大小为1/√β的组一样行动，由于圆的长度是恒定的，你将拥有√β个这样的组。基本上，这就是你如何获得你的状态。

为此，我们实际上研究了……这实际上是统计部分。我没有太多时间讲这个，但我们使用[听不清]公式和[听不清]研究了核密度估计器的模态数。

当我做这个演示时，我总是被问到的问题是，你的数据怎么了？你是不是告诉我，只有一个数据，每个人都进入一个集群状态？我在做什么？

有两件事正在发生。第一件事是，有两种类型的数据。

有输入数据，也就是我的提示，还有训练数据。让我们从输入数据开始。

你给了我一个提示，这个提示被发送到一个集群状态。

我有一个映射，它只由β参数化，但通常，它由所有这些KQV参数化。

如果我给它另一个提示，我会得到另一个集群状态。

我有一个映射，它将不同的初始分布映射到不同的最终分布。所以，在这里，我肯定没有丢失我的数据。

我并不是说这个映射是单射的或任何东西，但肯定有一个映射，它对不同的输入做不同的事情。

当然，我的训练数据在哪里？实际上，我的训练数据在这个映射的参数中。

我们所做的是，我们说，好吧，这个映射的行为……所以这个映射将每个人都发送到集群分布的集合。

在这个集合中，它肯定会对我们得到的输入数据进行区分。我们找到了一个更简单的映射，它做同样的事情，并且具有类似的形式。

你可能会问的一个问题是，这是如何……我们可以证明这个东西会变成一个集群状态，对吧？问题是，它会做什么？

最可能的结果是什么？现在，让我们更深入地研究一下，说，好吧，如果我有两个不同的输入分布，我能否说出它们的最终分布会有什么不同？

我们可以证明的是，这并不是负熵的梯度流，但它实际上以线性速率减少熵。

这意味着如果你取负熵，它会线性增长。所以这做了什么。它基本上是从你的初始分布中吸取熵。

当然，这里我需要一个具有密度的分布μ，这样我才能定义这个量。

但如果我开始……我总是可以在连续分布上定义这些动态。它的作用是，基本上，从分布中吸取熵。

你有这个平滑的东西，它只保留了一些集群。所以它非常……这就是反向热流会为我们做的，对吧？这些非常不稳定，但这就是我们试图做的。

我们试图吸取熵。

我要总结一下了。我的主要信息是，Transformer会聚集token。

我希望有人会问我关于神经坍缩的问题，这样我就可以告诉大家这与神经坍缩有何不同。

我认为亚稳态……所以在这里，我们花费了……我们的大部分数学是关于证明我们渐近地到达一个单一的集群，但这需要指数时间才能到达那里。因此，我们长时间陷入的那些亚稳态非常重要，我们需要研究它们。

我希望对这个压缩方案有更好的理解，对吧？

我的意思是，证明熵增长，我想知道是否有一个更好的熵概念，这个东西实际上是在试图最大化，也许可以得到一些稳定性。某种东西可以告诉我，好吧，这实际上是在压缩你的输入提示，我想了解。

当然，还有通往完整Transformer的垫脚石，从添加多个KQ和V开始。

例如，我们进行了一些实验，表明如果K、Q和V是随机选择的，那么我们会观察到类似的现象。

当然，它们是，我会说，宏观上是定性的……MLP层的作用是什么？

实际上，人们并不关注所有的token。每个token不仅关注所有token，通常只关注先前的token。

这些被称为解码器Transformer。这就是ChatGPT正在做的，这些被称为掩码注意力。你只关注其他token的一个子集，我们也在研究这个问题。

理解头的作用也是我们没有触及的问题。我认为现在是研究Transformer的一个非常激动人心的时刻。

带着这些问题，我想结束了。感谢大家的关注。

[鼓掌]

[听不清] 有人向Philippe提问吗？

听起来你在谈论玻璃。在材料科学中，这种方法……

到一个子空间而不是一个答案或一个高度对称的解是人们线性担心玻璃的原因。

没错，所以这……

这是它们终点的一个捷径。是的，所以Allen-Cahn正在研究这个介质，对吧？

他只是展示了事物如何通过一些集群状态坍缩，然后开始合并在一起。

我主要对数学方面感兴趣，但绝对地，我提到的那些偏微分方程正是在玻璃的背景下研究的。

有一些工具。所以Felix Otto和Maria Reznikoff有一篇论文，他们介绍了慢流形假设，他们在研究正在发生的事情。

他们基本上表明，你会坍缩到一个小的……到一个流形上，你在上面移动得非常缓慢，从这个流形，你开始坍缩到另一个流形上。

所以，是的，有一些工具，只是不能直接应用在这里。但是，是的，它是……但也许在物理层面上，有更多的工具，但在数学层面上，它们还没有像我希望的那样发展。是的。这是我的感觉。

就物理理解而言，对于玻璃，存在一个有组织的状态，它在玻璃所能达到的范围之外。

好的，绝对的。

这正是Transformer正在发生的事情，对吧？单一集群是这个有组织的状态，我们在实践中永远不会看到它具有有限数量的层。即使我们有连续的层，我认为……

所以这里有两件事，对吧？当你将你的层变为连续时，你不必将时间范围设置为无限。

你可以有一个连续的层，从时间0到时间1。Transformer在实践中所做的是，他们说，“好吧，让我们开始……让我们决定事物何时开始结晶，将水平线放在那里。然后，无论你给我多少层，我都会尽可能精细地使用离散时间，直到这个时间。”但从来没有人……这些东西永远不会达到无穷大。

我让这个变成一个单一集群的方式……抱歉，通过这个坍缩，实际上是采用一个已经训练好的Transformer，然后将它堆叠在自身之上。

实际上，我展示了40多层，但我们使用的Transformer只有，我不知道，15层。所以我们只是将它们堆叠在一起，这样我们就可以人为地增加这个Transformer的层数。

你将问题分解，并查看收敛到部分解的部分，然后从那里返回。是的。我们不知道如何做到这一点。

我的意思是，这是一个好问题。我们只是不知道如何做到这一点。所以，也许我们把剩下的问题留到线下讨论。

我们有一个休息时间，直到10:30，届时我们将继续进行闪电演讲，然后我们将有[听不清]的下一个全体演讲。

让我们再次感谢Philippe。[鼓掌]
