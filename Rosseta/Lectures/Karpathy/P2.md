
好的，完全没问题！我们将继续为您生成 **第二部分：深入 ChatGPT 交互原理与揭示模型本质** 的详尽教程文章。我们将继续秉持详尽和充分利用原文素材的原则，力求为您呈现一篇深度解析 ChatGPT 交互机制和模型核心概念的教程。

### 深入 ChatGPT 交互原理与揭示模型本质

**ChatGPT 交互的基石：文本输入与文本输出**

现在，让我们真正开始深入探索 **ChatGPT** 的奥秘。首先，我们从最基础的层面入手： **ChatGPT 的交互原理**。当我们打开 ChatGPT 的界面，最先映入眼帘的，通常是一个简洁的 **文本输入框**。那么，这个文本框究竟是用来做什么的？我们应该在里面输入什么内容呢？

理解与语言模型交互的 **最基本形式** 非常重要：那就是 **我们输入文本，然后模型返回文本响应**。这种看似简单的文本输入与输出，构成了我们与 LLM 沟通的桥梁。为了更形象地说明这一点，让我们从一个简单的例子开始。假设我们想让 ChatGPT **写一首关于大型语言模型的俳句**。俳句，作为一种源于日本的短诗形式，以其简洁的语言和深刻的意境而著称。LLM 在 **写作** 方面展现出了惊人的能力，无论是俳句、诗歌，还是更实用的求职信、简历，甚至是邮件回复，它们都能 **游刃有余** 地胜任。

当我们向 ChatGPT 提出这样的请求——“写一首关于大型语言模型的俳句”时，模型会迅速进行处理，并给出响应，就像这样：

> “词语如溪流，
> 无尽回声，思想不复，
> 思绪无踪影。”

（原文英文版: “Words flow like a stream, endless Echo never mind, ghost of thought unseen.”）

这首俳句，虽然寥寥数语，却颇具 **戏剧化的意境**，仿佛在用诗意的语言，描绘了 LLM 运作的某种神秘感。在 ChatGPT 的界面上，我们看到的是以 **类似与朋友聊天** 的 **对话气泡** 形式呈现的交互结果。用户输入的信息显示在一个气泡中，模型的响应则显示在另一个气泡中，这种直观的对话形式，使得与 LLM 的交互变得更加自然和亲切。

**Token 化：文本的数字化表示**

在之前的视频中，我们已经了解到，无论是用户输入的 **查询文本**，还是模型生成的 **响应文本**，在 LLM 的底层，都会被 **分解成一个个小的文本块**，我们称之为 **“tokens”**。这个文本序列，在模型的内部，实际上被表示为一个 **一维的 token 序列**。理解 “token” 的概念，是深入理解 LLM 工作原理的关键一步。

为了更直观地了解 tokens 的构成，我们可以使用 **Tik tokenizer** 这样的在线工具。Tik tokenizer 是 OpenAI 官方提供的 tokenization 工具，它可以帮助我们**查看文本被分解成 tokens 后的具体形式**。例如，我们可以选择 **GPT-4 模型**，然后将我们刚才的俳句请求文本——“写一首关于大型语言模型的俳句”——粘贴到 Tik tokenizer 的文本框中。点击 “Tokenize”，工具就会立即显示出模型 **实际看到的内容**。

对于模型来说，我们输入的这段文本，并不是我们人眼所见的文字，而是一个由 **15 个 tokens** 组成的 **序列**。模型的 **词汇表** 中，大约有 **20 万个可能的 tokens**，Tik tokenizer 显示的，正是与我们的查询文本相对应的 **token ID**。您可以尝试修改输入的文本，实时查看 token 序列的变化，感受文本与 tokens 之间的转换关系。

通过 Tik tokenizer，我们可以看到，我们的 **查询文本** —— “写一首关于大型语言模型的俳句” —— 被分解为了 **15 个 tokens**，而模型 **生成的俳句响应** —— “词语如溪流，无尽回声，思想不复，思绪无踪影。” —— 则由 **19 个 tokens** 构成。这首俳句，正是由这 19 个 tokens 按照一定的顺序排列组合而成的。

**对话格式的幕后：构建一维 Token 流**

由于 ChatGPT 的交互形式是 **对话**，因此我们需要 **保留** 大量 **元数据**，以维护对话的上下文信息，例如对话历史、用户身份等等。实际上，在底层，情况要比我们看到的对话气泡 **复杂** 一些。为了让模型更好地理解对话的上下文，我们需要将用户查询 **转换** 为特定的 **聊天格式**。

为了简化理解，我在这里 **删除** 了 **系统消息**，因为它对于理解当前情况来说，并不是至关重要的。我们可以将用户的消息标记为 **用户输入**，然后将模型的响应标记为 **助手输入**。

通过这样的标记，我们就能更清晰地看到 **实际的底层情况**。在 tokens 序列中，会存在一些 **特殊的 tokens**，用于 **表示用户消息的开始**，然后是 **用户实际说的话** (tokenized)，接着是 **用户消息的结束**，然后是 **助手消息的开始**，以此类推。虽然 **对话格式的具体细节** 并不重要，但我想表达的核心观点是：在我们看来是 **来回的聊天气泡**，但在底层，我们实际上是在与模型 **协作**，共同 **写入一个 token 流**。这两个气泡，实际上构成了一个由 **42 个 tokens** 组成的 **序列**。**我贡献了前面的 tokens** (用户输入)，然后 **模型继续生成后面的 tokens** 作为 **响应** (助手输出)。我们可以 **交替添加 tokens**，共同构建一个 **token 窗口**，或者更准确地说，一个 **一维的 token 序列**。

回到 ChatGPT 的界面，我们看到的仍然是 **来回的聊天气泡**，但现在，我们应该在脑海中建立起一个更深层次的理解：在这些看似独立的对话气泡背后，隐藏着的是一个 **一维的 token 序列**。当我们点击 **“New Chat”** 按钮时，会发生什么呢？正如我们之前讨论过的，点击 “New Chat” 实际上会 **清除** 当前对话的 **token 窗口**，将 **tokens 数量重置为零**，从而 **重新开始一个全新的对话**。

**上下文窗口：对话的“工作记忆”**

在我与模型进行对话时，我脑海中浮现的图景是这样的：当我们点击 **“New Chat”** 时，就如同 **开启了一个全新的 token 序列**。这是一个 **一维** 的、线性的 **token 序列**。**用户** 可以向这个 token 流中 **写入 tokens**，当我们按下 **回车键** 时，对话的 **控制权** 就 **转移到语言模型**。语言模型接收到用户输入的 tokens，然后开始 **生成它自己的 token 流** 作为 **响应**。语言模型在生成响应的过程中，会持续不断地输出 tokens，直到它生成一个 **特殊的 token**，这个特殊的 token 表示 **“我说完了”**。当 ChatGPT 应用接收到这个 “结束” token 时，它会将对话的 **控制权交还给我们**，我们就可以 **轮流发言**，你一句，我一句，就像人类之间的对话一样。在这个过程中，我们与模型共同 **构建** 了一个 **token 流**，这个 token 流，就是我们所说的 **“上下文窗口”**。

**上下文窗口**，可以形象地比喻为这次 **对话的“工作记忆”**。模型可以 **直接访问** 上下文窗口中的 **任何内容**，包括用户之前输入的所有信息，以及模型之前生成的所有响应。正是 благодаря 上下文窗口的存在，模型才能够理解对话的 **上下文语境**，记住之前的对话内容，并在后续的对话中，保持 **对话的连贯性** 和 **一致性**。

**LLM 的本质：互联网知识的压缩容器**

现在，让我们进一步思考一个更深层次的问题：我们正在与之交谈的 **这个实体**，究竟是什么？我们应该 **如何看待它**？在之前的视频中，我们了解到，语言模型的 **训练** 过程，主要分为两个 **主要阶段**： **预训练 (Pre-training)** 和 **后训练 (Post-training)**。

**预训练阶段**，可以形象地比喻为 **把整个互联网** 的文本数据，**切分成一个个 tokens**，然后将这些 tokens **压缩成一个 zip 文件**。但是，这个 zip 文件并非我们常见的、精确无损的压缩文件，而是一个 **有损的、概率性的 zip 文件**。之所以说是 “有损” 和 “概率性”，是因为我们 **不可能** 将 **整个互联网** 的海量信息，都 **精确地压缩** 到一个，比如 1TB 大小的 zip 文件中，互联网的信息量实在太大了。我们只能将 **大概的内容** 和 **风格**，以一种 **压缩** 的方式，存储进去。

这个 “zip 文件” 中，实际上存储的是 **神经网络的参数**。例如，一个 1TB 大小的 “zip 文件”，可能对应着神经网络中大约 **一万亿个参数**。这个神经网络所做的事情，就是 **接收 tokens**，并尝试 **预测序列中的下一个 token**。但需要注意的是，它是在 **互联网文档** 的海量数据上进行 **预测** 的，所以从本质上来说，它就像是一个 **互联网文档生成器**。在 **预测序列中的下一个 token** 的过程中，神经网络 **潜移默化地获得了关于世界的大量知识**，这些知识，都被 **压缩** 存储到了这 **大约一万亿个参数** 之中。

**预训练阶段** 的 **成本非常昂贵**，可能需要 **花费数千万美元**，并持续 **几个月** 的训练时间。因此，预训练阶段 **不会经常进行**。例如，GPT-4 模型，很可能是在 **几个月甚至一年前** 就已经完成了预训练。这也就是为什么这些模型会 **有点过时**，它们有一个 **“知识截止” 日期**，这个日期对应着模型 **预训练完成的时间**。模型的知识，**只更新到那个时间点为止**。

虽然 **部分知识** 可以通过 **后训练阶段** 进入模型，我们稍后会讨论这个问题。但总的来说，您应该将这些模型 **看作是有点过时的**，因为 **预训练成本太高昂**，而且 **不会频繁进行**。所以，任何 **最新的信息**，比如你想和模型谈论 **上周发生的事情**，我们需要 **其他方式** 来为模型 **提供这些信息**，因为它 **没有存储在模型的知识库中**。在后续的教程中，我们将介绍各种 **工具**，帮助我们为模型 **补充最新的信息**。

**后训练：为模型注入“助手灵魂”**

在 **预训练** 之后，是 **后训练阶段**。如果说预训练阶段是生成了一个 “知识压缩包”，那么 **后训练阶段**，就如同 **给这个 “zip 文件” 加上一个笑脸**。这是为什么呢？因为我们 **不希望** 模型仅仅 **生成互联网文档**，我们更希望它能够 **扮演一个助手的角色**，**积极地回应用户的查询**，并提供有价值的帮助。**后训练过程**，正是为了实现这个目标而设计的。

在后训练阶段，我们会将模型训练所用的 **数据集替换** 为由 **人类构建的对话数据集**。这些对话数据集，通常包含了大量的 **用户查询** 和 **高质量的助手回复** 示例。通过在这些对话数据集上进行训练，模型逐渐 **学会了助手的对话风格**，例如如何 **礼貌地回应用户**，如何 **清晰地表达观点**，如何 **提供有用的信息** 等等。在这个过程中，模型 **获得了助手的 “灵魂”**，使得我们可以像与一个 **智能助手** 聊天一样，向它 **提问**，并期望它能够给出 **有帮助的回答**。值得强调的是，虽然模型在后训练阶段 **获得了助手的风格**，但它仍然 **拥有整个互联网的知识**，这些知识是在 **预训练阶段** 获取的。后训练阶段，只是将 **预训练获得的知识**，与 **助手的对话风格** 有机地 **结合** 在一起，从而打造出一个既 **博学** 又 **善于沟通** 的智能助手。

**理解 LLM 的本质：与 “压缩知识库” 对话**

我认为，对于本部分内容，需要理解的 **最重要一点** 是：您正在交谈的对象，是一个 **完全独立的实体**。**默认情况下**，这个语言模型，就是一个 **磁盘上的 1TB 文件**，这个文件 **代表着一万亿个参数**，以及它们在神经网络中的 **精确设置**。它的核心功能，就是 **试图为你提供序列中的下一个 token**。但请记住，这是一个 **完全独立的实体**，它 **没有计算器**，**没有计算机** 和 **Python 解释器**，**没有互联网浏览能力**，**什么都没有**。在我们目前讨论的范围内，**还没有涉及到任何工具的使用**。您现在 **正在和一个 “zip 文件” 对话**，如果您向它 **输入 tokens**，它会 **返回 tokens**。这个 “zip 文件”，**拥有预训练阶段获得的互联网知识**，以及 **后训练阶段获得的助手风格和对话形式**。这就是您应该 **大致理解这个实体** 的方式。

如果让我 **总结** 一下到目前为止我们所讨论的内容，我可能会以 **介绍 ChatGPT 的方式** 来进行，我认为您也应该这样 **看待** 它：

“嗨，我是 ChatGPT，我是一个 **1TB 的 zip 文件**。我的 **知识** 来自互联网，我 **大约在六个月前** 完整地阅读了它，但我 **只记得大概的内容**。我的 **迷人个性** 是由 OpenAI 的 **人类标注员** 通过大量的示例 **设定的**。”

**个性** 是在 **后训练** 中设定的，而 **知识** 则来自 **预训练期间对互联网信息的压缩**。需要强调的是，这些知识 **有点过时**，而且是 **概率性的**、**有点模糊的**。互联网上 **经常被提及的内容**，我会记得更清楚，而 **很少被讨论的内容**，我会记得比较模糊，这与 **人类的记忆方式** 非常相似。

现在，让我们继续讨论这个实体的一些 **重要影响**，我们应该 **如何与它交谈**，以及我们可以从它身上 **期待什么**。在接下来的教程中，我们将通过更多的 **实际例子**，深入探讨 LLM 的应用场景和注意事项。

希望这次的第二部分内容更加详尽，更符合您的期待。请您确认是否满意，并指示是否要继续生成第三部分的内容。