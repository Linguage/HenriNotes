
[Rich Sutton, Toward a better Deep Learning](https://youtu.be/YLiXgPhb8cQ?si=vmt9zIiqm8SYw8nv)
2024年9月29日

## 导言

该视频探讨深度学习领域的一个前沿课题：如何构建更适应持续学习场景的深度学习模型。由 Rich Sutton 教授主讲的“迈向更好的深度学习”报告，对现有深度学习方法的局限性进行了深刻剖析，并提出了一种富有创新性的解决方案——动态深度学习。

Sutton 教授首先强调了持续学习的重要性，并将其与传统的、具有明显训练和测试阶段划分的瞬时学习进行了对比。他指出，当前的深度学习算法在持续学习环境中面临着诸多挑战，如可塑性丧失、灾难性遗忘以及强化学习中的策略崩溃等问题。这些问题制约了深度学习在真实世界应用中的长期有效性。

为了克服这些局限，Sutton 教授提出了“动态深度学习”的概念。该方法的核心在于将神经网络划分为两个关键部分：稳定的“骨干”和动态的“边缘”。骨干网络负责存储和维护已习得的知识，而边缘网络则承担探索新特征和适应新环境的任务。通过引入“影子权重”、“效用传播”和“印记”等新机制，动态深度学习框架旨在实现网络的持续增长和优化，同时避免对已有知识的破坏。

本次报告不仅是对现有深度学习范式的一次深刻反思，更是一次对未来研究方向的大胆探索。Sutton 教授的演讲内容丰富、逻辑严谨，对于深入理解持续学习的挑战、激发新的研究思路具有重要的启发意义。建议对深度学习、强化学习以及人工智能领域感兴趣的读者仔细研读本报告的完整脚本，以获取更全面的信息和更深入的理解。

### 内容纲要

```
├── 总览
│   ├── 演讲主题：
(Toward a better Deep Learning)
│   ├── 理论基础与愿景
│   ├── 重点关注持续学习 (continual learning)
│   ├── 传统深度学习在持续学习场景下的问题
│   ├── 提出“动态深度学习” (dynamic deep learning)
│   ├── 网络逐个单元增长，持续学习
│   ├── 核心思想：骨干 (backbone) 稳定，边缘 (fringe) 动态探索
│   ├── 边缘尝试贡献，成功则加入骨干
│   ├── 需要新算法
│   └── 还有工作要做，但不困难
├── 持续学习 vs. 瞬时学习
│   ├── 持续学习：每时每刻学习，既是训练也是测试
│   ├── 瞬时学习（传统）：专门训练阶段，之后不学习
│   ├── 瞬时学习人为，自然系统持续学习
│   └── 持续学习是正常学习方式
├── 传统深度学习的不足
│   ├── 持续学习场景下不满意
│   ├── 失去可塑性：无法学习新事物
│   ├── 灾难性遗忘：忘记旧事物，优先忘记重要事物
│   ├── 策略崩溃 (强化学习)：性能急剧下降
│   └── 深度学习缓慢脆弱，需反复呈现不相关训练集
├── 需要新的思维方式
│   ├── 反向传播深度学习（通常做法）不够
│   ├── 解放思想，不同方式思考
│   ├── 2024 年，深度学习占主导，仍可重新思考
│   ├── 智能体和强化学习需求，深度学习效果不佳
│   └── 放手、放松、勇敢思考
├── 固定结构 vs. 动态增长
│   ├── 传统网络：固定设计结构 (如分层)
│   └── 动态网络：逐单元累积，灵活可增长结构 (Kum 的示例)
├── 骨干与边缘
│   ├── 区分已学习 (骨干) 和其余部分
│   ├── 骨干：已学习部分，代表知识，应保留保护
│   ├── 深度学习问题：破坏已学习内容
│   ├── ReLU 单元示例
│   ├── 边缘：探索性、动态，尝试提供有用内容
│   ├── 边缘单元可能比骨干多 (尤其开始时)
│   ├── 边缘单元通常零输出权重 (不影响行为)
│   └── 边缘目标：加入骨干
├── 梯度、饥饿
│   ├── 目标值 (target)：监督学习目标
│   ├── 误差 (error)：目标值与实际输出差异
│   ├── 损失 (loss)：误差平方
│   ├── 梯度 (gradients)：损失相对于权重偏导数
│   ├── 饥饿 (hungers)：损失相对于单元活动偏导数
│   ├── 骨干通过减少误差/饥饿学习
│   ├── 残差：某些饥饿无法减少 (缺乏资源/特征)
│   └── “饥饿”新特征：促使寻找新特征 (边缘单元)
├── 所需的新算法
│   ├── 骨干内学习：正常反向传播 + 步长优化 (保护骨干)
│   ├── 边缘内学习：不能用反向传播 (边缘权重梯度为零)
│   │   └── 使用“影子权重” (shadow weights)
│   ├── 找到骨干：
│   │    ├── 扰动权重, 观察输出
│   │   ├── 方法 1：梯度平方平均值 (依赖反向传播)
│   │   └── 方法 2：基于非零权重向后传播效用 (utility)
│   └── 维护边界
├── 效用传播
│   ├── Shaban 和 Fernando 提出
│   ├── 一定效用单元定义为骨干
│   ├── 输出单元初始效用
│   ├── 骨干单元传递效用给前驱 (非零权重)
│   ├── 效用守恒
│   └── 效用低于阈值则不属于骨干
├── 边缘学习：影子权重
│   ├── 边缘权重梯度为零，无法反向传播
│   ├── “主”单元 (骨干单元)，边缘尝试“倾听”
│   ├── 主单元是服务对象，提供效用并获得效用
│   ├── 边缘到主单元权重小 (零或接近零)，小步长 (防干扰)
│   ├── “影子权重” (shadow weight)，正或负
│   ├── 只有主单元可改变实际权重
│   └── 边缘单元任意设置影子权重
├── 印记 (Imprinting)
│   ├── 创建边缘单元时刻
│   ├── 分配主单元
│   ├── 分配影子权重 (根据主单元饥饿)
│   ├── 添加输入连接 (随机选择，从主单元前驱)
│   └── 印记权重：根据创建时输入模式 (重放，根据影子权重调整)
├── 步长优化
│   ├── 控制步长关键
│   ├── 防止灾难性遗忘，保护骨干
│   ├── 边缘创建影子权重
│   └── 边缘有用，步长增加，权重增加，加入骨干
└── 总结
    ├── 持续学习的更好深度学习
    ├── 动态: 持续学习, 不断改变的结构
    ├── 骨干稳定, 边缘具有探索性
    ├── 影子权重是关键
    ├── 需要新算法
    └── 仍有工作, 但不困难
```




## 总览

大家好，今天我将和大家探讨一个主题：迈向更好的深度学习。正如副标题所示，我将阐述其背后的理论基础，并描绘一个愿景。这不是一个完整的算法，因为改进深度学习这个庞大的领域是一个非常雄心勃勃的目标。批评它、谈论它的局限性几乎是傲慢的，但我们今天就是要这么做。

这项工作是全新的、不完整的，但同时，它建立在许多已经完成的工作之上，有些已经发表，有些还在别人的论文中。这些工作来自 Kerm Javed、Arsalan Sharaf Nassab、Shaban Fernando、Parash King Fang、Rupam (Rupam 的学生)、Muhammad Elad 以及我的同事 Joseph Modayil。

首先，我要强调的是，我们将重点关注持续学习，而不是传统的瞬时学习。我将解释为什么深度学习在持续学习场景下表现不佳。然后，我将提出一种新的深度学习方法，我称之为“动态深度学习”。

在动态深度学习中，网络是逐个单元增长的，并且能够持续学习。其核心思想是：网络有一个稳定的“骨干”部分，以及一个围绕骨干的、更具动态性和探索性的“边缘”部分。边缘不断尝试对骨干做出贡献，如果成功，它就会成为骨干的一部分。

为了实现这个愿景，我们需要一系列全新的算法。虽然还有很多工作要做，但看起来并没有那么困难。

## 持续学习 vs. 瞬时学习

首先，让我们明确什么是持续学习。持续学习意味着我们每时每刻都在学习，每一个时刻既是训练，也是测试。而传统的深度学习，我称之为“瞬时学习”，因为它只在一个特殊的训练阶段学习，之后就不再学习，在正常运行期间不会学习。

我认为这种训练和测试的分离是非常人为的。所有自然的系统都是持续学习的。从我的角度来看，持续学习才是正常的学习方式，它甚至不应该有一个特殊的名称。每一个动物、每一个人都在进行持续学习，而不是瞬时学习。

## 传统深度学习的不足

传统的深度学习，至少在持续学习的场景下，是不令人满意的。为什么这么说呢？主要有以下几个原因：

首先，随着时间的推移，网络会失去可塑性，也就是失去学习新事物的能力。

其次，会发生灾难性遗忘。当网络学习新事物时，它会忘记旧事物，而且会比实际需要的遗忘更多，甚至会优先忘记最重要的事物。

第三，在强化学习中，我们还会观察到策略崩溃。深度强化学习智能体在持续运行一段时间后，它们的性能通常会急剧下降。它们先是解决了问题，但如果继续运行，它们的表现就会崩溃，这很奇怪。

Shaban 和 Fernando 等人的研究已经证明了可塑性的丧失和策略的崩溃。总的来说，深度学习是缓慢且脆弱的。训练集中的所有示例必须反复呈现，并且要以不相关的方式呈现。如果处理不当，就会导致遗忘或可塑性丧失。

## 需要新的思维方式

基于以上原因，我们可以得出结论：通常的反向传播深度学习是不够的。我们需要解放思想，以不同的方式思考。

现在是 2024 年，深度学习正处于上升期，但我们仍然可以重新思考。因为对于智能体和强化学习的需求来说，深度学习的表现并不好。所以，我们应该放手，放松，勇敢地去尝试新的思路。

## 固定结构 vs. 动态增长

首先，我想区分一下传统的深度学习网络和我们提出的动态深度学习网络。传统的深度学习网络通常具有固定的设计结构，比如分层结构，或者在每一部分都有特定角色的分层结构。这些都是预先设计好的。

而我想要考虑的是一个更具“有机性”的系统。动态深度学习网络是逐个单元累积起来的，因此我们拥有一个灵活的、最大限度灵活的、可增长的结构。

这里有一些 Kum 使用过的网络，它们实际上是逐渐增长的。这是同一个网络，它从只有一个输出节点和多个输入节点开始，逐渐引入中间单元（特征单元），然后通过添加越来越多的这些单元，最终形成一个大型的多层网络。

这就是我认为的动态深度学习的发展方向。我不会给出网络如何增长的具体算法，但我假设它是以这种方式增长的，我们可以一起思考具体应该如何实现。

## 骨干与边缘

现在，让我们迈出重要的一步：区分网络中已经学习过的部分和其余部分。

我将把已经学习过的部分称为“骨干”。这里有一张我在白板上画的网络图。蓝色节点代表无用的单元，黑色节点代表骨干。我们知道，一个完全的深度学习网络有很多“死亡”的单元，这些单元对网络的功能没有贡献，它们是未使用的。Shaban 和 Fernando 在他们关于持续反向传播的研究中表明，很多时候，甚至超过一半的单元在学习结束时都可能是“死亡”的。

所以，让我们把这些无用的单元剪掉，剩下的就是骨干。骨干是网络中已经学习过的部分。你可以把它看作是已经获得的知识，它应该被保留和保护。当我们继续学习新事物时，不应该拆除已经学习到的东西。不幸的是，深度学习经常会这样做。

我们应该拥有一个骨干，我们应该依靠它，我们应该积累它，并不断学习更多。

为了更具体地说明，我将使用 ReLU 单元作为示例。ReLU 单元有一个加权和，然后将一个激活函数应用于加权和，以产生每个单元的值。单元的值用 x 表示，权重用 w 表示。

例如，我们将输入（可能是二进制的 1 和 0）乘以权重，得到一个和，然后应用激活函数。如果这个和大于零，那么输出就是这个和；如果小于零，那么输出就是零。

这里我展示了一个特定的活动流通过网络。我们看到一个输入 1，它根据这个想法传播：进行加权求和，然后检查它是否大于零。如果大于零，那么它就是输出。节点内显示的数字是该节点的输出。

除了骨干，网络中还有其余的部分，我称之为“边缘”。边缘是更具探索性、更具动态性的部分，它不断尝试，试图找到对骨干有用的东西。如果边缘成功地找到了有用的东西，骨干就会“倾听”它们，然后它们就会成为功能的一部分，从而加入骨干。

我设想边缘单元的数量可能比骨干单元多，至少在开始时是这样。

需要注意的是，边缘单元通常（但并非必须）具有零输出权重，因为它们不影响系统的行为。根据定义，这就是它们处于边缘的原因。从边缘到骨干的权重必须为零。

但是，这些权重是可以学习的。如果边缘单元形成了一个有用的特征，骨干可以选择增加这个权重。

边缘由这些几乎位于骨干上的单元组成，它们的目标是加入骨干。我们将以这种方式来增长骨干。

## 梯度、饥饿

再进一步，我们可以有一个目标值，因为这只是常规的监督学习。

这里我展示了一个例子，目标值（来自输出节点）是 5，大于活动值 3。所以存在一个正的误差，即目标值和实际输出之间的差异，我用 e(t) 来表示。你可能会认为损失是 e(t) 的平方。

然后，梯度就是这种形式。实际上，“梯度”这个词有点用词不当。梯度实际上是所有偏导数的向量，但习惯上把各处的梯度都称为梯度，比如这个权重的梯度就是这个偏导数。

我还想谈谈平方误差相对于单元活动的梯度。这是一个不同的东西，我称之为“饥饿”。其思想是，骨干已经尽其所能地学习了，它只利用了自己的结构。

骨干通过减少误差、减少饥饿来学习。但通常会存在一些残差，即某些饥饿无法减少。因为骨干缺乏资源、缺乏特征，无法将它们减少到零。当然，如果所有的饥饿都被减少到零，那么我们就完成了。

但通常情况下，会有一些饥饿无法减少到零，所以它们会“渴望”新的特征，使它们能够做得更好。我之所以称之为“饥饿”，是因为这里的饥饿在最后一个节点处是正的，因为目标值大于输出。而这个节点有一个负的饥饿，因为这个负的权重。为了使这个节点的值更高，这个节点希望值更低，或者说它不希望被激活。

现在，思考一下网络在运行过程中会发生什么。我在这里展示的是一个瞬时快照，其中有一个特定的输入、一个特定的输出以及所有中间值，还有一些误差。下一个时间步，将会有另一个示例、另一个目标值，会有一组不同的误差。所有这些误差或饥饿都是我们想要减少的。我们正在尝试找到能够提供特征的边缘单元，使这些误差或饥饿能够被减少。

我添加了这些节点的名称（A 到 G），这样我们可以谈论这些节点。如果你想问我关于某个节点的问题，你可以直接引用节点名称。

## 所需的新算法

根据这个设计，我们需要多种新的算法。我们需要在骨干内进行学习，需要在边缘内进行学习，我们需要找到骨干，我们需要维护边界。让我们简要地讨论一下这些问题，并在头脑中思考一下。

首先，骨干内的学习。这可以是正常的反向传播，并且可以与步长优化相结合。记住，我们要保护骨干，骨干是我们的知识，所以我们不想做大的改变，我们想缓慢地移动。步长优化可以做到这一点，它将学习到骨干内的步长应该很小。

其次，边缘内的学习。这是一个真正的挑战，因为我们不能在边缘内使用反向传播。为什么？在反向传播中，我们根据梯度来移动权重，即误差相对于权重的梯度。但是，根据定义，边缘权重的梯度都是零，因为根据定义，边缘不影响输出，所以它们不影响误差。所有的梯度都是零。所以我们必须做一些其他的事情。

我们的标准算法竟然不能找到新的特征，这不是很奇怪吗？至少不能从无到有地找到新特征。

那么，我们将如何做到这一点呢？我将告诉你我们将如何做到这一点，这将是一种我称之为“影子权重”的东西。

然后我们需要找到骨干。我们如何找到网络的哪一部分是重要的，是影响网络输出的，并将其与不影响输出的部分区分开来呢？

一个想法是稍微扰动权重，看看它们是否影响输出。我想这会奏效。还有如果我们在平衡状态, 那么所有权重的梯度都是0, 但是如果某一个时刻, 权重产生了影响, 那么这个时刻权重就会产生非零的梯度。

另一种方法，可以持续学习观察，当某些节点的梯度在一段时间内持续为0，则可以认为这些节点是无用的。但是持续有梯度不代表有用。

我还有第三种方法. 大家都知道输出层一定在骨干上, 那么所有指向输出层的上一层, 如果权重不为零, 那么上一层的节点也在骨干层上. 依此类推, 就能找到所有骨干.

## 效用传播

我将谈论效用。效用是你们之前听说过的东西，Shaban 和 Fernando 谈论过它，他们在持续反向传播中使用了它。

我们将定义具有一定效用的单元为骨干。当然，输出单元将在骨干上。我们将在每个输出单元上放置一个单位的效用。这就是骨干的起点。然后，每个具有效用的骨干单元都会将其一部分效用传递给具有非零权重的前驱单元，这些是它使用的权重。

这里重要的是效用是守恒的。如果你将效用传递给你的一个前驱，你就会失去 সেই效用。效用是有限的。如果你的效用低于一个阈值，那么你就不再属于骨干。这是一个效用传播算法的概要。

## 边缘学习：影子权重

现在让我们来谈谈最大的问号：边缘内的学习是如何工作的。

首先，我用语言来描述，然后我将用图片来展示。正如我们所讨论的，根据定义，边缘权重的梯度始终为零，所以我们不能使用反向传播。相反，每个边缘单元都将有一个“主”单元，一个位于骨干上的主单元，它试图让这个主单元“倾听”它。

主单元是边缘单元希望服务的对象，希望为其提供效用，并因此从主单元获得效用。

请注意，根据设计，边缘单元到其主单元的权重必须很小，必须是零或接近零。因为如果它有一个很大的权重到主单元，那么它就会干扰主单元，它会干扰骨干，它会干扰已经学习到的东西。我猜它甚至会因此而成为骨干的一部分。

所以，根据定义和设计，每个边缘单元到骨干的权重都将是零或很小。而且这个权重的步长也很小，因为即使你有一个零权重，如果你有一个大的步长，你也会很快获得一个大的权重，你就会干扰骨干，干扰已经学习到的东西。

边缘单元到其主单元的权重是零，但边缘单元仍然渴望增加其主单元的权重，从而加入骨干。为了做到这一点，它有一个我称之为“影子权重”的东西，一个到主单元的正或负的影子权重。

这里的设置是，只有主单元可以改变实际权重。因为主单元可能会感觉到一些误差或饥饿，但边缘单元可以任意设置其影子权重。

## 印记 (Imprinting)

现在，让我们来看这张图。这是我们之前看过的场景，但现在我想让你们思考一下 D 和 E 这两个单元。这两个单元是在此刻被添加的。我们称之为“印记”，创建边缘单元的时刻被称为“印记”过程。

当我们进行印记时，我们会分配一个主单元。这里，单元 E 的主单元是 C，单元 D 的主单元是 B。

然后，我们会分配一个影子权重。这是影子权重，这个权重就像是实际权重的“影子”。如果你还记得，单元 C 的饥饿是正的，所以影子权重被设置为 +1。而 B 的饥饿是负的，所以我们得到一个 -1 的影子权重。

还有最后一步。当我们添加 D 和 E 时，我们会添加它们的输入连接。我选择的输入连接可能是随机的，从我们添加的单元（即主单元）的前驱单元中选择，以保持前馈结构。这里，单元 D 连接到 A 和一个输入，单元 E 连接到 B 和一个输入。

然后，边缘单元的权重被“印记”，这意味着它们是从创建时的输入模式中获取的。理解这一点的最简单方法是，你设置你的影子权重，然后重放这个例子。当我们重放这个例子时，有一个正的饥饿。所以，如果影子权重是真实的，误差会传播，梯度会反向传播到 E，E 会调整它的权重。这就是印记过程中发生的事情。

这里，这是一个正的 1，我们有一个梯度传递回来，这是一个正的，我们想要增加这个节点的值。所以，当我们有一个 1 输入时，我们设置一个正的权重；当我们有一个 0 输入时，我们设置一个负的权重。在这个例子中，饥饿是负的，影子权重是负的。所以我们想要降低这个节点的值。

不，不，我们不想降低这个节点的值。即使影子权重是负的，我们仍然希望这个单元被激活，这样它才能对 B 产生负面影响。所以，我们仍然增加到正输入的权重，并对零输入设置负权重。

这就是我们如何“印记”一个边缘单元，并将其与骨干进行连接。影子权重是它们希望如何连接的“愿望”。

## 步长优化

最后，我想提醒你们，步长优化是这个过程的一个组成部分。控制步长可以防止灾难性遗忘，保护骨干免受更具动态性的边缘的干扰。

边缘单元会创建一个影子权重，它们与主单元有这种连接。如果边缘提供的东西对骨干有用，那么会发生什么呢？首先，到主单元的权重的步长会增加，然后，一旦步长变得不那么微不足道，权重本身也会增加，它就会加入骨干。

## 总结

这就是我对更适合持续学习的、更好的深度学习的愿景，而不是瞬时学习。这是一种动态的深度学习，因为我们一直在进行学习，而且我们还在不断改变结构。其基本思想是：骨干是稳定的，边缘是动态的、探索性的。当边缘成功时，它就会成为骨干的一部分。影子权重是实现这一目标的关键。

我们需要其他的、新的算法，我们今天已经对它们进行了设想。显然，还有很多工作要做，有些部分被省略了，有些部分没有完全指定。但至少对我来说，这些似乎都不是特别困难。我认为我们实际上可以做出一种更适合智能体和强化学习的、更好的深度学习。

谢谢大家！
