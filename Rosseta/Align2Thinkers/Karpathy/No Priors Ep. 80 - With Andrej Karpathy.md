No Priors Ep. 80 | 与OpenAI和Tesla的Andrej Karpathy的对话
- 原文标题：No Priors Ep. 80 | With Andrej Karpathy from OpenAI and Tesla - YouTube
- 链接：[https://www.youtube.com/watch?v=hM_h0UA7upI&t=162s](https://www.youtube.com/watch?v=hM_h0UA7upI&t=162s)

- **文章类别**：访谈实录

---

**内容整理**：

### 访谈主题
本次访谈主要围绕Andrej Karpathy在自动驾驶、人工智能教育以及大型语言模型（LLM）研究等领域的见解和经验展开。

### 访谈人物
- **Andrej Karpathy**：知名研究员、AI教育者、魔方高手，曾在OpenAI担任早期团队成员，领导Tesla的Autopilot项目，目前专注于AI教育。

### 主要话题

#### 自动驾驶技术
- **Tesla与Waymo的竞争**：
  - Karpathy认为Tesla在自动驾驶领域领先于Waymo，尽管外界可能不这么看。他指出Tesla面临的是软件问题，而Waymo面临的是硬件问题。Tesla已经实现了大规模的车辆部署，这使得其在数据收集和模型训练方面具有优势。
  - 他提到Tesla的自动驾驶系统已经从依赖大量C++代码转变为更多地依赖神经网络，这一转变使得系统更加高效和可扩展。

- **技术与监管的挑战**：
  - Karpathy强调，从演示阶段到产品化，再到全球推广，自动驾驶技术面临着巨大的挑战。这不仅包括技术难题，还包括监管等非技术因素。他以Weo为例，说明了从十年前的演示到如今的产品化，经历了漫长的过程。

#### 人工智能教育
- **AI在教育中的应用**：
  - Karpathy表达了他对AI教育的热情，他认为AI可以为人们提供个性化的学习体验，帮助每个人达到他们的潜力极限。他提到了通过AI实现的自适应学习，即根据学习者的水平和背景提供定制化的学习内容。

- **教育的未来**：
  - 他展望了一个全球化的教育未来，其中AI可以帮助人们克服语言障碍，实现知识的全球传播。他还提到了教育的娱乐化趋势，认为在AGI（人工通用智能）时代，教育将更多地被视为一种娱乐形式。

#### 大型语言模型（LLM）研究
- **Transformer架构**：
  - Karpathy对Transformer架构给予了高度评价，认为它是一个“神奇的算法”，能够处理大规模的数据并进行有效的训练。他指出，Transformer的出现使得神经网络的训练变得更加高效和可扩展。

- **数据与模型的挑战**：
  - 他提到，随着Transformer的发展，数据集和损失函数成为了新的瓶颈。为了训练更强大的模型，需要更多的数据和更精细的损失函数设计。他还提到了合成数据的重要性，认为合成数据是未来发展的关键，但同时也需要注意数据的多样性和丰富性。

- **模型的效率与性能**：
  - Karpathy认为，当前的模型在参数数量上可能过于庞大，存在很多冗余。他预测，未来可能会出现更小、更高效的模型，这些模型将能够更好地利用有限的资源，同时保持强大的认知能力。

### 结论与展望
- **AI的潜力与挑战**：
  - Karpathy对AI的未来发展持乐观态度，但他也承认存在许多挑战，包括技术、监管和数据等方面的问题。他认为，通过持续的研究和创新，这些问题将逐渐得到解决。

- **教育的变革**：
  - 他强调了AI在教育领域的变革潜力，认为AI可以为每个人提供个性化的学习路径，帮助人们实现更高的学习成就。他还提到了教育的全球化和娱乐化趋势，认为这将使学习变得更加有趣和吸引人。

- **模型的未来发展**：
  - Karpathy预测，未来的模型将更加高效和小型化，同时保持强大的认知能力。他还提到了模型的多样化和专业化趋势，认为未来可能会出现针对不同领域的专门模型，这些模型将共同构成一个复杂的生态系统。

#人工智能 ， #自动驾驶 ， #教育技术 ， #大型语言模型

### **视频框架**

* **0:00 开场介绍**
* **0:33 自动驾驶汽车的演变**
    * 0:33 自动驾驶领域现状和类比 AGI
    * 1:38 自动驾驶技术的成熟度：demo 与产品的差距
    * 2:01 自动驾驶技术的全球化挑战
* **2:23 特斯拉与 Waymo 自动驾驶方案对比**
    * 2:26 特斯拉的软件优势与 Waymo 的硬件优势
    * 3:26 软件问题的解决预期与硬件成本的比较
    * 3:49 特斯拉在训练阶段的传感器使用策略
    * 4:25 端到端深度学习在自动驾驶中的应用
    * 5:53 监督数据稀缺性问题和预训练的重要性
* **6:32 使用汽车模型训练 Optimus 人形机器人**
    * 6:40 汽车与人形机器人的技术迁移
    * 7:48 Optimus 快速启动的背后原因
    * 8:39 人形机器人应用场景：B2C vs B2B
    * 9:54  Lex Fridman 的“捡树叶”挑战
* **10:26 人形形态的理由**
    * 10:53  统一平台的优势和人类操作的便利性
    * 11:34  单一神经网络多任务处理的重要性
    * 12:04  人形机器人成本与替代方案的思考
* **13:22 机器人技术现存的挑战**
    * 13:36  人形机器人下半身的控制策略
    * 13:58  机器人领域关注的重点：操作和远程操作
    * 14:21  现有技术与迭代改进
* **14:45 AI 研究现状：Transformer 模型的优势与瓶颈**
    * 14:50 Transformer 模型的通用性和效率
    * 15:35 Transformer 架构的关键创新
    * 16:16  AI 发展的瓶颈：数据和损失函数
    * 17:23  互联网数据的问题与合成数据的应用
    * 18:47  合成数据使用中的挑战：模型坍塌和熵的维持
* **20:27 人类认知与 AI 模型的类比**
    * 20:36  AI 模型在特定认知方面的潜在优势
    * 21:36  计算机与人脑的对比：记忆力、学习效率等
* **22:12 人类认知与 AI 能力的融合**
    * 22:18  AI 作为工具对人类的增强
    * 22:35  人机融合的伦理思考：Neuralink 等技术
    * 23:15  AI 作为“外脑皮层”的未来
    * 23:44  地图导航和通用翻译器等例子
    * 24:38  AI 工具对人类认知能力的影响
* **25:28  LLM 研究现状与市场结构对未来可及性的影响**
    * 25:40  开源与闭源平台的生态系统
    * 25:57  “非你的权重，非你的大脑”：数据所有权的讨论
    * 26:26  开源模型在人机融合中的重要性
* **27:10 构建高性能小模型**
    * 27:22  当前模型存在冗余信息
    * 27:39  小型认知核心的可能性
    * 27:57  蒸馏技术在模型小型化中的作用
    * 28:16  认知能力的数学表达探讨
    * 29:17  边缘设备上的“外脑皮层”
    * 29:29  LLM 组成的“公司”：并行化与专业化
* **30:33 Andrej 目前在 AI 教育方面的工作**
    * 30:36  个人背景与教育的热情
    * 30:48  AI 赋能人类的愿景
    * 31:25  AI 辅助个性化教育的潜力
    * 31:56  AI 驱动教育产品的构建思路：教师与 AI 的角色
    * 33:09  AI 助教的比喻
    * 33:29  AI 能力的现实评估与产品开发
    * 33:53  教育对人类潜能的激发
    * 34:45  个性化学习中的适应性挑战
    * 35:14  多语言支持与跨学科类比
* **36:17 AI 驱动的教育如何重塑知识网络和地位**
    * 36:20  学术界的“血统”现象
    * 36:46  AI 打破教育门槛的潜力
    * 37:04  知识传播的集群效应
    * 37:27  教育与学徒制
    * 37:38  文化对动机和社会地位的影响
    * 38:15  多伦多与硅谷的文化差异
    * 39:08  学习与娱乐的边界
    * 39:27  通过教育系统改变动机
    * 39:56  后 AGI 时代教育的意义
    * 40:41  学习如同“大脑的健身”
* **41:26 Eureka Labs 的首门课程**
    * 41:29  目标受众：本科水平的技术人员
    * 41:41  教育模式的变革：终身学习
    * 42:10  课程上线时间：预计明年年初
* **42:25 年轻人应该学习什么来为未来做好准备**
    * 42:30  推荐学科：数学、物理、计算机科学
    * 42:36  思维技能的培养
    * 43:14  “有用”或“有益”的教育
    * 43:33  数学学习的体验分享
    * 43:44  学科多样性的价值

### **要点内容**

* 自动驾驶技术已经取得很大进步，但从 demo 到大规模应用还有很长的路要走，Tesla 和 Waymo 各有优劣，未来竞争格局仍不明朗。
* 人形机器人是汽车技术的延伸，技术迁移可行，未来应用场景广阔，但 B2B 场景可能先于 B2C 场景落地。
* Transformer 模型具有通用性和高效性，是 AI 领域的重要突破，但当前 AI 发展的瓶颈在于数据和损失函数，合成数据是未来的方向，但需要注意模型坍塌和熵的维持。
* 人类认知与 AI 模型可以类比，AI 在特定认知方面（如记忆力）可能优于人类，人机融合是未来的趋势，但需要考虑伦理和数据所有权问题。
* 构建高性能小模型是可能的，当前模型存在冗余信息，蒸馏技术可以有效减小模型规模，未来可能会出现由 LLM 组成的“公司”。
* AI 可以赋能教育，实现个性化和规模化教学，打破教育门槛，激发人类潜能，但需要注意学习与娱乐的边界，以及文化对动机的影响。
* 年轻人应该学习数学、物理、计算机科学等基础学科，培养思维技能，为未来做好准备。
* Eureka Labs 正在开发一门 AI 课程，预计明年年初上线，目标受众是本科水平的技术人员，旨在推动终身学习。

总而言之，Andrej Karpathy 对 AI 的未来充满信心，认为 AI 可以赋能人类，推动社会进步，并强调了教育在未来社会中的重要性。


---

### 视频脑图

```
对话纲要
    ├── 开场介绍
    │   └── Sarah 介绍嘉宾 Andrej Karpathy 及其背景
    ├── 自动驾驶汽车的演变
    │   ├── 自动驾驶领域现状和类比 AGI
    │   ├── 自动驾驶技术的成熟度：demo 与产品的差距
    │   └── 自动驾驶技术的全球化挑战
    ├── 特斯拉与 Waymo 自动驾驶方案对比
    │   ├── 特斯拉的软件优势与 Waymo 的硬件优势
    │   ├── 软件问题的解决预期与硬件成本的比较
    │   ├── 特斯拉在训练阶段的传感器使用策略
    │   ├── 端到端深度学习在自动驾驶中的应用
    │   └── 监督数据稀缺性问题和预训练的重要性
    ├── 使用汽车模型训练 Optimus 人形机器人
    │   ├── 汽车与人形机器人的技术迁移
    │   ├── Optimus 快速启动的背后原因
    │   ├── 人形机器人应用场景：B2C vs B2B
    │   └── Lex Fridman 的“捡树叶”挑战
    ├── 人形形态的理由
    │   ├── 统一平台的优势和人类操作的便利性
    │   ├── 单一神经网络多任务处理的重要性
    │   └── 人形机器人成本与替代方案的思考
    ├── 机器人技术现存的挑战
    │   ├── 人形机器人下半身的控制策略
    │   ├── 机器人领域关注的重点：操作和远程操作
    │   └── 现有技术与迭代改进
    ├── AI 研究现状：Transformer 模型的优势与瓶颈
    │   ├── Transformer 模型的通用性和效率
    │   ├── Transformer 架构的关键创新
    │   ├── AI 发展的瓶颈：数据和损失函数
    │   ├── 互联网数据的问题与合成数据的应用
    │   └── 合成数据使用中的挑战：模型坍塌和熵的维持
    ├── 人类认知与 AI 模型的类比
    │   ├── AI 模型在特定认知方面的潜在优势
    │   └── 计算机与人脑的对比：记忆力、学习效率等
    ├── 人类认知与 AI 能力的融合
    │   ├── AI 作为工具对人类的增强
    │   ├── 人机融合的伦理思考：Neuralink 等技术
    │   ├── AI 作为“外脑皮层”的未来
    │   ├── 地图导航和通用翻译器等例子
    │   └── AI 工具对人类认知能力的影响
    ├── LLM 研究现状与市场结构对未来可及性的影响
    │   ├── 开源与闭源平台的生态系统
    │   ├── “非你的权重，非你的大脑”：数据所有权的讨论
    │   └── 开源模型在人机融合中的重要性
    ├── 构建高性能小模型
    │   ├── 当前模型存在冗余信息
    │   ├── 小型认知核心的可能性
    │   ├── 蒸馏技术在模型小型化中的作用
    │   ├── 认知能力的数学表达探讨
    │   ├── 边缘设备上的“外脑皮层”
    │   └── LLM 组成的“公司”：并行化与专业化
    ├── Andrej 目前在 AI 教育方面的工作
    │   ├── 个人背景与教育的热情
    │   ├── AI 赋能人类的愿景
    │   ├── AI 辅助个性化教育的潜力
    │   ├── AI 驱动教育产品的构建思路：教师与 AI 的角色
    │   ├── AI 助教的比喻
    │   ├── AI 能力的现实评估与产品开发
    │   ├── 教育对人类潜能的激发
    │   ├── 个性化学习中的适应性挑战
    │   └── 多语言支持与跨学科类比
    ├── AI 驱动的教育如何重塑知识网络和地位
    │   ├── 学术界的“血统”现象
    │   ├── AI 打破教育门槛的潜力
    │   ├── 知识传播的集群效应
    │   ├── 教育与学徒制
    │   ├── 文化对动机和社会地位的影响
    │   ├── 多伦多与硅谷的文化差异
    │   ├── 学习与娱乐的边界
    │   ├── 通过教育系统改变动机
    │   └── 后 AGI 时代教育的意义
    ├── Eureka Labs 的首门课程
    │   ├── 目标受众：本科水平的技术人员
    │   └── 教育模式的变革：终身学习
    └── 年轻人应该学习什么来为未来做好准备
        ├── 推荐学科：数学、物理、计算机科学
        ├── 思维技能的培养
        ├── “有用”或“有益”的教育
        ├── 数学学习的体验分享
        └── 学科多样性的价值
```



---

## 视频教本（中文翻译）

好的，根据脚本上下文，可以判断 **Sarah** 和 **Elad** 是采访者，**Andrej Karpathy** 是受访者。以下是重新组织后的访谈录，使用了框架中的标题，并尽量保持原文的完整性和可读性：

**开场介绍**

**Sarah:** 欢迎回到 No Priors！今天我们的嘉宾是 Andrej Karpathy，相信大家对他已经非常熟悉了。Andrej 是一位著名的 AI 研究员，深受大家喜爱的 AI 教育家，也是魔方爱好者。他是 OpenAI 的早期成员，曾领导特斯拉的 Autopilot 项目，现在致力于 AI 教育。今天我们将和他聊聊 AI 研究的现状、他的新公司以及我们对 AI 未来的展望。Andrej，欢迎来到我们的节目！

**Andrej:** 谢谢，很高兴来到这里！

**自动驾驶汽车的演变**

**Elad:** 你曾经领导特斯拉的 Autopilot 团队，现在我们实际上已经有完全自动驾驶的乘用车上路了。你怎么看待自动驾驶领域目前的现状？我们应该以怎样的速度期待自动驾驶能力的提升，或者说乘用车的普及？

**Andrej:** 是的，我在自动驾驶领域工作了大概五年，我认为这是一个非常迷人的领域。我想先聊聊目前行业里发生的一些情况，我经常会把自动驾驶和 AGI 进行类比，也许是因为我对自动驾驶更熟悉。我觉得在某种程度上，我们在自动驾驶领域已经实现了某种“AGI”，因为现在已经有一些系统，付费用户已经可以使用了。例如，Waymo 在旧金山非常常见，你们可能也体验过，我也坐过很多次，感觉非常棒，它可以带你去任何地方，而且这是一项你正在付费使用的产品。有趣的是，我第一次体验 Waymo 大概是在十年前，大约是 2014 年，当时是一位在那工作的朋友带我体验的。十年前，它就载着我绕着街区转了一圈，当时的体验就近乎完美了。从十年前的那次 demo 到现在成为一个我可以付费使用的产品，并且它还在不断扩展，这中间花了十年时间。

**Sarah:** 你认为这十年中，有多少是由于监管原因，有多少是由于技术原因？你认为技术什么时候真正准备好了？

**Andrej:** 我认为在 30 分钟的单次 demo 体验中，你是无法体会到所有技术细节的，你也不会遇到他们在这十年中必须处理的所有问题。从 demo 到产品，这中间存在着巨大的鸿沟，其中也包括监管等因素。但我确实认为，我们在自动驾驶领域已经在某种程度上实现了 AI。然而，真正让我着迷的是，这项技术的全球化还没有开始。虽然你可以在一个地方体验 demo，但世界还没有因此而改变，这还需要很长时间。从 demo 到真正的全球化应用，我认为这中间还有很大的差距。这也和我对 AGI 的看法类似，我猜想当我们实现 AGI 时，情况也会类似。

**特斯拉与 Waymo 自动驾驶方案对比**

**Elad:**  在自动驾驶领域再多聊一会儿，很多人认为 Waymo 领先于特斯拉，但我个人认为特斯拉领先于 Waymo。我知道现在看起来可能不是这样，但我仍然非常看好特斯拉及其自动驾驶项目。我的看法是，特斯拉面临的是软件问题，而 Waymo 面临的是硬件问题。我认为软件问题更容易解决。特斯拉已经在全球范围内规模化部署了大量的汽车，而 Waymo 还需要达到这个规模。所以，一旦特斯拉能够真正部署并让自动驾驶系统正常工作，我认为那将是非常了不起的。我昨天刚体验了最新的版本，它已经能带我在很多地方行驶了，他们最近确实取得了很大的进步。

**Sarah:** 我最近也经常使用它，它的表现确实相当不错，昨天还帮我完成了一些不可思议的操作，团队的工作给我留下了深刻的印象。

**Andrej:**  所以我仍然认为特斯拉主要面临的是软件问题，而 Waymo 主要面临的是硬件问题。所以，虽然目前 Waymo 看起来领先，但当我们展望未来十年，看看谁真正实现了规模化，谁从中获得了最大的收益，我仍然认为特斯拉在这些方面处于领先地位。

**Elad:**  你认为我们还需要多久才能解决这个软件问题，达到一定的水平？因为就像你说的，Waymo 的车上有很多昂贵的激光雷达和其他传感器来支持软件系统的运行。如果能够像特斯拉那样只使用摄像头，就能有效地降低巨大的成本和复杂性，并且可以在各种类型的汽车上实现。你认为这种转变什么时候会发生？

**Andrej:**  我希望在未来几年内，大概是这样的时间。但实际上，有趣的是，我认为人们可能没有意识到特斯拉实际上也使用了大量昂贵的传感器，他们只是在训练阶段使用。有一些车会带着激光雷达四处行驶，他们做了很多无法规模化的事情，他们有额外的传感器等等，他们也做测绘，所有这些你都是在训练阶段做的。然后在测试时，将其提炼成一个部署到汽车上的仅使用视觉的方案，这就像是一种对传感器的套利和成本控制。

**Sarah:**  是的。

**Andrej:**  我认为这实际上是一个非常聪明的策略，但还没有被完全认识到。我认为这个策略会奏效，因为像素中包含了信息，并且我相信神经网络能够处理这些信息。是的，在训练阶段，我认为这些传感器非常有用，但在测试阶段，我认为它们的作用没有那么大。

**Elad:**  似乎还有另一个转变是，从大量针对边缘情况设计的启发式方法转向了端到端的深度学习。这也是最近发生的另一个转变，你能谈谈这方面的情况吗？

**Andrej:**  我认为这从一开始就是特斯拉的计划，我之前也谈到过神经网络如何逐步“吞噬”整个软件栈。因为当我加入特斯拉时，软件栈中有大量的 C++ 代码，而现在在实际运行在车上的测试包中，C++ 代码已经少了很多。当然，后端还有很多我们没有谈到的东西。神经网络逐渐接管了系统，首先是在图像级别进行检测，然后是多张图像进行预测，再然后是多个时间点的多张图像进行预测，你逐渐抛弃了 C++ 代码，最终你只需要发出转向指令。所以我认为特斯拉正在逐步“吞噬”整个软件栈。我的理解是，目前的 Waymo 实际上并不是这样，他们尝试过，但最终没有采用这种方式，这是我目前的理解，但我不确定，因为他们没有公开谈论过。但我从根本上相信这种方法，我认为对于特斯拉来说，假设十年后，端到端系统将只是一个神经网络。视频流输入到神经网络，然后输出控制指令，你必须逐步构建，一点一点地实现。即使是我们之前做的所有中间预测和所有事情，我认为它们并没有误导开发，我认为它们是其中的一部分，因为有很多充分的理由支持这样做。在自动驾驶中，你只是在模仿人类的行为等等，你只有非常少的监督信息来训练一个庞大的神经网络，对于训练这么多参数来说，信号太少了。所以这些中间表征等等可以帮助你开发特征和检测器，然后对于端到端部分来说，这就变成了一个更容易的问题。所以我猜想，虽然我不是团队成员所以并不清楚，但是应该有大量的预训练正在进行，这样你就可以对端到端进行微调了。总而言之，我认为逐步“吞噬”是必要的，特斯拉的做法是正确的，而且看起来正在奏效，所以我非常期待。如果你从头开始，你一开始也没有数据，所以这是有道理的。

**使用汽车模型训练 Optimus 人形机器人**

**Sarah:**  你在离开之前参与了特斯拉的人形机器人 Optimus 项目，我有很多问题想问你！首先，有哪些技术可以迁移？

**Andrej:**  基本上所有技术都可以迁移，而且我认为人们还没有充分认识到这一点。这是一个很大的论断。当你仔细观察时，你会发现汽车基本上就是机器人。汽车就是机器人，而且我认为特斯拉不是一家汽车公司，这是一种误解，特斯拉是一家机器人公司，是一家规模化的机器人公司。因为他们不仅仅是在制造一个产品，他们还在制造“制造机器的机器”，这是完全不同的事情。所以我认为特斯拉是一家规模化的机器人公司。从汽车到人形机器人的技术迁移，实际上并没有花费太多精力。事实上，在 Optimus 机器人的早期版本中，它还以为自己是一辆汽车，因为它使用了完全相同的计算机和摄像头。这非常有趣，因为我们在机器人上运行汽车的网络，但它却在办公室里走来走去，它试图识别可行驶的区域，但现在这些区域都是可以行走的区域。但它实际上在某种程度上也通用了，当然也需要一些微调等等。但它以为自己在开车，但实际上它是在环境中移动，可以把它看作是一个机器人。很多东西都可以迁移，但你可能会缺少一些组件，比如执行和动作数据。

**Sarah:**  是的，你肯定会缺少一些组件。

**Andrej:**  我想说的另一点是，很多东西都可以迁移，Optimus 项目启动的速度之快让我印象深刻。因为当 Elon 说我们要开始做这个项目时，人们就带着所有正确的工具出现了，所有的东西都很快就到位了，所有的 CAD 模型，所有的供应链，我当时就觉得，哇，特斯拉内部有这么多构建机器人的专业知识，而且都是相同的工具。他们只是像变形金刚电影里那样，从汽车形态重新配置和调整，但本质上是相同的。你需要考虑所有相同类型的组件，无论是硬件方面、规模化方面，还是智能方面。在智能方面，也有大量的技术迁移，不仅是特定的网络，还包括所有的方法、标注团队、如何协调以及人们采用的方法，我认为有大量的技术可以迁移。

**Sarah:**  你认为人形机器人的首批应用领域是什么？很多人想象它可以帮忙洗衣服等等，我认为这些应用场景会来得比较晚。我认为 B2C 不应该是正确的起点，因为我不认为我们可以让机器人“误伤”奶奶，这是我的看法。因为存在太多的法律责任问题，我不认为这是正确的。

**Andrej:**  我的意思是，它可能会摔倒或什么的，你知道，这些东西还不够完美，还需要一些改进。所以我认为最好的客户首先是你自己，我认为特斯拉很可能会这样做，就像大家看到的那样，我非常看好特斯拉。第一个客户就是你自己，你在工厂里进行孵化等等，可以做很多物料搬运之类的工作。这样你就不必创建与第三方合作的合同，因为这涉及到很多繁重的工作，还需要律师等等。首先在内部进行孵化，然后我认为第二步是 B2B，你可以去其他拥有大型仓库的公司，我们可以做物料搬运，我们可以做所有这些工作，草拟合同，设置围栏等等。然后在公司内部孵化之后，我认为就可以开始进入 B2C 应用了。我认为我们也会看到 B2C 机器人，像宇树科技（Unitree）等公司已经开始推出一些我很想要的机器人，我已经买了一个。

**Sarah:**  好的，是 G1 吗？

**Andrej:**  是的，是 G1。所以我可能也会买一个，而且可能会有一个基于这些平台进行开发的生态系统。但我认为，就规模化应用而言，我会预期采用类似的路径。一开始是大量的物料搬运，然后逐渐转向更多特定的、更人性化的应用场景。我非常期待的一项应用是 Lex Fridman 提出的“捡树叶”挑战，我希望 Optimus 能够走在街上，踮起脚尖，一片一片地捡起树叶，这样我们就不需要吹叶机了。我认为这是可行的，而且这是一项了不起的任务，所以我希望这是首批应用之一。

**Sarah:**  耙树叶应该也可以，只是非常安静地耙树叶。

**Andrej:**  安静地耙树叶，这很可爱。他们确实有类似功能的机器，只是不是人形的。

**人形形态的理由**

**Elad:**  我们能谈谈人形这个概念吗？最简单的解释是，这个世界是为人类设计的，你只需要构建一套硬件。

**Sarah:**  正确的做法是构建一个模型，让它能够在这套硬件上执行越来越多的任务。

**Elad:**  我认为还有另一种观点认为，人类对于任何特定任务来说都不是最优的，你可以让人类变得更强壮、更大、更小等等，为什么我们不去做一些超人的事情呢？你怎么看待这个问题？

**Andrej:**  我认为人们可能低估了任何固定成本投入到单一平台中的复杂性，我认为任何单一平台都需要付出巨大的成本。所以我认为集中化并拥有一个能够完成所有任务的单一平台是非常有意义的。我想说，人形的形态也非常有吸引力，因为人们可以非常容易地对其进行远程操作，这是一个非常有用的数据收集方式，因为人们显然可以非常容易地对其进行远程操作，我认为这一点经常被忽视。当然还有你提到的方面，就是世界是为人类设计的等等，所以这一点也很重要。我认为人形平台会有一些变体，但我认为任何平台都有很大的固定成本。我想说的最后一点是，你可以从不同任务之间的迁移学习中获益良多。在 AI 领域，你真正想要的是一个单一的神经网络来执行多任务，完成各种各样的事情，这是你获得所有智能和能力的地方，这也是为什么语言模型如此有趣。因为你有一个单一的文本领域，可以处理各种不同的问题，它们都在一个神经网络中共享知识。我认为你需要这样的平台，你需要为捡树叶收集的所有数据都能对所有其他任务有所帮助。如果你为每个任务都构建一个专用的系统，你就无法从许多任务之间的迁移学习中获益。

**Sarah:**  是的，我认为有一个观点是，G1 的售价大约是 3 万美元，但在一定的物料清单（BOM）成本下，似乎很难构建一个非常有能力的人形机器人。如果你想把一个机械臂装在轮子上，我可以做很多事情，也许有一些更便宜的方法可以在开始时构建一个通用平台，你觉得呢？从硬件的角度来看，构建通用平台的更便宜的方法？

**Elad:**  是的，我认为这是有道理的。你可以在它上面装一个轮子，而不是脚等等。我确实觉得，我不知道这是否会让你陷入局部最优。我只是觉得，选择一个平台并将其做到完美，从长远来看是一个相当不错的选择。当然，还有另一点是，我认为它对人们来说会很熟悉，人们会理解它，也许你想和它交谈。我觉得它的心理方面也可能更倾向于人形平台。除非人们害怕它，实际上更喜欢一个更抽象的平台。但我不知道，如果只是一个“怪物”在做事情，我不知道这是否更友好。

**Sarah:**  有趣的是，宇树的另一种形态是机器狗，这是一种更友好、更熟悉的形象。但人们看了《黑镜》之后，突然觉得机器狗变成了一种可怕的东西，所以很难想清楚。我只是觉得从心理上来说，人们更容易理解正在发生的事情。

**机器人技术现存的挑战**

**Sarah:**  你认为在实现这一愿景的过程中，在技术里程碑方面还缺少什么？

**Elad:**  关于机器人技术？

**Sarah:**  是的，人形机器人或其他任何人类形态的机器人。

**Andrej:**  我不知道我是否对这方面有非常深入的了解。但我确实认为，对于人形的下半身来说，你可能不想通过演示来进行模仿学习，这很有趣。因为对于下半身来说，更多的是倒立摆控制之类的事情，而对于上半身来说，你需要大量的远程操作、数据收集和端到端训练等等。所以，一切都变得非常混合，我不知道这些系统如何交互。

**Elad:**  当我和一些业内人士交流时，他们关注的很多是驱动，以及操作，类似数字操控之类的？

**Andrej:**  是的，我确实认为在一开始，需要大量的远程操作来启动项目，模仿行为，让它在 95% 的时间里都能正常工作，然后考虑人机比例，逐渐让人们成为机器人的监督者，而不是直接执行任务。所有这些事情都会随着时间的推移逐渐发生。我不认为有什么具体的障碍是我非常熟悉的，我只是认为有很多繁重的工作。现有的工具，比如 Transformer，就像一块漂亮的“组织”，你可以给它任意的任务，你只需要数据，你需要以正确的形式输入数据，你需要训练它，你需要用它进行实验，你需要部署它，并不断迭代。这只是很多繁重的工作，我不认为有什么具体的技术障碍阻碍了我们。

**Elad:**  在大块“组织”研究方面，我们目前处于什么阶段？

**Andrej:**  “大块组织”研究？我们处于一个非常好的阶段。我认为 Transformer 的神奇之处还没有被充分认识到，它不仅仅是另一个神经网络，它是一个非常神奇、极其通用的神经网络。例如，当人们谈论神经网络的缩放法则时，这些缩放法则实际上在很大程度上是 Transformer 的一个特性。在 Transformer 出现之前，人们使用 LSTM 并将它们堆叠起来，你实际上无法获得清晰的缩放法则，而且它实际上无法训练，也无法工作。Transformer 是第一个真正可以缩放的东西，你可以得到缩放法则，一切都变得有意义了。所以，这是一个通用的训练计算机，我把它看作是一台计算机，但它是一台可微的计算机，你可以给它输入和输出，用反向传播训练数十亿个这样的计算机，它实际上可以把自己组织成一个可以完成任务的东西。所以我认为这实际上是我们偶然发现的一种神奇的算法。我认为其中有一些关键的创新，比如残差连接，这是一个已经存在的组件，还有层归一化，也需要加入其中，还有注意力机制，以及去除了像 tanh 这样的饱和非线性激活函数，因为它们会扼杀梯度信号。所以有四五个创新，它们都已存在，并被整合到 Transformer 中，这就是谷歌在他们的论文中所做的。这个东西实际上是可以训练的，突然之间你就可以得到缩放法则，突然之间你就拥有了这样一块可以训练的“组织”。这是一个重大的突破。

**Sarah:**  你觉得我们还没有接近这个突破的极限，对吗？因为我认为有一个关于数据瓶颈的讨论。

**AI 发展的瓶颈**

**Sarah:**  以及下一代规模化的成本问题，你是怎么看待这些问题的？

**Andrej:**  这就是问题的关键，我不认为神经网络架构本身是根本性的瓶颈，它已经不是瓶颈了。而在 Transformer 出现之前，它是瓶颈。现在它不是瓶颈了，所以现在我们更多地讨论损失函数是什么，数据集是什么。我们更多地讨论这些问题，它们几乎成为了瓶颈，而不是根据你的需求重新配置的通用“组织”。所以我认为这就是现在很多活动转移的方向，这也是为什么很多应用这项技术的公司和人员，他们不考虑 Transformer 架构，他们不考虑架构。你知道 Llama 的发布，Transformer 并没有太大变化，我们添加了 RoPE 相对位置编码，这是主要的变化，其他一切都不是那么重要，只是一些小的改进。实际上，只有 RoPE 被添加进来，这就是过去五年 Transformer 发生的变化。所以在这方面并没有太多的创新，每个人都认为这是理所当然的，让我们训练它等等。然后每个人主要是在数据集和损失函数的细节上进行创新，所以所有的活动都转移到了那里。

**Sarah:**  但在那个领域，有一种观点认为，当我们使用互联网数据时，情况更容易，而我们已经用完了互联网数据。所以问题实际上是围绕合成数据或更昂贵的数据收集。

**Andrej:**  我认为这是一个很好的观点，所以这也是目前 LLM 领域很多活动集中的地方。互联网数据并不是你想要的 Transformer 数据，它只是一个最近邻，实际上它可以让你走得很远，这很令人惊讶。但互联网数据是一堆网页，对吧？你真正想要的是你大脑内部的思想独白，这是理想的轨迹。你大脑中解决问题时的轨迹，如果我们有十亿个这样的轨迹，AGI 就基本上实现了，在很大程度上。但我们没有这些数据，所以现在很多活动都集中在，我们把互联网数据，这些数据实际上可以让你非常接近目标，因为互联网上恰好有足够的推理痕迹和大量的知识，Transformer 可以让它发挥作用。所以，我认为现在很多活动都围绕着将数据集重构成这些内部独白的形式，我认为有大量的合成数据生成可以帮助实现这一点。有趣的是，当前的模型在多大程度上帮助我们创建下一代模型，这就像一个不断改进的阶梯。

**Elad:**  你认为合成数据有多重要，或者说它能让我们走多远？因为就像你说的，每个模型都能帮助你更好地训练后续的模型，至少可以创建一些工具，比如数据标注等等。其中一部分是合成数据，你认为合成数据有多重要？

**Andrej:**  非常重要，我认为这是我们取得进展的唯一途径，我们必须让它发挥作用。我认为使用合成数据时，你只需要小心，因为这些模型会无声地坍塌，这是主要问题之一。如果你去问 ChatGPT 一个笑话，你会发现它只会说三个笑话，大多数时候它只会说一个笑话，有时它会说三个笑话，这是因为模型坍塌了，而且是无声的。当你查看任何单个输出时，你只看到一个例子，但当你实际查看分布时，你会发现它不是一个非常多样化的分布，它已经无声地坍塌了。当你进行合成数据生成时，这是一个问题，因为你实际上非常需要熵，你需要数据集的多样性和丰富性，否则你会得到坍塌的数据集。当你查看任何单个示例时，你都看不到这一点，但分布已经失去了大量的熵和丰富性，所以它会无声地恶化。这就是为什么你必须非常小心，你必须确保维持数据集中的熵，这方面有很多技术。例如，有人发布了一个角色（Persona）数据集，这个数据集包含 10 亿个角色，比如人类的背景信息。哦，我是一名教师，或者我是一名艺术家，我住在这里，我做这个等等。它就像关于虚构的人类背景的小段落。当你在做合成数据生成时，你不仅要说“完成这项任务并以这种方式完成”，还要想象你正在向这个人描述，你把这些信息放进去，现在你强迫它探索更多的空间，你获得了一些熵。所以，我认为你只需要非常小心地注入熵，维持分布，这是困难的部分，我认为人们可能没有充分认识到这一点。所以，基本上我认为合成数据绝对是未来，我们不会用完数据，这是我的印象，我只是认为你必须小心。

**人类认知与 AI 模型的类比**

**Elad:**  你认为我们现在从这项研究中学到了关于人类认知的哪些东西？

**Andrej:**  我不知道我们是否学到了什么。有人可能会说，例如，弄清楚我们想要的推理轨迹的形状，有助于实际理解大脑的工作原理，我会谨慎对待这些类比。但总的来说，我认为这是一种非常不同的东西，但我确实认为可以进行一些类比。例如，我认为 Transformer 在很多方面实际上比人类大脑更好，我认为它们实际上是一个效率更高的系统。它们之所以不如人类大脑，主要是数据问题，这是我的第一近似估计。例如，Transformer 记忆序列比人类强得多，如果你给它一个序列，并对该序列进行一次前向和后向传递，然后如果你给它前几个元素，它将完成序列的其余部分，它记住了那个序列，而且非常擅长。如果你让人类看一次序列，他们不可能记住它。所以，Transformer 实际上，我认为我们基于梯度的优化，我们一直用于训练神经网络的前向和后向更新，实际上在某些方面比大脑更有效。这些模型更好，它们只是还没有准备好大放异彩，但在许多认知方面，我认为它们可能会在正确的输入下变得更好。

**Elad:**  这对所有应用的计算机来说通常都是正确的，就像你说的记忆力。

**Andrej:**  完全正确。我认为人类大脑有很多限制，你知道，工作记忆非常小，我认为 Transformer 的工作记忆要大得多，而且这种情况将持续下去。它们是效率更高的学习者，人类大脑在各种限制下工作，我们并不清楚人类大脑中是否进行了反向传播，我们不清楚它是如何工作的。它是一个非常随机的动态系统，它有很多限制，它在环境条件下工作等等。所以我确实认为我们拥有的东西实际上可能比大脑更好，只是还没有到那一步。

**人类认知与 AI 能力的融合**

**Elad:**  你怎么看待随着时间的推移，用不同的 AI 系统增强人类？你认为这是一个可能的方向吗？还是不太可能？

**Sarah:**  增强？用 AI 模型增强人类？

**Elad:**  当然。

**Sarah:**  但在哪种意义上呢？

**Elad:**  也许，我认为总的来说是肯定的。因为我是说，它有一个抽象的版本，你把它当作一个工具来使用，这是外部版本，还有一种融合的场景，你知道很多人最终会谈论到。

**Sarah:**  是的，是的，我的意思是，我们已经在某种程度上融合了。

**Elad:**  问题是，存在输入输出瓶颈，但在大多数情况下，你知道，在你的指尖，如果你有任何这些模型，这有点不同。因为人们已经争论了 40 到 50 年，技术工具只是人类能力的延伸。

**Sarah:**  计算机是人类思维的自行车。

**Elad:**  完全正确。但在 AI 社区中有一部分人认为，例如，我们吸收未来 AI 潜在冲突或其他问题的方式，将通过某种形式的融合，比如 Neuralink 的设想等等。

**Andrej:**  是的，我还不知道这种融合会是什么样子，但我当然可以看到，你想要减少工具使用的输入输出延迟。我把它看作是在我们的新皮层之上构建的外脑皮层，它只是下一层，它恰好在云端等等，但它是大脑的下一层。

**Elad:**  2000 年代初的《Accelerando》一书中有一个版本，基本上一切都体现在一副与你的大脑相连的计算眼镜中，你戴着它，如果你丢了它，你一定会觉得自己失去了一部分个性和记忆。

**Andrej:**  我认为这很有可能。现在手机已经差不多是这样了，我认为当你把你的科技产品放在一边时，情况会更糟。你就像一个赤裸裸地身处大自然中的人类，你失去了部分智力，这非常令人焦虑。

**Sarah:**  一个非常简单的例子就是地图，我注意到现在很多人实际上不能很好地在他们的城市中导航了，因为他们总是使用逐向导航。如果我们有通用翻译器，我认为这并不遥远，如果你把你的设备放在一边，你就失去了与不会说英语的人交谈的能力。

**Andrej:**  我很乐意将我大脑的那部分重新用于进一步的研究。我不知道你是否看过那个孩子的视频，他拿着一本杂志，试图在杂志上滑动。让我着迷的是，这个孩子不明白什么是与生俱来的，什么是技术凌驾于自然之上的，因为它变得如此透明。我认为这可能看起来很相似，人们会开始假设这些工具的存在，然后当你把它们拿走时，你会意识到，我猜人们不知道什么是技术，什么不是。如果你戴着这个东西，它总是在为你翻译或做类似的事情，那么人们可能会失去基本的认知能力。

**Sarah:**  可能不再存在。

**Andrej:**  是的，天生如此，我们会专业化。你不理解说西班牙语的人，这太奇怪了。或者当你去迪士尼乐园时，所有的物体都是活的，我认为我们可能会进入一个这样的世界，为什么我不能和物体说话？就像今天你已经可以和 Alexa 说话，你可以让她做一些事情等等。

**Sarah:**  我见过一些玩具公司，他们基本上试图在玩具中嵌入一个 LLM，可以与孩子互动。

**Andrej:**  当你走到一扇门前，你不能直接说“开门”，这不是很奇怪吗？

**Elad:**  我不知道你是否看过《越空狂龙》或《我，机器人》，人们嘲笑你不能和物体说话的想法。

**Sarah:**  是的，如果你不能和物体说话，这太奇怪了。如果我们谈论的是一个外脑皮层，这感觉是一个非常重要的东西，可以普及它的使用。你怎么看待目前 LLM 研究的市场结构？你知道，只有少数几个大型实验室有能力进行下一代训练。这如何转化为人们在未来可以使用的东西？

**Andrej:**  你刚才提到的可能是生态系统的现状，对吧？我们有几个封闭平台的寡头垄断，然后我们有一个落后一些的开放平台，比如 Meta 的 Llama 等等，这有点像镜像了开源生态系统。我确实认为，当我们开始把它看作是一个外脑皮层时，在加密领域有一句话，“不是你的密钥，就不是你的代币”，那么“不是你的权重，就不是你的大脑”呢？这很有趣，因为一家公司有效地控制了你的外脑皮层，因此，如果这是我的外脑皮层，它的一部分开始感觉有点侵入性，我认为人们会更关心所有权。

**Sarah:**  是的，你意识到你在租用你的大脑。

**Elad:**  租用你的大脑似乎很奇怪。思想实验是，你是否愿意放弃所有权和控制权来租用一个更好的大脑？因为我愿意。

**Andrej:**  是的，我认为这就是权衡，我们会看到结果如何。但也许可以默认使用封闭版本，因为它们很棒，但在各种情况下你都有备用方案。我认为这有点像现在的情况，当一些封闭源提供商的 API 出现故障时，人们开始使用他们完全控制的开放生态系统作为备用方案，他们觉得自己有能力这样做。所以，也许这就是大脑的延伸，你应该在任何情况下都可以使用开源的东西，但在大多数情况下，你实际上是这样做的。所以开源的东西继续发展非常重要。

**Elad:**  我认为是这样的，100%。这并不是一个显而易见的观点，或者人们现在可能同意的观点，但我认为 100% 是这样。我一直在思考的一件事是，在某种意义上，你能达到的最小的性能模型是什么？无论是参数大小还是其他方面。我对你的观点有点好奇，因为你对蒸馏和小模型都思考了很多。

**构建高性能小模型**

**Andrej:**  我认为它可以非常小，而且我认为当前的模型浪费了大量的容量来记忆一些无关紧要的东西，比如它们记住 SHA 哈希值，它们记住一些古老的东西。

**Sarah:**  因为数据集没有得到最好的管理。

**Andrej:**  完全正确。我认为这些都会消失，我们只需要进入认知核心。我认为认知核心可以非常小，它只是用来思考，如果它需要查找信息，它知道如何使用不同的工具。

**Elad:**  那是 30 亿参数，还是 200 亿参数？

**Andrej:**  我认为甚至 10 亿就足够了，我们可能会达到那个程度，模型可以非常非常小。我认为它们可以非常小的根本原因是，我认为蒸馏是有效的，也许这是我唯一想说的，蒸馏的效果非常好。蒸馏是指你用一个非常大的模型或大量的计算资源或其他东西来监督一个非常小的模型，你实际上可以将大量的能力压缩到一个非常小的模型中。

**Elad:**  是否有某种数学表示或信息论公式来描述这一点？因为这似乎你应该能够计算出，就认知能力而言，最小值或最大值是多少？

**Andrej:**  也许，也许可以这样想，我们回到我们正在使用的互联网数据集，互联网数据中只有 0.001% 是认知，99.99% 是信息，你知道，我认为大部分信息对思考部分没有用。

**Elad:**  是的，我猜也许另一种表达这个问题的方式是，是否存在一个关于认知能力的数学表示，相对于模型大小，或者你如何根据你想要实现的目标，在最小值或最大值方面捕捉认知？也许没有好的方法来表示这一点。

**Andrej:**  所以我认为也许 10 亿参数可以让你获得一个良好的认知核心，我认为 10 亿可能都太多了，我不知道，我们会看到的，这非常令人兴奋。如果你考虑到边缘设备与云端的问题，以及使用模型的原始成本等等。

**Elad:**  是的，这非常令人兴奋。但在不到 10 亿参数的情况下，我在本地设备上也有我的外脑皮层。

**Andrej:**  是的，而且它可能不是一个单一的模型，对吧？思考这将如何发展是很有趣的，因为我确实认为你需要从并行化中获益。你不希望有一个顺序过程，你希望有一个并行过程。我认为公司在某种程度上也是一种工作的并行化，但公司内部存在层级结构，因为这是一种信息处理方式，以及组织内部需要进行的信息缩减。所以，我认为我们最终可能会得到由 LLM 组成的公司，这对我来说并不奇怪。你有不同能力的模型，专门针对各种独特的领域，也许有一个程序员等等，它实际上将开始在很大程度上类似于公司。你有程序员、项目经理，以及你知道的类似角色的 LLM，它们并行工作，并共同协调计算。所以，也许认为它更像是一个群体、一个生态系统是不正确的，它就像一个生物生态系统，你有专门的角色和生态位，我认为你会开始类似于这样。根据问题的难度，你会自动升级到群体的其他部分。CEO 是一个非常聪明的云模型，但主力可以是便宜得多的模型，甚至可能是开源模型，而且我的成本函数与你的成本函数不同。

**Sarah:**  是的，这可能很有趣。你离开了 OpenAI，你正在从事教育工作，你一直是一位教育工作者。

**Andrej 目前在 AI 教育方面的工作**

**Sarah:**  为什么选择做这个？

**Andrej:**  首先，我一直是一名教育工作者，我热爱学习和教学，所以这只是我长期以来一直充满热情的领域。我想说的另一点是，有一个宏观图景在推动着我，我认为 AI 领域有很多活动，我认为其中大部分是为了取代或替代人们，我会说这有点像把人们推到一边。但我总是对任何能够赋能人们的事情更感兴趣，我觉得我在高层次上是“人类团队”的一员，我对 AI 能够赋能人们的事情感兴趣。我不希望未来人们处于自动化的边缘，我希望人们处于一个被赋能的状态，我希望他们变得了不起，甚至比现在更了不起。我发现非常有趣的另一个方面是，如果一个人拥有所有学科的完美导师，他能走多远？我认为如果一个人拥有任何学科的完美课程，他可以走得非常远。我们可以在一些富人身上看到这一点，他们可能有私人导师，他们确实走得很远。所以我认为我们可以用 AI 来实现这一点，甚至超越它。实际上，这方面有非常清晰的文献，可以追溯到 80 年代，关于一对一辅导。

**Elad:**  我认为人们的学习效果可以提高一个标准差。

**Sarah:**  是两个吧？是 Bloom 的研究。

**Andrej:**  是的，没错，有很多非常有趣的前例。

**Elad:**  你如何看待通过 AI 的视角来实现这一点，或者说，什么样的首批产品能够真正帮助实现这一点？因为有一些书，比如《钻石时代》，里面提到了《少女指南》等等。

**Andrej:**  我确实受到了其中一些方面的启发。所以在实践中，我目前正在尝试构建一门课程，我希望它成为一门你想学习 AI 就可以学习的课程。问题的关键在于，我以前也教过课程，比如我在斯坦福大学教过 CS231n，那是第一门深度学习课程，而且非常成功。但问题是，你如何真正地扩展这些课程？你如何让你的目标受众可能是地球上的 80 亿人，而且他们说着不同的语言，能力水平也各不相同。你和一个老师无法扩展到这样的受众，所以问题是，你如何利用 AI 来扩展一位优秀教师的教学能力？我的想法是，教师主要负责课程的创建和课程设计，因为以目前的 AI 能力，我不认为模型能够创建一门好的课程。但我认为它们可以成为面向学生的前端，并向他们解释课程。所以，教师不再直接面向学生，教师不再是前端，教师在后端设计材料和课程，AI 是前端，它可以说各种不同的语言，它可以带你学习课程。

**Sarah:**  我应该把它想象成助教类型的体验，还是这不是一个好的类比？

**Andrej:**  这是我的一种思考方式，它是 AI 助教。我主要把它看作是面向学生的前端，它是实际与学生交互的东西，并带他们学习课程。我认为这在今天是可行的，而且它还不存在，我认为它可以做得非常好。然后随着时间的推移，随着能力的提高，你可以以各种方式重构这个设置。我喜欢找到 AI 能力的当前状态，并对它有一个好的模型。我认为很多公司可能没有直观地理解当前的能力在哪里，然后他们最终构建的东西有点超前于现有的能力，或者可能不够雄心勃勃。所以我认为这是一个可能的最佳点，而且非常有趣和令人兴奋。

**Sarah:**  我想回到你说的某一点，我认为这一点非常鼓舞人心，特别是考虑到你的背景和对我们目前研究水平的理解。从本质上讲，就是我们不知道在拥有更好的工具的情况下，人类在学习方面的表现极限是什么。我认为有一个非常简单的类比，我们一个月前刚举办了奥运会，对吧？你知道，一名跑步运动员，现在任何一项运动的最佳成绩都比 10 年前好得多，撇开兴奋剂不谈，只是因为你开始训练得更早，你有一个非常不同的训练计划，我们有更好的科学理解，我们有技术，我们有体育教育。你相信如果我们从工具和课程开始，我们可以作为人类走得更远，这太棒了。

**Andrej:**  我认为我们甚至还没有触及到可能性的表面。所以我认为基本上有两个维度，第一是全球化的维度，我希望每个人都能接受非常好的教育。但另一个维度是，一个人能走多远？我认为这两点都非常有趣和令人兴奋。

**Elad:**  通常当人们谈论一对一学习时，他们会谈论它的适应性方面，即你在他们所处的水平上挑战他们。你认为你今天可以用人工智能做到这一点，还是这是未来的事情？现在更多的是关于覆盖面和多种语言，以及全球最低的水果？

**Andrej:**  这些都是唾手可得的成果，例如不同的语言，我认为当前的模型实际上非常擅长翻译，并且可以有针对性地翻译材料。所以我认为很多事情都是唾手可得的成果。对个人背景的适应性，我认为不是唾手可得的成果，但我不认为它高高在上或遥不可及，这是你肯定想要的。因为不是每个人都有相同的背景，而且如果你熟悉过去的其他学科，那么将你所知道的东西进行类比是非常有用的，这在教育中非常有效。所以这绝对是一个你想要利用的维度，但我认为这开始变得不那么明显，需要一些工作。我认为简单的版本并不遥远，你可以想象只需提示模型，比如“嘿，我懂物理”或“我懂这个”，你可能会得到一些东西。但我猜我所说的是一些真正有效的东西，而不是一些你可以演示并且有时有效的东西。我的意思是它真的以人类的方式工作。

**Sarah:**  这就是我问适应性的原因，因为人们的学习速度也不同，或者某些事情他们觉得有挑战性，而其他人则不觉得，反之亦然。所以这有点像你如何根据具体情况进行调整，我想你可以随着时间的推移重新引入这个人擅长或不擅长的内容。这就是人工智能的问题，我觉得很多这些功能都只是一步之遥，所以你总是可以得到演示，但你真的得到了一个产品吗？你明白我的意思吗？所以在这个意义上，我想说演示很接近，但产品还很远。

**AI 驱动的教育如何重塑知识网络和地位**

**Sarah:**  我们之前谈到的一件事，我认为非常有趣，就是你在研究社区中看到的“血统”现象，你来自某些实验室，每个人都在八卦彼此的实验室。我认为很大一部分诺贝尔奖获得者实际上曾经在以前的诺贝尔奖获得者的实验室工作过，所以这里存在一些传播，我不知道是文化、知识、品牌还是什么。在一个以人工智能教育为中心的世界里，你如何看待这些网络和知识的传播？你如何看待“血统”，或者它是否重要？

**Andrej:**  我实际上不想生活在一个“血统”过于重要的世界里，所以我希望人工智能能帮助我们稍微打破这种结构。这感觉有点像通过一些有限的稀缺资源进行把关，比如拥有这种“血统”的人数是有限的。所以我觉得这有点像那方面的问题，所以我希望它能打破这一点。这绝对是一方面，实际学习是一方面，“血统”是另一方面。

**Elad:**  这也是一个聚合的集群效应，对吧？就像为什么所有的或大部分的人工智能社区都在湾区，或者为什么大部分的金融科技社区都在纽约？所以我认为很大程度上也是因为你将具有共同兴趣和信念的非常聪明的人聚集在一起，然后他们从这个共同的核心传播出去，然后他们以一种有趣的方式分享知识。

**Andrej:**  是的，在某种程度上，很多这种行为已经转移到了网上，特别是对于年轻人来说。我认为其中一个方面有点像教育方面，如果你是当今社区的一部分，你会得到大量的教育和学徒训练等等，这非常有帮助，可以让你在这个领域达到一个被赋能的状态。我认为另一方面是文化方面，你的动机是什么，你想做什么，文化推崇什么，他们把什么放在基座上，他们基本上崇拜什么。例如，在学术界，是 H 指数，每个人都关心 H 指数，你发表的论文数量等等。我曾经是那个社区的一员，我看到了这一点，我觉得现在我来到了不同的地方，在所有不同的社区中都有不同的偶像。我认为这对人们的动机、他们在哪里获得社会地位以及什么对他们来说真正重要有很大的影响。我在斯洛伐克长大，那也是一个非常不同的环境，在加拿大也是一个非常不同的环境，那里重要的事情也不同。

**Sarah:**  抱歉打断一下，是冰球吗？

**Andrej:**  哈哈，举个例子，我想说在加拿大，我在多伦多大学，多伦多，我不认为那是一个非常鼓励创业的环境，你甚至不会想到你应该创业。人们不这样做，你也不知道有朋友在这样做，你不知道你应该崇拜它，人们不会阅读关于所有创始人的书籍并谈论他们，这不是你渴望或关心的事情。每个人都在谈论的是，哦，你在哪里实习，你以后要去哪里工作？人们只是接受了有一组固定的公司，你应该从中选择一个，并让自己与其中一个保持一致，这就是你所崇拜的或类似的东西。所以这些文化方面非常强大，甚至可能是主导变量，因为我几乎觉得现在教育方面已经更容易了，很多东西都已经可用了等等。所以我认为这主要是你所处的文化方面的问题。

**Sarah:**  关于这一点，你和我几周前谈到的一件事是，我认为你也在网上发布了关于这一点的内容，学习和娱乐之间是有区别的，学习实际上应该是困难的。我认为这与地位的问题有关，以及谁是偶像。你认为通过这样的系统，你能在多大程度上改变动机？如果这是一个阻碍因素，你是否专注于为人们提供资源，使他们能够在自己的能力范围内尽可能地走得更远，比历史上任何时候都更远，这已经很鼓舞人心了。或者你是否真的想改变有多少人想要学习，或者至少让自己走上这条道路？

**Andrej:**  “想要”是一个有歧义的词，我想说我想让学习变得更容易，然后也许人们可能不想学习。例如，今天人们想学习是为了实际的原因，比如他们想找工作等等，这完全说得通。所以在前 AGI 社会，教育是有用的，我认为人们会受到这种激励，因为他们在经济阶梯上向上爬。在后 AGI 社会，我们只是所有的社会，我认为教育在更大程度上是一种娱乐，包括成功的教育成果。

**Elad:**  对，不仅仅是让内容冲刷你的大脑。

**Andrej:**  是的，我认为是这样的。

**Elad:**  成果是指理解、学习、能够贡献新知识，或者你怎么定义它。

**Andrej:**  我认为这不是巧合，如果你回到 200 年前、300 年前，从事科学研究的人都是贵族或有钱人。

**Elad:**  我们都将成为贵族，和 Andrej 一起学习。

**Andrej:**  是的，我确实认为这与你之前的引言非常相似，我觉得学习有点像去健身房锻炼大脑，这感觉就像去健身房。我的意思是，去健身房很有趣，人们喜欢举重等等。

**Elad:**  有些人不去健身房。

**Andrej:**  不不不，有些人去，但这需要努力。

**Elad:**  是的，这需要努力。

**Andrej:**  这需要努力，但这也很有趣，你也会有回报，你会以各种方式对自己感觉良好，对吧？我认为教育基本上与此相同。所以这就是当我说的“教育不应该是‘有趣的’”等等时的意思，我的意思是它确实很有趣，但这是一种特定的乐趣，我想是这样的，对吧？我确实认为，也许在后 AGI 时代，我希望发生的事情是，人们实际上会经常去“健身房”，不仅是身体上的，还有精神上的，这是我们所崇尚的，受过高等教育，而且，是的。

**Sarah:**  我能问你最后一个关于 Eureka 的问题吗？因为我认为这对人们来说会很有趣，第一门课程的受众是谁？

**Eureka Labs**

**Andrej:**  我主要把它看作是一门本科水平的课程，所以如果你正在攻读技术领域的本科学位，我认为这将是理想的受众。我认为我们现在看到的是，我们有一个过时的教育概念，你上学，然后毕业去工作，对吧？显然，这将完全崩溃，特别是在一个变化如此之快的社会中，随着技术变化得非常非常快，人们会更频繁地回到学校。所以这有点像本科水平，但我想说，任何年龄段的任何处于这个水平的人都在范围内，我认为年龄会非常多样化。但我认为这主要是针对那些有技术背景并且主要想真正理解它的人，达到一定的程度。

**Sarah:**  他们什么时候可以学习这门课程？

**Andrej:**  我本来希望是在今年年底，但我有很多事情要做，所以我认为可能是明年初，这是大概的时间表。我正在努力把它做得非常好，这需要时间才能达到目标。

**Sarah:**  实际上我还有一个最后的问题，与此有点相关，如果你现在有小孩，你认为他们应该学习什么才能拥有一个有用的未来？

**年轻人应该学习什么来为未来做好准备**

**Andrej:**  在我看来，有一个正确的答案，正确的答案主要是，我想说像数学、物理、计算机科学这样的学科。我这么说的原因是因为我认为它有助于思维技能的培养，这只是最好的思维技能核心，这是我的观点。当然，我有特定的背景等等，所以我会这么认为，但这只是我的观点。我认为我上物理课和所有这些其他课程，塑造了我的思维方式，我认为这对解决问题非常有帮助等等。所以，如果我们在一个前 AGI 的世界里，这将是有用的，在后 AGI 时代，你仍然需要能够以任何能力运作的被赋能的人类。所以我认为这对人们来说是正确的答案，他们应该做什么和学习什么，它要么有用，要么有益。所以我认为这是正确的答案，我认为很多其他的东西你可以在以后补充，但在人们有很多时间和注意力的关键时期，我认为应该主要花在做这些繁重的、简单的操作任务和工作上，而不是繁重的记忆任务和工作上。我获得了数学学位，我觉得当我这样做的时候，我的大脑中开辟了一条新的沟槽，这条沟槽以后很难开辟。当然，我也会加入其他的东西，我不反对所有其他的学科等等，我认为拥有广泛的多样性实际上是美好的。但我确实认为 80% 的时间应该花在这类事情上，因为与我们的工具相比，我们不是高效的记忆者。

**Sarah:**  感谢你参加我们的节目，非常有趣！

**Andrej:**  很高兴来到这里！

**Sarah:**  在 Twitter 上关注我们 @NoPriorsPod，订阅我们的 YouTube 频道，如果你想看到我们的脸，可以在 Apple Podcasts、Spotify 或任何你收听播客的地方关注我们的节目，这样你每周都会收到新的节目，并在 no-priors.com 上注册电子邮件或查找每集的文字记录。


---


