Jeff Dean & Noam Shazeer：谷歌25年历程，从PageRank到通用人工智能（AGI）  
- 原文标题：Jeff Dean & Noam Shazeer – 25 years at Google: from PageRank to AGI  
- 链接：[YouTube视频链接](https://www.youtube.com/watch?v=v0gjI__RyCY&t=74s)  

- **文章类别**：访谈实录  

---

**内容整理**：

### 访谈主题
Jeff Dean和Noam Shazeer在谷歌的25年历程，从最初的PageRank、MapReduce等技术，到如今的通用人工智能（AGI）和Gemini模型。

### 访谈嘉宾
- **Jeff Dean**：谷歌首席科学家，参与了谷歌众多变革性技术的研发，如MapReduce、BigTable、TensorFlow、AlphaChip和Gemini等。
- **Noam Shazeer**：现代大型语言模型（LLM）架构和技术创新的核心人物，包括Transformer、Mixture of Experts、Mesh TensorFlow等，并且是Gemini项目的共同负责人之一。

### 访谈框架
```
├── 个人经历与早期加入谷歌
│   ├── Jeff Dean的早期工作（如MapReduce、BigTable）
│   ├── Noam Shazeer加入谷歌的契机
├── 谷歌的技术发展与变革
│   ├── 从理解公司运作到失去“全知”状态
│   ├── 谷歌招聘与加入谷歌的动机
├── 硬件与算法的协同进化
│   ├── Moore's Law对系统设计的影响
│   ├── 硬件加速与算法优化的结合
├── 语言模型与AI的发展
│   ├── 早期语言模型（如2007年的2万亿token N-gram模型）
│   ├── Transformer架构的诞生与影响
│   ├── Gemini模型的当前进展
├── 未来展望
│   ├── 长期上下文处理与信息检索的融合
│   ├── AI在多模态和代码生成中的应用
│   ├── 推理时间计算的提升
└── 责任与伦理
    ├── AI的潜在风险与防范措施
    ├── 谷歌的AI原则与未来发展方向
```

### 核心内容
- **个人经历与早期加入谷歌**  
  - **Jeff Dean**：2000年加入谷歌，早期参与了搜索、爬虫和索引系统的开发，见证了谷歌从25人小团队到全球性公司的成长。  
  - **Noam Shazeer**：1999年在伯克利读博期间对谷歌产生兴趣，2000年加入谷歌，最初被吸引是因为谷歌的搜索技术和公司文化。  

- **谷歌的技术发展与变革**  
  - **理解公司运作**：随着公司规模扩大，从了解每个人的工作内容到只能了解项目进展，再到对新项目的陌生感。  
  - **加入谷歌的动机**：Noam最初希望在谷歌积累财富后专注于AI研究，但最终发现谷歌是AI研究的理想场所。  

- **硬件与算法的协同进化**  
  - **Moore's Law的影响**：过去几十年中，硬件性能提升对系统设计的影响，尤其是计算加速和算法优化的结合。  
  - **硬件加速**：从通用CPU到专用TPU和GPU的演变，推动了深度学习的发展。  

- **语言模型与AI的发展**  
  - **早期语言模型**：2007年，谷歌训练了一个2万亿token的N-gram语言模型，用于机器翻译，但面临性能瓶颈。  
  - **Transformer架构**：Noam Shazeer是Transformer架构的发明者之一，这一架构彻底改变了现代语言模型的设计。  
  - **Gemini模型**：目前，Gemini模型在多模态处理和推理能力上取得了显著进展，未来将支持更长的上下文和更复杂的任务。  

- **未来展望**  
  - **长期上下文处理**：将搜索引擎的广泛索引能力和语言模型的深度推理能力结合，支持更长的上下文处理。  
  - **多模态与代码生成**：AI在视频、音频和代码生成等多模态领域的应用前景广阔。  
  - **推理时间计算**：通过增加推理时的计算量，提升模型的准确性和效率。  

- **责任与伦理**  
  - **AI的潜在风险**：讨论了AI可能带来的风险，如错误信息传播和自动化攻击。  
  - **谷歌的AI原则**：强调谷歌在AI开发中的责任，确保模型的安全性和可靠性。

### 文章标签
#谷歌 #人工智能 #技术发展 #语言模型 #硬件加速 #AI伦理


---

## 视频要点

**0:00:00 - 开场介绍**

-   Dwarkesh Patel 介绍嘉宾 Jeff Dean 和 Noam Shazeer。
-   Jeff Dean 是 Google 首席科学家，参与了 MapReduce、BigTable、Tensorflow、AlphaChip、Gemini 等项目。
-   Noam Shazeer 是 Transformer、Mixture of Experts、Mesh Tensorflow、Gemini 等现代 LLM 关键架构和技术的发明者或共同发明者。
-   讨论主题包括他们在 Google 的 25 年，从 PageRank 到 MapReduce，再到 Transformer、MoE、AlphaChip，以及未来的 AGI/ASI。
-   特别提到 Jeff 对 Pathways 的愿景：硬件和算法设计的互补循环，以及超越自回归。
-  Noam 展望：世界GDP将增长100倍；在谷歌数据中心运行一百万个自动化研究员；活到 3000 年。

**0:03:29 - 1999 年加入 Google**

- Noam Shazeer 在2000年底加入Google, 他的导师是Jeff Dean.
- 当时公司规模很小，Jeff Dean 几乎编写了所有代码。
- 随着公司发展，经历了几个阶段：认识所有人 -> 认识所有工程师 -> 认识所有项目 -> 不知道所有项目（收到“Platypus 项目”邮件）。
- Noam 在 1999 年的一个招聘会上看到 Google，但认为它已经很大了。2000 年申请，被 Google 的指数级增长的搜索查询量图表吸引。
- Noam 当时想通过在初创公司赚钱，然后有足够的钱来做 AI 研究。
- Noam提到朋友在2000年新年愿望活到3000年，通过发明AI来实现.
- Google 的目标“组织世界信息并使其普遍可用和有用”需要先进的 AI。

**0:06:20 - 摩尔定律的未来**

- 摩尔定律在过去几十年发生了变化。
- 20 年前到 10 年前，硬件每 18 个月就会变得更快。
- 最近，通用 CPU 的性能提升放缓（制造工艺改进需要 3 年，多核处理器的改进不如以前）。
- 但同时，出现了更多专用计算设备，如机器学习加速器（TPU 和 ML 专用 GPU）。
- 算法正在跟随硬件发展。算术变得非常便宜，而数据移动相对昂贵。深度学习得益于此（矩阵乘法）。
- 从 CPU 和 GPU 转向专门为深度学习设计的 TPU 是一个重要的转变。
- 关键是识别机会成本，例如，Larry Page 认为第二大成本是税收，最大成本是机会成本。
- 通过量化可以大幅提高吞吐量与成本的比率。

**0:11:04 - 未来的 TPU**

-   一个趋势是模型量化（降低精度）。
-   TPUv1 使用 8 位整数进行推理。
-   现在可以使用更低的精度进行训练（INT4、FP4，甚至 2 位或 1 位）。
-   算法设计者和芯片设计者需要共同设计，以实现性能提升。

**0:13:56 - Jeff 的本科论文：并行反向传播**

-   Jeff 在 1990 年的本科毕业论文中研究了神经网络的并行反向传播训练。
-   在 32 处理器 Hypercube 机器上实现了模型并行性和数据并行性。
-   当时认为神经网络是正确的抽象，但需要大约 100 万倍的计算量才能解决实际问题。

**0:15:54 - 2007 年的 LLM**

-   2007 年，Google 的机器翻译团队参加了 DARPA 竞赛。
-   Google 的系统赢得了比赛，但翻译一个句子需要 12 个小时。
-   Jeff 与团队合作设计了一个内存压缩的 N-gram 数据表示，将翻译时间缩短到 100 毫秒。
-   使用了 5-gram 模型，处理了 2 万亿个单词。
-   构建了一个数据结构，可以在 200 台机器上存储所有数据。
-  之后大型语言模型开始被用于其他任务，如自动补全。

**0:25:09 - “Holy shit”时刻**

-   Google Brain 团队早期构建了一个可以训练非常大的神经网络的基础设施。
-   使用 2000 台计算机（16000 个核心）对 1000 万个随机选择的 YouTube 帧进行无监督学习。
-   模型能够构建一个表示，其中一个神经元会对猫的图像产生兴奋。
-   在有监督的 ImageNet 20000 类别挑战赛上取得了很好的结果，相对提高了 60%。
-   这表明扩大神经网络规模是有效的。

**0:27:28 - AI 实现 Google 的最初使命**

-   Google 是一家“组织世界信息”的公司，这比信息检索更广泛。
-   AI 可以根据指导创建新信息（例如，写信、总结视频）。
-   多模态能力表明 AI 不仅仅是文本，还包括理解世界上的各种信息（人类和非人类的，如激光雷达、基因组信息、健康信息）。
-   目标是将信息转化为有用的见解，帮助人们做各种事情（娱乐、复杂问题、多模态、编码）。

**0:32:00 - 在上下文中进行搜索**

-   当前模型存在幻觉和事实性问题。
-   上下文窗口中的信息非常清晰，因为有注意力机制。
-   希望模型能够处理数万亿个 token（整个互联网、所有个人信息）。
-   需要算法近似来实现对大量 token 的概念性关注。
-   模型参数可以高效地记忆事实，而上下文中的 token 需要更多内存。

**0:36:12 - 内部编码模型**

-   Google 已经使用内部代码库对 Gemini 模型进行了进一步训练。
-   Sundar 表示，25% 的代码是由 AI 模型生成的。

**0:37:29 - 2027 年的模型会做什么？**

-   希望模型能够更高效地帮助研究人员。
-   AI 可以帮助软件开发人员提高生产力（例如，从高级规范生成代码）。
-   研究人员可以指导系统生成实验代码并运行。
-  百万员工可以瞬间生成，而且可以互相检查。

**0:43:20 - 每天都有新架构？**

-   可能有更多的 AI 研究人员（NeurIPS 有 15000 人参加）。
-   如果研究人员数量增加 1000 倍，可能会每年或每天都有突破。
-   小规模实验对于验证想法很重要。

**0:49:10 - 自动化芯片和智能爆炸**

-   未来 AI 改进可能部分由硬件和规模驱动，但更重要的是算法改进和模型架构变化。
-   自动化探索想法可以更快地推进。
-   芯片设计过程可以大大加快（从 18 个月缩短到几个月）。
-   这可以实现更多的专业化，缩短硬件设计时间。
-   如果制造时间在改进的内循环中，可能会出现快速改进。
-  智能可能以越来越快的速度爆炸。

**0:53:07 - 未来推理扩展**

-   推理时计算是未来的一个重要改进领域。
-   即使是大型语言模型，每次 token 的操作成本也很低（比阅读纸质书便宜得多）。
-   可以通过增加计算量使模型更智能。
-   模型可以主动探索不同的解决方案，进行搜索，获取信息。
-   可以有一个“旋钮”，通过增加推理时计算量来获得更好的答案。
-   Rich Sutton 的“Bitter Lesson”论文表明，学习和搜索是非常有效的技术。

**1:02:38 - 已经进行多数据中心运行**

-   已经在多个都市区域进行多数据中心训练。
-   训练对延迟不敏感，带宽更重要。
-   异步训练可以帮助扩展。
-   同步训练更容易理解，但异步训练可以扩展更多。

**1:08:15 - 大规模调试**

-   小规模实验对于发明改进很重要。
-   大规模实验需要将改进堆叠在一起，看看它们是否有效。
-   复杂性会带来问题，但需要权衡。

**1:12:41 - 快速起飞和超级对齐**

-   模型正在逐代变得更好。
-   模型能够将任务分解为更多步骤，并更可靠地解决问题。
-   需要理解 AI 的进展，并确保社会从中受益（教育、医疗保健）。
-   需要防范风险（错误信息、自动黑客攻击）。
-   Google 的 Responsible AI 原则是一个很好的框架。
-   AI 的发展可能会很快，需要谨慎。

**1:20:51 - 一百万个邪恶的 Jeff Dean**

-   如果出现一个与 Jeff Dean 或 Noam Shazeer 水平相当的模型，并且有数百万个邪恶版本，那将非常糟糕。
-   需要确保 AI 的安全性。
-   可以使用 AI 来检查自身和其他系统。
-   需要控制 AI 的能力，防止其被滥用。

**1:24:22 - 在 Google 的有趣时光**

-   早期在 Google 的时光非常有趣，当时流量增长迅速，正在构建被数十亿人使用的系统。
-   现在与 Gemini 团队合作也很令人兴奋。
-   模型正在变得越来越好，这令人难以置信。

**1:27:51 - 2030 年的世界计算需求**

-   推理计算需求将大幅增长。
-   人们会发现这些模型可以做各种事情，使用量会增加。
-   可能需要非常高效的推理硬件。
-   AI 可能会消耗世界 GDP 的很大一部分。
-   世界 GDP 可能会大幅增长。
-   可能会有数百万到数十亿个机器人建造数据中心。
-   每个人使用的 AI 计算量将是天文数字。
-   需要廉价的硬件平台。

**1:34:37 - 回归模块化**

-   持续学习（模型随着时间推移而改进）是一个有趣的方向。
-   稀疏模型很重要，因为不同的部分擅长不同的事情。
-   Gemini 1.5 Pro 是一个混合专家模型。
-   未来的模型可能具有更自然的结构，模块可以独立开发。
-   可以有专门的团队改进特定语言或问题。
-   这是一种持续学习的形式。
-   可以更快地进行研究。

**1:44:48 - 将大型 MoE 保存在内存中**

-   混合专家模型需要将整个模型保存在内存中。
-   为了提高效率，需要使用非常大的批处理大小。
-   未来的专家可能会在计算成本上有所不同。
-   Pathways 系统支持可变成本的组件。
-   需要非常大的批处理，并在推理时异步推送。

**1:49:35 - 一个模型中的所有 Google**

-   Google 的许多服务可以使用 Gemini 模型。
-   可以共享底层模型的能力。
-   可能会有一个大型基础模型，并为不同设置添加不同的模块。
-   可能会有内部模块供 Google 员工使用。

**1:57:59 - 蒸馏缺少什么**

-   蒸馏是一种有用的工具，可以将模型转换为不同的形式。
-   可以在模块级别进行蒸馏。
-   需要更好的蒸馏技术。
-   可能需要改变训练目标。
-   模型应该在某些 token 上更加努力地思考。
-   可以从视觉数据中学习更多。
-   模型可以通过采取行动来学习。

**2:03:10 - 开放研究的利弊**

-   分享信息有助于看到机会的大小。
-   世界将会有巨大的进步。
-   Google 会发布一些信息，但不会发布所有信息。
-   需要在发布、产品化和保密之间进行权衡。
-   参加 NeurIPS 等会议并分享想法很重要。

**2:09:58 - 坚持到底**

-   了解新领域的一种方法是关注正在发生的事情，与同事交谈，关注研究论文。
-   与具有不同专业知识的人合作可以做一些个人无法做到的事情。
-   谦逊很重要。
-   需要激励人们放弃不起作用的想法并尝试新事物。
-   Google Brain 的 UBI 芯片分配很有帮助。
-   需要自上而下和自下而上的结合，以激励协作和灵活性。
-   提出有趣的方向很重要。

---

## 视频脚本

*   **Dwarkesh Patel (DP)**: 采访者
*   **Jeff Dean (JD)**: 受访者，Google 首席科学家
*   **Noam Shazeer (NS)**: 受访者，AI 领域专家，Gemini 联合负责人


### **0:00:00 - 开场介绍**

**DP**: 今天我非常荣幸地邀请到 Jeff Dean 和 Noam Shazeer。Jeff 是 Google 的首席科学家，在 Google 的 25 年里，他参与了现代计算领域最具变革性的系统：MapReduce、BigTable、Tensorflow、AlphaChip，当然还有现在的 Gemini。Noam 则是当前 AI 革命的功臣，他是现代 LLM 中所有主要架构和技术的发明者或共同发明者：Transformer、Mixture of Experts、Mesh Tensorflow 等等。他们也是 Google DeepMind Gemini 的三位联合负责人中的两位。非常感谢二位参加这次访谈。

**JD**: 谢谢，很高兴来到这里。

---

### **0:03:29 - 1999 年加入 Google**

**DP**: 你们都在 Google 工作了 25 年或接近 25 年。在公司早期，你们可能了解所有事情是如何运作的。这种情况是什么时候停止的？你们觉得有一个明确的时刻吗？

**NS**: 我是在 2000 年底加入的，当时公司有一个传统：每个人都有一个导师。我当时什么都不懂，就问我的导师所有问题，而我的导师什么都知道。后来发现我的导师是 Jeff。并不是 Google 的每个人都知道所有事情，只是 Jeff 知道，因为他基本上编写了所有代码。

**JD**: 你过奖了。我认为随着公司的发展，你会经历不同的阶段。我加入的时候，我们大概有 25、26 个人。所以你最终会记住每个人的名字，即使我们在不断壮大，你也能记住所有新加入的成员。但在某个时候，你记不住公司里每个人的名字了，但你仍然知道所有从事软件工程工作的人。然后你又记不住软件工程团队里所有人的名字了，但你至少知道每个人正在做的所有不同项目。再后来，公司变得足够大，你会收到一封电子邮件，说 Platypus 项目将在周五发布，而你会想：“Platypus 项目到底是什么？”

**NS**: 通常这是一个非常好的惊喜。你会想：“哇，Platypus 项目！我完全不知道我们在做这个。”

**JD**: 但我认为，即使你不知道每一个细节，也要了解公司的情况，哪怕是很高的层面。认识公司里的很多人也很重要，这样你就可以向别人询问更多细节，或者找出该和谁谈。通过一层间接关系，通常可以在公司里找到合适的人，如果你有一个随着时间推移建立起来的良好人际网络的话。

**DP**: 顺便问一下，Google 是如何招募你们的？

**JD**: 实际上，是我主动联系了他们。

**DP**: Noam，你是如何被招募的？

**NS**: 1999 年我在一个招聘会上看到了 Google，我以为它已经是一家大公司了，觉得加入没有意义，因为我认识的每个人都在用 Google。我想那是因为我当时是伯克利的在读研究生。我好像从几个研究生项目中辍学过几次。后来发现它其实并没有那么大。我没有在 1999 年申请，但在 2000 年心血来潮给他们发了一份简历，因为我觉得这是我最喜欢的搜索引擎，我想我应该申请多个地方的工作。但后来发现这真的很有趣，看起来像是一群聪明人在做好事。他们墙上有一张非常漂亮的蜡笔图，显示每天的搜索查询量，是有人一直在维护的。它看起来非常符合指数级增长。我想，“这些人会非常成功，而且看起来他们有很多很好的问题要解决。”所以我想，“好吧，也许我会在那里工作一段时间，然后有足够的钱去做我想做的 AI 工作。”

**DP**: 从某种程度上来说，你做到了，对吧？

**NS**: 是的，完全按照计划进行了。

**DP**: 你在 1999 年就开始考虑 AI 了？

**NS**: 是的，那是 2000 年。我记得在研究生院的时候，我的一个朋友告诉我，他 2000 年的新年愿望是活到 3000 年，他打算通过发明 AI 来实现这个目标。我想，“哦，这听起来是个好主意。”

我当时没有想到你可以在一家大公司做这件事。但我想，“嘿，似乎有很多人在初创公司赚了很多钱。也许我也可以赚点钱，然后我就有足够的钱来长期从事 AI 研究。”但实际上，Google 是一个非常适合从事 AI 工作的地方。

**JD**: 我喜欢 Google 的一点是，我们的雄心一直都是需要非常先进的 AI 才能实现的。因为我认为“组织世界信息并使其普遍可用和有用”实际上是一个非常广泛的任务。这并不是说公司会做一件小事然后就一直做下去。而且你也可以看到，我们最初所做的事情是朝着这个方向发展的，但你可以在这个方向上做更多的事情。

---

### **0:06:20 - 摩尔定律的未来**

**DP**: 在过去二三十年里，摩尔定律如何改变了你在设计新系统、确定哪些项目可行时必须考虑的因素？现在的限制是什么？现在能做哪些以前显然不能做的事情？

**JD**: 我认为在过去的几十年里，情况发生了很大的变化。从 20 年前到 10 年前，情况非常好，因为你只需要等待，18 个月后，你就会得到更快的硬件，而你不需要做任何事情。但最近，我觉得基于通用 CPU 的机器扩展并没有那么好，制造工艺的改进现在需要三年而不是两年。多核处理器和类似技术的架构改进也没有像 20 到 10 年前那样给我们带来同样的提升。但与此同时，我们看到更多专用的计算设备，如机器学习加速器、TPU，以及最近更专注于 ML 的 GPU，这使得我们实际上可以从我们想要运行的更现代的计算类型中获得非常高的性能和良好的效率，这些计算类型不同于试图运行 Microsoft Office 或类似东西的 C++ 代码。

**NS**: 感觉算法正在跟随硬件发展。基本上，现在的情况是，算术非常非常便宜，而数据移动相对昂贵得多。所以几乎所有的深度学习都是因此而兴起的。你可以用 N³ 次运算和 N² 字节的数据通信来实现矩阵乘法。

**JD**: 我认为转向面向硬件是一个重要的转变，因为在那之前，我们的 CPU 和 GPU 并不特别适合深度学习。然后我们开始在 Google 构建 TPU，它们实际上只是低精度线性代数机器，一旦你拥有了它，你就会想利用它。

**NS**: 关键似乎是识别机会成本。就像 Larry Page 经常说的：“我们的第二大成本是税收，我们的最大成本是机会成本。”如果他没说过，那我就引用错他很多年了。

但基本上，机会成本是指你错过的机会是什么？在这种情况下，我想就是你有这么多的芯片面积，但你只放了很少的算术单元。把整个芯片都填满算术单元！你可以完成数量级更多的算术运算。

现在，还有什么需要改变？算法和数据流等等。

**JD**: 顺便说一句，算术可以是真正的低精度，这样你就可以挤进更多的乘法器单元。

---

### **0:11:04 - 未来的 TPU**

**DP**: Noam，我想继续讨论你说的算法一直在跟随硬件发展。如果你想象一个反事实的世界，假设内存成本的下降幅度大于算术成本，或者颠倒一下你看到的动态。

**NS**: 好吧，数据流非常便宜，而算术不便宜。

**DP**: 今天的 AI 会是什么样子？

**JD**: 你会有更多对非常大的内存的查找。

**NS**: 是的，它可能看起来更像 20 年前的 AI，但方向相反。我不确定。我好像是 2012 年加入 Google Brain 的。我离开 Google 几年，碰巧回去和我妻子一起吃午饭，我们碰巧坐在 Jeff 和早期的 Google Brain 团队旁边。我想，“哇，这是一群聪明人。”

**JD**: 我好像说过，“你应该考虑一下深度神经网络。我们在那里取得了一些不错的进展。”

**NS**: “这听起来很有趣。”好吧，所以我又回来了……

**JD**: 我把他拉回来了，这太棒了。

**NS**: ……加入 Jeff，那是 2012 年。我好像每 12 年加入一次 Google：2000 年、2012 年和 2024 年。

**DP**: 2036 年会发生什么？

**NS**: 我不知道。我想我们拭目以待。

**DP**: 你们正在考虑改变未来版本的 TPU 的哪些权衡，以整合你们对算法的思考？

**JD**: 我认为一个总体趋势是，我们在量化或降低模型精度方面做得越来越好。我们从 TPUv1 开始，甚至不确定是否可以用 8 位整数对模型进行量化和服务。但我们有一些早期证据表明这可能是可行的。所以我们说，“太好了，让我们围绕这个构建整个芯片。”

然后随着时间的推移，我认为你已经看到人们也能够使用更低的精度进行训练。但推理精度也提高了。人们现在使用 INT4 或 FP4，如果你对 20 年前的超级计算浮点数专家说我们要使用 FP4，他们会说，“什么？这太疯狂了。我们喜欢浮点数中的 64 位。”

甚至更低，有些人正在将模型量化为两位或一位，我认为这是一个趋势，肯定——

**DP**: 一位？就像 0 或 1？

**JD**: 是的，就是 0-1。然后你有一组位的符号位或类似的东西。

**NS**: 这确实需要共同设计，因为如果算法设计者没有意识到你可以通过较低的精度获得大大提高的性能和吞吐量，算法设计者当然会说，“我当然不想要低精度。这会带来风险。”然后这会让人恼火。

然后如果你问芯片设计者，“好吧，你想构建什么？”然后他们会问今天编写算法的人，他们会说，“不，我不喜欢量化。这很烦人。”所以你实际上需要看到整个图景，并弄清楚，“哦，等等，我们可以通过量化来大大提高我们的吞吐量与成本的比率。”

**JD**: 然后你会说，是的，量化很烦人，但你的模型会快三倍，所以你必须处理。

---

### **0:13:56 - Jeff 的本科论文：并行反向传播**

**DP**: 在你们的职业生涯中，你们都曾在不同时期从事过与我们现在用于生成式 AI 的东西非常相似的工作。1990 年，Jeff，你的毕业论文是关于反向传播的。2007 年——这是我在准备这一集之前才意识到的事情——你们在 2007 年训练了一个用于语言建模的两万亿个 token 的 N-gram 模型。

请带我回顾一下你们开发那个模型时的情景。你们当时是怎么想的？你们认为你们当时在做什么？

**JD**: 让我从本科论文开始说起。我在大四的时候，在一门关于并行计算的课程的一个章节中接触到了神经网络。我需要写一篇毕业论文才能毕业，一篇荣誉论文。所以我找到了教授，我说，“哦，做一些关于神经网络的事情会很有趣。”

所以，我和他决定我要在 1990 年实现几种不同的并行化反向传播训练神经网络的方法。我在论文中用了一些有趣的名称来称呼它们，比如“模式分区”之类的。但实际上，我在一台 32 处理器的 Hypercube 机器上实现了模型并行性和数据并行性。

在一种方法中，你将所有示例分成不同的批次，每个 CPU 都有一个模型的副本。在另一种方法中，你将一堆示例沿着具有模型不同部分的处理器进行流水线处理。我比较了它们，这很有趣。

我对这种抽象感到非常兴奋，因为我觉得神经网络是正确的抽象。它们可以解决当时没有其他方法可以解决的小型玩具问题。我当时天真地认为 32 个处理器就能够训练出非常棒的神经网络。

但事实证明，我们需要大约 100 万倍的计算量，它们才能真正开始用于实际问题，但从 2008 年底、2009 年、2010 年左右开始，由于摩尔定律，我们开始拥有足够的计算量，实际上使神经网络能够用于实际事物。这有点像我重新开始研究神经网络的时候。

但在那之前，在 2007 年……

**DP**: 对不起，实际上我能问一下这个吗？

**JD**: 哦，当然可以。

**DP**: 首先，与其他学术成果不同，它实际上只有四页，你可以直接阅读它。

**JD**: 它有四页，然后是 30 页的 C 代码。

**DP**: 但它是一个制作精良的成果。请告诉我 2007 年的论文是如何产生的。

**JD**: 哦，是的，所以，我们有一个由 Franz Och 领导的机器翻译研究团队，他可能是一年前加入 Google 的，还有其他一些人。每年他们都会参加 DARPA 的一个竞赛，翻译几种不同的语言到英语，我想是中国到英语和阿拉伯语到英语。

Google 团队提交了一个参赛作品，它的工作方式是你周一收到 500 个句子，你必须在周五提交答案。我看到了这个结果，我们以相当大的优势赢得了比赛，以 Bleu 分数衡量，这是翻译质量的衡量标准。

所以我联系了 Franz，这个获胜团队的负责人。我说，“这太棒了，我们什么时候发布它？”他说，“哦，好吧，我们不能发布这个。它不太实用，因为它需要 12 个小时来翻译一个句子。”我说，“好吧，这似乎很长时间。我们怎么解决这个问题？”

原来他们并没有真正为高吞吐量设计它，显然。它正在一个大型语言模型中进行 100,000 次磁盘搜索，他们对每个要翻译的单词计算统计数据——我不会说“训练”。

显然，进行 100,000 次磁盘搜索并不是很快。但我说，“好吧，让我们深入研究一下。”所以我花了大约两三个月的时间和他们一起设计了一个 N-gram 数据的内存压缩表示。

我们使用——N-gram 基本上是关于每个 N 个单词序列在一个大型语料库中出现的频率的统计数据，所以你基本上有，在这种情况下，我们有 2 万亿个单词。当时大多数 N-gram 模型都使用二元组或三元组，但我们决定使用五元组。

所以，基本上，我们尽可能多地处理了当时网络上的所有内容，记录每个五字序列出现的频率。然后你有一个数据结构，上面写着，“好吧，‘我真的很喜欢这家餐厅’在网络上出现了 17 次，或者类似的东西。

所以我构建了一个数据结构，可以让你将所有这些都存储在 200 台机器的内存中，然后有一个批处理 API，你可以说，“这是我在这一轮需要为这个单词查找的 100,000 个东西”，我们会并行地把它们全部给你。这使我们能够从需要一个晚上翻译一个句子到基本上在 100 毫秒内完成一些事情。

**DP**: 有一个 Jeff Dean 事实的列表，就像 Chuck Norris 的事实一样。例如，“对于 Jeff Dean 来说，NP 等于‘没问题’。”其中一个，很有趣，因为现在我听你这么说，实际上，这有点道理。其中一个是，“光速是每小时 35 英里，直到 Jeff Dean 决定在一个周末对其进行优化。”从 12 小时到 100 毫秒，我必须计算一下数量级。

**JD**: 所有这些都非常讨人喜欢。它们很有趣。它们就像我的同事们搞砸了的一个愚人节玩笑。

**DP**: 显然，回想起来，你可以通过考虑单词之间的关系来开发整个互联网的潜在表示，这个想法就像：是的，这就是大型语言模型。这就是 Gemini。当时，这只是一个翻译想法，还是你认为这是另一种范式的开始？

**JD**: 我认为一旦我们为翻译构建了它，大型语言模型的服务就开始用于其他事情，比如补全……你开始输入，它会建议什么补全是有意义的。

所以这绝对是 Google 中大量使用语言模型的开始。Noam 在 Google 从事过许多其他工作，比如使用语言模型的拼写纠正系统。

**NS**: 那是 2000 年、2001 年，我想它都在一台机器的内存中。

**JD**: 是的，我想它是一台机器。他在 2001 年构建的拼写纠正系统非常棒。他向整个公司发送了这个演示链接。

我尝试了所有被破坏的拼写，每个由几个单词组成的查询，比如“scrumbled uggs Bundict”——

**NS**: 我记得那个，是的。

**JD**: ——而不是“scrambled eggs benedict”，它每次都正确了。

**NS**: 是的，我想那是语言建模。

**DP**: 但当时，当你们开发这些系统时，你们是否有这种感觉，“看，你让这些东西越来越复杂，不要考虑五个单词，考虑 100 个单词，1000 个单词，那么潜在的表示就是智能”。基本上，这种洞察力是什么时候出现的？

**NS**: 不完全是。我不认为我曾经觉得，好吧，N-gram 模型会——

**JD**: ——席卷世界——

**NS**: ——是的：“成为”人工智能。我想当时，很多人对贝叶斯网络感到兴奋。那似乎令人兴奋。

肯定看到了那些早期的神经语言模型，既有其中的魔力，“好吧，这正在做一些非常酷的事情”，而且它也让我觉得这是世界上最好的问题，因为它非常非常容易陈述：给我一个关于下一个单词的概率分布。此外，那里有几乎无限的训练数据。有网络的文本；你有数万亿个无监督数据的训练示例。

**JD**: 是的，或者自监督。

**NS**: 自监督，是的。

**JD**: 这很好，因为你然后有了正确的答案，然后你可以在除了当前单词之外的所有单词上进行训练，并尝试预测当前单词。这是一种惊人的能力，可以从对世界的观察中学习。

**NS**: 然后它是 AI 完备的。如果你能很好地做到这一点，那么你几乎可以做任何事情。

---

### **0:25:09 - “Holy shit”时刻**

**DP**: 在科学史上，有一个有趣的讨论，关于想法是仅仅存在于空气中，并且大想法具有某种必然性，还是它们是从某种切线方向被提取出来的。在这种情况下，我们以这种非常合乎逻辑的方式来阐述它，这是否意味着基本上，这种……

**NS**: 感觉就像它存在于空气中。肯定有一些，比如神经图灵机，一些关于注意力的想法，比如拥有这些键值存储，这些存储可以在神经网络中用于关注事物。我认为在某种意义上，它存在于空气中，在某种意义上，你需要一些团队去做这件事。

**JD**: 我喜欢把很多想法看作是部分存在于空气中，当你试图解决一个新问题时，你可能会眯着眼睛看几个不同的、也许是独立的研究想法。你从这些想法中获得一些灵感，然后有一些方面没有解决，你需要弄清楚如何解决它。一些已经存在的事物的变形和一些新事物的结合导致了一些新的突破或新的研究成果，这些成果以前并不存在。

**DP**: 有没有一些关键时刻让你印象深刻，你正在研究一个领域，你提出了这个想法，你有一种“天哪，我不敢相信这行得通”的感觉？

**JD**: 我记得的一件事是在 Brain 团队的早期。我们专注于“让我们看看是否可以构建一些基础设施，让我们能够训练非常非常大的神经网络”。当时，我们的数据中心没有 GPU；我们只有 CPU。但我们知道如何让很多 CPU 一起工作。

所以我们构建了一个系统，使我们能够通过模型和数据并行性来训练相当大的神经网络。我们有一个用于在 1000 万个随机选择的 YouTube 帧上进行无监督学习的系统。它是一个空间局部表示，所以它会根据尝试从高级表示中重建事物来构建无监督表示。

我们让它工作并在 2000 台计算机上使用 16,000 个核心进行训练。过了一会儿，该模型实际上能够在最高级别构建一个表示，其中一个神经元会被猫的图像激发。它从未被告知猫是什么，但它在训练数据中看到了足够多的正面面部视图的猫的例子，以至于该神经元会为此打开，而不是为其他东西打开。

同样，你会有其他的神经元用于人脸和行人的背部，以及类似的东西。这很酷，因为它是从无监督学习原理中构建这些非常高级的表示。然后我们能够在有监督的 ImageNet 20,000 类别挑战赛上获得非常好的结果，相对提高了 60%，这在当时相当不错。

那个神经网络可能比之前训练过的神经网络大 50 倍，并且它得到了很好的结果。所以这有点告诉我，“嘿，实际上扩大神经网络规模似乎，我认为这会是一个好主意，而且它似乎是，所以我们应该继续努力。”

---

### **0:27:28 - AI 实现 Google 的最初使命**

**DP**: 这些例子说明了这些 AI 系统如何适应你刚才提到的：Google 本质上是一家组织信息的公司。在这种情况下，AI 正在寻找信息之间、概念之间的关系，以帮助更快地将想法、你想要的信息传递给你。

现在我们正在使用当前的 AI 模型。显然，你可以在 Google 搜索中使用 BERT，你可以问这些问题。它们仍然擅长信息检索，但更重要的是，它们可以为你编写整个代码库并完成实际工作，这不仅仅是信息检索。

那么你是如何看待这个问题的？如果你们正在构建 AGI，Google 是否仍然是一家信息检索公司？AGI 可以进行信息检索，但它也可以做很多其他的事情。

**JD**: 我认为我们是一家“组织世界信息”的公司，这比信息检索更广泛。也许：“根据你给出的指导组织和创建新信息”。

“你能帮我给我的兽医写一封关于我的狗的信吗？它有这些症状”，它会起草那封信。或者，“你能输入这个视频，你能每隔几分钟生成一个关于视频中发生的事情的摘要吗？”

我认为我们的多模态能力表明它不仅仅是文本。它是关于理解世界上所有不同模态中存在的信息，包括人类的和非人类的，比如自动驾驶汽车上的奇怪的激光雷达传感器，或者基因组信息，或者健康信息。

然后，你如何提取和转换这些信息，使其成为对人们有用的见解，并利用它来帮助他们做他们想做的各种事情？有时是，“我想通过与聊天机器人聊天来获得娱乐。”有时是，“我想要这个非常复杂的问题的答案，没有单一的来源可以检索。”你需要从 100 个网页中提取信息，弄清楚发生了什么，并制作一个有组织的、综合的版本。

然后处理多模态事物或与编码相关的问题。我认为这些模型的能力非常令人兴奋，而且它们正在快速改进，所以我很高兴看到我们的发展方向。

**NS**: 我也很高兴看到我们的发展方向。我认为组织信息显然是一个价值万亿美元的机会，但一万亿美元已经不酷了。酷的是一千万亿美元。

显然，我们的想法不仅仅是积累一大笔钱，而是为世界创造价值，当这些系统能够真正为你做某事、编写你的代码或解决你自己无法解决的问题时，可以创造更多的价值。

为了大规模地做到这一点，我们将必须非常非常灵活和动态，因为我们正在提高这些模型的能力。

**JD**: 是的，我对许多基本研究问题感到非常兴奋，这些问题之所以出现，是因为你看到如果我们尝试这种方法或朝着这个大方向发展，某些事情可能会得到实质性的改进。也许这会奏效，也许不会。

但我也认为，看到我们可以为最终用户实现什么，然后我们如何从那里倒推，以实际构建能够做到这一点的系统，这是有价值的。举个例子：组织信息，这应该意味着世界上的任何信息都应该被任何人使用，无论他们说什么语言。

我认为我们已经做了一些，但这还远不是“无论你说什么语言，在数千种语言中，我们都可以让你获得任何内容并使其可用。任何视频都可以用任何语言观看”的完整愿景。我认为这将非常棒。我们还没有完全做到这一点，但这绝对是我看到的前景，应该是可能的。

---

### **0:32:00 - 在上下文中进行搜索**

**DP**: 谈到你可能尝试的不同架构，我知道你现在正在研究的一件事是更长的上下文。如果你想到 Google 搜索，它有整个互联网的索引作为上下文，但这是一个非常浅的搜索。然后显然语言模型现在有有限的上下文，但它们真的可以思考。这就像黑暗魔法，上下文学习。它可以真正思考它所看到的东西。

你是如何看待将 Google 搜索和上下文学习之类的东西合并起来的？

**JD**: 是的，我先试着回答一下，因为——我已经考虑过这个问题一段时间了。你看到这些模型的一件事是它们非常好，但它们确实会产生幻觉，有时会有事实性问题。部分原因是你在数万亿个 token 上进行了训练，并将所有这些都混合在你数千亿个参数中。

但这都是有点模糊的，因为你已经把所有这些 token 混合在一起了。模型对这些数据有一个相当清晰的看法，但它有时会感到困惑，并会给出错误的日期或其他信息。

而上下文窗口中的信息，在模型的输入中，是非常清晰的，因为我们在 transformers 中有这个非常好的注意力机制。模型可以关注事物，并且它知道它正在处理的确切文本或视频或音频的确切帧或任何东西。

现在，我们的模型可以处理数百万个 token 的上下文，这已经相当多了。这是数百页的 PDF，或 50 篇研究论文，或数小时的视频，或数十小时的音频，或这些东西的某种组合，这很酷。但如果模型能够关注数万亿个 token，那就太好了。

它能否关注整个互联网并为你找到合适的东西？它能否为你关注你所有的个人信息？我想要一个可以访问我所有的电子邮件、所有的文档和所有的照片的模型。

当我让它做某事时，它可以在我的允许下利用这些信息来帮助解决我想让它做的事情。但这将是一个巨大的计算挑战，因为朴素的注意力算法是二次方的。你几乎可以让它在相当多的硬件上工作数百万个 token，但没有希望让它简单地达到数万亿个 token。

所以，我们需要对你真正想要的东西进行大量的算法近似：一种让模型在概念上关注更多 token（数万亿个 token）的方法。也许我们可以把所有的 Google 代码库都放在上下文中，供每个 Google 开发人员使用，把世界上所有的源代码都放在上下文中，供任何开源开发人员使用。那将是惊人的。

**NS**: 这将是难以置信的。模型参数的美妙之处在于它们在记忆事实方面非常节省内存。你可能可以记住每个模型参数大约一个事实或类似的东西。

而如果你在上下文中有一些 token，在每一层都有很多键和值。每个 token 可能有 1KB，1MB 的内存。

**JD**: 你取一个单词，然后把它放大到 10KB 或类似的东西。

**NS**: 是的。实际上，围绕着，A，你如何最小化它？以及 B，你需要拥有哪些单词？有没有更好的方法来访问这些信息？

Jeff 似乎是解决这个问题的合适人选。好吧，从 SRAM 一直到数据中心全球级别的内存层次结构是什么样的？

---

### **0:36:12 - 内部编码模型**

**DP**: 我想更多地谈谈你提到的事情：看，Google 是一家拥有大量代码和大量示例的公司。如果你只考虑这一个用例及其含义，那么你就拥有 Google 可以专有访问的大量代码，即使你只是在内部使用它来提高开发人员的效率和生产力。

**JD**: 需要明确的是，我们实际上已经在使用我们的内部代码库对 Gemini 模型进行了进一步的训练，供我们的内部开发人员使用。但这与关注所有内容不同，因为它有点将代码库混合到一堆参数中。将它放在上下文中可以使事情更清晰。

即使是经过进一步训练的内部模型也非常有用。我认为 Sundar 已经说过，我们现在检入代码库的 25% 的字符是由我们的基于 AI 的编码模型在人工监督下生成的。

---

### **0:37:29 - 2027 年的模型会做什么？**

**DP**: 在未来一两年内，根据你在地平线上看到的能力，你如何想象你自己的个人工作？在 Google 成为一名研究人员会是什么样子？你有一个新想法或其他东西。在你与这些模型交互的方式上，一年后会是什么样子？

**NS**: 我想我们会让这些模型变得更好，并希望能够提高生产力。

**JD**: 是的，除了研究相关的背景，任何时候你看到这些模型被使用，我认为它们都能够提高软件开发人员的生产力，因为它们可以接受一个高级规范或描述你想要完成的事情的句子，并给出一个相当合理的初步结果。从研究的角度来看，也许你可以说，“我真的希望你探索类似于这篇论文中的想法，但也许让我们尝试使其卷积化或其他什么。”

如果你能做到这一点，并且让系统自动生成一堆实验代码，也许你看看它，你会说，“是的，这看起来不错，运行它。”这似乎是一个很好的梦想方向。

在未来一两年内，你可能会在这方面取得很大进展，这似乎是可行的。

**DP**: 这似乎被低估了，因为你实际上可以拥有数百万额外的员工，你可以立即检查他们的输出，员工可以检查彼此的输出，嘿，立即流式传输 token。

**JD**: 对不起，我不是故意低估它。我认为这非常令人兴奋。我只是不喜欢炒作尚未完成的事情。

---

### **0:38:34 - 自主软件工程师**

**DP**: 我确实想更多地讨论这个想法，因为如果你有一个类似于自主软件工程师的东西，这似乎是一件大事，特别是从一个想要构建系统的研究人员的角度来看。好吧，让我们来讨论一下这个想法。作为一个在你们的职业生涯中致力于开发变革性系统的人，这个想法是，你不必像今天 MapReduce 或 Tensorflow 的等价物那样编写代码，只需要说，“这就是我想要的分布式 AI 库的样子。帮我写出来。”

你认为你的生产力可以提高 10 倍吗？100 倍？

**JD**: 我印象很深刻。我想是在 Reddit 上看到的，我们有一个新的实验性编码模型，它在编码和数学等方面要好得多。一个外部人员尝试了它，他们基本上提示它并说，“我希望你实现一个没有外部依赖项的 SQL 处理数据库系统，请用 C 语言实现。”

根据这个人的说法，它实际上做得相当不错。它生成了一个 SQL 解析器、一个分词器、一个查询规划系统和一些用于磁盘上数据的存储格式，实际上能够处理简单的查询。从这个提示（就像一段文字或类似的东西）中，即使是初步的结果也似乎大大提高了软件开发人员的生产力。

我认为你最终可能会得到其他类型的系统，这些系统可能不会尝试在单个半交互式的“40 秒内响应”的情况下完成这项工作，但可能会运行 10 分钟，并可能在 5 分钟后中断你，说，“我已经做了很多，但现在我需要得到一些输入。你关心处理视频还是只处理图像之类的吗？”如果你有很多这样的后台活动正在发生，你似乎需要一些方法来管理工作流程。

---

### **0:40:44 - AI 员工管理界面**

**DP**: 你能多谈谈这个吗？如果你可以启动数百万员工，数十万员工，他们能够以惊人的速度打字，你认为我们需要什么样的界面？这几乎就像你从 1930 年代的票据交易或其他东西转变为现在的 Jane Street 或类似的东西。你需要一些界面来跟踪所有这些正在发生的事情，让 AI 集成到这个大的 monorepo 中并利用它们自己的优势，让人类跟踪正在发生的事情。基本上，三年后 Jeff 或 Noam 的日常工作是什么样的？

**NS**: 它可能与我们现在拥有的类似，因为我们已经将并行化作为一个主要问题。我们有很多非常非常聪明的机器学习研究人员，我们希望他们都能一起工作并构建 AI。

所以实际上，人与人之间的并行化可能类似于机器之间的并行化。我认为这肯定对需要大量探索的事情有好处，比如，“提出下一个突破。”

如果你有一个在 ML 领域肯定会奏效的绝妙想法，那么如果你很聪明，它有 2% 的机会奏效。大多数情况下，这些事情都会失败，但如果你尝试 100 件或 1000 件或 100 万件事情，那么你可能会发现一些惊人的东西。我们有足够的计算能力。像现在顶级的实验室可能拥有比训练 Transformer 所需的计算能力多一百万倍的计算能力。

---

### **0:43:20 - 每天都有新架构？**

**DP**: 是的，实际上，这是一个非常有趣的想法。假设在当今世界，大约有 10,000 名 AI 研究人员在这个社区中提出了突破性的——

**JD**: 可能不止这些。上周 NeurIPS 有 15,000 人参加。

**NS**: 哇。

**DP**: 100,000，我不知道。

**JD**: 是的，也许吧。抱歉。

**DP**: 不，不，有正确的数量级是好的。这个社区每年提出 Transformer 规模的突破的几率是，比如说，10%。现在假设这个社区的规模扩大了 1000 倍，并且在某种意义上，这是一种对更好架构、更好技术的并行搜索。

我们是否会得到——

**JD**: 每天都有突破？

**DP**: ——每年或每天都有突破？

**NS**: 也许吧。听起来可能不错。

**DP**: 但这感觉像 ML 研究吗？如果你能够尝试所有这些实验……

**NS**: 这是一个好问题，因为我不知道人们是否一直在这样做。我们肯定有很多好主意。每个人似乎都想以最大规模运行他们的实验，但我认为这是一个人类问题。

**JD**: 在 1/1000 规模的问题上进行测试，然后在上面审查 100,000 个想法，然后扩大那些看起来有希望的想法，这非常有帮助。

---
### **0:45:43 - 软件与算法改进的反馈循环**

**DP**: 所以，世界可能没有认真对待的一件事：人们意识到制作一个大 100 倍的模型在指数上更难。需要多 100 倍的计算量，对吧？所以人们担心从 Gemini 2 到 3 等等，这是一个指数级更难的问题。

但也许人们没有意识到另一个趋势，即 Gemini 3 提出了所有这些不同的架构思想，尝试它们，你看到了什么有效，你不断提出算法进步，使下一个模型的训练更容易。你能把这个反馈循环进行到什么程度？

**JD**: 我认为人们应该意识到的一件事是，这些模型从一代到另一代的改进通常部分是由硬件和更大规模驱动的，但同样甚至更重要的是由主要的算法改进和模型架构、训练数据组合等方面的重大变化驱动的，这确实使模型在应用于模型的每个 flop 上都更好，所以我认为这是一个很好的认识。然后，我认为如果我们有自动化的想法探索，我们将能够审查更多的想法，并将它们纳入这些模型的下一代的实际生产训练中。

这将非常有帮助，因为这有点像我们目前正在与许多杰出的机器学习研究人员一起做的事情：查看大量的想法，筛选那些在小规模上看起来效果很好的想法，看看它们在中等规模上是否效果很好，将它们带入更大规模的实验中，然后决定在最终的模型配方中添加一堆新的有趣的东西。如果我们能够通过那些机器学习研究人员只是轻轻地引导一个更自动化的搜索过程，而不是亲手照看大量的实验，来使这个过程快 100 倍，那将是非常非常好的。

**NS**: 唯一没有加快的是最大规模的实验。你仍然会做这些 N = 1 的实验。实际上，你只是试图把一堆聪明人放在房间里，让他们盯着这个东西，并弄清楚为什么这有效，为什么这无效。

**JD**: 为此，更多的硬件是一个很好的解决方案。还有更好的硬件。

**NS**: 是的，我们指望着你。

---

### **0:49:10 - 自动化芯片和智能爆炸**

**DP**: 所以，理论上，有软件，有算法方面的改进，未来的 AI 可以做到。还有你们正在研究的东西。我会让你描述它。

但是，如果你陷入这样一种情况：仅仅从软件层面，你就可以在几周或几个月内制造出越来越好的芯片，而更好的 AI 显然可以做得更好，那么这个反馈循环如何不最终导致 Gemini 3 需要两年时间，然后 Gemini 4——或者同等级别的跳跃现在是六个月，然后是第五级是三个月，然后是一个月？由于软件方面（硬件方面和算法方面）的改进，你可能比你想象的要快得多地达到超人智能。

**JD**: 最近我对我们如何能够大大加快芯片设计过程感到非常兴奋。正如我们之前所说，目前设计芯片的方式大约需要 18 个月的时间，从“我们应该制造一个芯片”到你交给 TSMC，然后 TSMC 花四个月的时间来制造它，然后你把它拿回来并把它放在你的数据中心里。

所以这是一个相当长的周期，而制造时间在其中只占很小的一部分。但是，如果你能让它成为主要部分，这样你就不必用 150 个人花 12 到 18 个月的时间来设计芯片，而是可以用几个人通过一个更加自动化的搜索过程来缩小这个范围，探索芯片的整个设计空间，并从芯片设计过程的各个方面获得关于系统试图在高级别探索的各种选择的反馈，那么我认为你可以获得更多的探索，并更快地设计出你真正想交给晶圆厂的东西。

这将是非常棒的，因为你可以缩短制造时间，你可以通过以正确的方式设计硬件来缩短部署时间，这样你就可以把芯片拿回来，然后把它们插入某个系统。这将实现更多的专业化，这将缩短硬件设计的时间框架，这样你就不必那么远地展望什么样的 ML 算法会很有趣。相反，你只需要看看未来六到九个月，它应该是什么？而不是两年、两年半。

这将非常酷。我确实认为制造时间，如果这是你改进的内循环，你会喜欢……

**DP**: 它有多长？

**JD**: 领先的节点，不幸的是，需要的时间越来越长，因为它们比以前的旧节点有更多的金属层。所以这往往会使它需要三到五个月的时间。

**DP**: 好吧，但训练运行也需要这么长时间，对吧？所以你可以同时进行这两项工作。

**JD**: 可能吧。

**DP**: 好吧，所以我想你不能比三到五个月更快。但是你可以——而且，是的，你正在快速开发新的算法思想。

**NS**: 这可以快速发展。

**JD**: 这可以快速发展，这可以在现有的芯片上运行并探索许多很酷的想法。

**DP**: 所以，这难道不是一种情况，你……我认为人们有点期望，啊，会有一个 sigmoid 函数。再说一次，这也不是肯定的。但就像，这是一种可能性吗？这个想法是，你有一种能力爆炸，在人类智能的尾部非常迅速地发生，它变得越来越聪明，速度越来越快？

**NS**: 很有可能。

**JD**: 是的。我喜欢这样想。现在，我们的模型可以接受一个相当复杂的问题，并可以在模型内部将其分解为一堆步骤，可以拼凑出这些步骤的解决方案，并且通常可以为你提供你所要求的整个问题的解决方案。

但这并不是超级可靠，它擅长将事物分解为 5 到 10 个步骤，而不是 100 到 1000 个步骤。所以，如果你能从，是的，80% 的时间它可以给你一个完美的答案，对于一个 10 步长的问题，到 90% 的时间它可以给你一个完美的答案，对于一个 100 到 1000 步长的子问题，这将是这些模型能力的一个惊人的改进。我们还没有做到这一点，但我认为这是我们渴望达到的目标。

**NS**: 我们不需要为此提供新的硬件，但我们会接受它。

**JD**: 永远不要拒绝新的硬件。

---

### **0:53:07 - 未来推理扩展**

**NS**: 未来不久的一个重要改进领域是推理时计算，在推理时应用更多的计算。我想我喜欢这样描述它：即使是一个巨大的语言模型，即使你每个 token 进行一万亿次操作，这比现在大多数人做的还要多，操作的成本大约是 10 的负 18 次方美元。所以你得到一百万个 token 只需要一美元。

我的意思是，与一个相对便宜的消遣相比：你出去买一本平装书并阅读它，你为每 10,000 个 token 支付 1 美元。与语言模型交谈比阅读平装书便宜 100 倍。

所以这里有很大的空间可以说，好吧，如果我们可以让这个东西更贵但更聪明，因为我们比阅读平装书便宜 100 倍，我们比与客户支持代理交谈便宜 10,000 倍，或者比雇用软件工程师或与你的医生或律师交谈便宜一百万倍或更多。我们能否添加计算并使其更智能？

我认为我们在不久的将来会看到的很多起飞都是这种形式。我们过去一直在利用和改进预训练，以及后训练，这些东西将继续改进。但利用推理时的“更努力地思考”将是一场爆炸。

**JD**: 是的，推理时间的一个方面是，我认为你希望系统主动探索一堆不同的潜在解决方案。也许它自己做一些搜索，得到一些信息，消耗这些信息，并弄清楚，哦，现在我真的很想知道更多关于这件事的信息。所以现在它迭代地探索如何最好地解决你向这个系统提出的高级问题。

我认为有一个旋钮，你可以让模型通过更多的推理时计算给你更好的答案，这似乎我们现在有一些技术可以做到这一点。你把旋钮调得越高，你在计算方面的成本就越高，但答案就越好。

这似乎是一个很好的权衡，因为有时你想非常努力地思考，因为这是一个超级重要的问题。有时你可能不想花费大量的计算来计算“一加一的答案是什么”。也许系统——

**DP**: 不应该决定提出新的集合论公理或其他什么！

**JD**: ——应该决定使用计算器工具而不是一个非常大的语言模型。

**DP**: 有趣。那么，是否存在任何阻碍推理时间的因素，比如你可以线性地扩展推理时计算？或者这基本上是一个已经解决的问题，我们知道如何投入 100 倍的计算量，1000 倍的计算量，并获得相应更好的结果？

**NS**: 我们正在研究算法。所以我相信我们会看到越来越好的解决方案，因为这 10,000 多名研究人员正在努力解决这个问题，其中许多人在 Google。

**JD**: 我认为我们在我们自己的实验工作中确实看到了一些例子，如果你应用更多的推理时计算，答案会比你只应用 10 倍更好，你可以获得比 x 量的推理时计算更好的答案。这似乎有用且重要。

但我认为我们想要的是，当你应用 10 倍时，获得比我们今天得到的更大的答案质量改进。这就是设计新算法，尝试新方法，找出如何最好地花费 10 倍而不是 x 来改进事物。

**DP**: 它看起来更像搜索，还是看起来更像只是在更长的时间内保持线性方向？

**JD**: 我真的很喜欢 Rich Sutton 写的关于 Bitter Lesson 的论文，Bitter Lesson 实际上是一篇很好的一页纸论文，但它的本质是你可以尝试很多方法，但两种非常有效的技术是学习和搜索。

你可以在算法上或计算上应用和扩展它们，你通常会得到比你可以应用于各种问题的任何其他方法更好的结果。

搜索必须是花费更多推理时间解决方案的一部分。也许你探索了几种不同的解决这个问题的方法，但那种方法行不通，但这种方法效果更好。我要更多地探索一下。

---
### **1:02:38 - 已经进行多数据中心运行**

**DP**: 这如何改变你未来数据中心规划等的计划？这种搜索可以在哪里异步完成？它必须是在线、离线吗？这如何改变你需要的校园规模和类似的考虑因素？

**JD**: 我们已经在做了。我们支持多数据中心训练。我认为在 Gemini 1.5 技术报告中，我们说我们使用了多个都市区域，并在每个地方使用了一些计算进行训练。然后在这些数据中心之间有一个相当长的延迟但高带宽的连接，这很好。

训练很有趣，因为训练过程中的每一步通常，对于一个大型模型，通常需要几秒钟或类似的时间。所以，它 50 毫秒的延迟并不重要。

**NS**: 只是带宽。

**JD**: 是的，只是带宽。

**NS**: 只要你能在进行一步的时间内同步不同数据中心的所有模型参数，然后累积所有梯度，你就很好。

**JD**: 然后我们有很多工作，甚至从早期的 Brain 时代开始，当我们使用 CPU 机器并且它们非常慢时。我们需要进行异步训练来帮助扩展，其中每个模型副本都会进行一些本地计算，将梯度更新发送到集中式系统，然后异步应用它们。模型的另一个副本也会做同样的事情。

它会让你的模型参数稍微摆动一下，它会让人们对理论保证感到不舒服，但它实际上似乎在实践中有效。

**NS**: 从异步到同步是如此令人愉快，因为你的实验现在是可复制的，而不是你的结果取决于同一台机器上是否运行了网络爬虫。所以，我很高兴在 TPU pod 上运行。

**JD**: 我喜欢异步。它只是让你扩展得更多。

**NS**: 用这两部 iPhone 和一台 Xbox 或其他什么。

**JD**: 是的，如果我们能给你异步但可重复的结果呢？

**NS**: 哦。

**JD**: 所以，一种方法是有效地记录操作序列，比如哪个梯度更新在何时以及在哪一批数据上发生。你不必在日志或类似的东西中记录实际的梯度更新，但你可以重播该操作日志，以便获得可重复性。然后我想你会很高兴。

**NS**: 可能吧。至少你可以调试发生了什么，但你不一定能够比较两个训练运行。因为，好吧，我改变了一个超参数，但我也——

**JD**: 网络爬虫。

**NS**: ——网络爬虫搞砸了，有很多人同时在流媒体播放超级碗。

**JD**: 促使我们从 CPU 上的异步训练转向完全同步训练的原因是我们拥有这些超快的 TPU 硬件芯片和 pod，它们在 pod 中的芯片之间具有令人难以置信的带宽。然后，为了扩展到这一点之外，我们拥有非常好的数据中心网络，甚至跨都市区域网络，这使我们能够为我们最大的训练运行扩展到多个都市区域的许多 pod。我们可以完全同步地做到这一点。

正如 Noam 所说，只要梯度累积和参数在都市区域之间的通信相对于步长发生得足够快，你就很好了。你真的不在乎。但我认为随着你扩大规模，可能需要比我们现在拥有的更多的异步性，因为我们可以让它工作，我们的 ML 研究人员对我们能够将同步训练推到多远感到非常高兴，因为它是一个更容易理解的心理模型。你只是让你的算法与你作斗争，而不是异步性和算法与你作斗争。

**NS**: 随着你扩大规模，会有更多的事情与你作斗争。这就是扩展的问题，你并不总是知道是什么在与你作斗争。是因为你在某些地方把量化推得太远了吗？还是你的数据？

**JD**: 也许是你的对抗性机器 MUQQ17，它正在设置你所有梯度中的指数的第七位或其他什么。

**NS**: 对。所有这些东西都只会使模型稍微变差，所以你甚至不知道发生了什么。

**JD**: 这实际上是神经网络的一个问题，它们对噪声的容忍度很高。你可以在很多方面把事情设置得有点错误，它们只是想出办法来解决这个问题或学习。

**NS**: 你可以在你的代码中存在错误。大多数情况下，这不会有任何影响。有时它会使你的模型变差。有时它会使你的模型变得更好。然后你发现了一些新的东西，因为你以前从未大规模地尝试过这个错误，因为你没有预算。

---
### **1:08:15 - 大规模调试**

**DP**: 调试或解码实际上是什么样子的？你有这些东西，其中一些使模型更好，一些使模型更差。当你明天上班时，你如何找出最显著的输入是什么？

**NS**: 在小规模上，你做大量的实验。研究的一部分涉及，好吧，我想单独发明这些改进或突破。在这种情况下，你想要一个你可以分叉和破解的漂亮的简单代码库，以及一些基线。

我的梦想是早上醒来，想出一个主意，在一天内把它破解出来，运行一些实验，在一天内得到一些初步结果。就像，好吧，这看起来很有希望，这些东西有效，这些东西无效。

我认为这是非常可行的，因为——

**JD**: 在小规模上。

**NS**: 在小规模上，只要你保持一个漂亮的实验代码库。

**JD**: 也许一个实验需要运行一个小时或两个小时，而不是两周。

**NS**: 这很好。所以有研究的这一部分，然后有一些扩大规模。然后你有集成的部分，你想把所有的改进都堆叠在一起，看看它们是否在大规模上有效，看看它们是否都能协同工作。

**JD**: 对，它们是如何相互作用的？对，你认为它们可能是独立的，但实际上也许在改进我们处理视频数据输入的方式和我们更新模型参数的方式之间存在一些有趣的交互。也许这对视频数据比其他东西的交互更多。

可能会发生各种你可能没有预料到的交互。所以你想运行这些实验，在那里你把一堆东西放在一起，然后定期确保你认为好的所有东西在一起都很好。如果不是，了解它们为什么不能很好地协同工作。

**DP**: 两个问题。一，最终结果是事情不能很好地堆叠在一起的频率是多少？这是一种罕见的事情还是经常发生？

**NS**: 它 50% 的时间都会发生。

**JD**: 是的，我的意思是，我认为大多数事情你甚至不会尝试堆叠，因为最初的实验效果不是很好，或者它显示的结果相对于基线来说不是很有希望。然后你有点把这些东西拿出来，你试着单独扩大它们。

然后你会说，“哦，是的，这些看起来很有希望。”所以我要把它们包含在我现在要捆绑在一起的东西中，并尝试推进并与其他看起来有希望的东西结合起来。然后你运行实验，然后你会说，“哦，好吧，它们并没有真正起作用。让我们试着调试一下原因。”

**NS**: 然后还有权衡，因为你想让你的集成系统尽可能干净，因为复杂性——

**JD**: 代码库方面。

**NS**: ——是的，代码库和算法方面。复杂性会带来伤害，复杂性会使事情变慢，带来更多风险。

然后与此同时，你希望它尽可能好。当然，每个研究人员都希望他的发明进入其中。所以那里肯定存在挑战，但我们一直合作得很好。

---

### **1:12:41 - 快速起飞和超级对齐**

**DP**: 好吧，那么回到整个动态“你发现越来越好的算法改进，模型随着时间的推移变得越来越好”，即使你把硬件部分排除在外。世界是否应该更多地考虑这一点，你们是否应该更多地考虑这一点？

有一种情况是，AI 需要 20 年的时间才能随着时间的推移慢慢变得更好，你可以改进一些东西。如果你搞砸了什么，你可以修复它，这没什么大不了的，对吧？它并没有比你发布的上一个版本好多少。

还有一种情况是，你有一个很大的反馈循环，这意味着 Gemini 4 和 Gemini 5 之间的两年是人类历史上最重要的两年。因为你从一个相当不错的 ML 研究人员变成了超人智能，因为这个反馈循环。如果你认为第二种情况是可行的，这会如何改变你处理这些越来越高的智能水平的方式？

**NS**: 我已经停止清理我的车库了，因为我在等待机器人。所以可能我更倾向于我们会看到很多加速。

**JD**: 是的，我的意思是，我认为理解正在发生的事情和趋势是什么非常重要。我认为现在趋势是模型正在逐代变得更好。我没有看到这在接下来的几代中放缓。

所以这意味着，比如说，未来两到三代的模型将能够……让我们回到将一个简单的任务分解为 10 个子部分并在 80% 的时间里完成它的例子，到可以将一个非常高级的任务分解为 100 或 1000 个部分并在 90% 的时间里完成它的东西。这是模型能力的一个重大飞跃。

所以我认为人们了解该领域的进展非常重要。然后这些模型将被应用于许多不同的领域。我认为确保我们作为一个社会从这些模型可以做的事情中获得最大的好处来改善事物是非常好的。我对教育和医疗保健等领域感到非常兴奋，让所有人都能获得信息。

但我们也意识到它们可能被用于错误信息，它们可能被用于自动黑客攻击计算机系统，我们希望尽可能多地设置保障措施和缓解措施，并了解模型的能力。我认为作为一个整体，Google 对我们应该如何处理这个问题有一个非常好的看法。我们的负责任 AI 原则实际上是一个很好的框架，用于思考如何在不同的上下文和设置中提供越来越好的 AI 系统，同时还要确保我们做正确的事情，确保它们是安全的，不会说有毒的东西等等。

**DP**: 我觉得突出的一点是，如果你放大并观察人类历史的这个时期，如果我们处于这样的世界：看，如果你对 Gemini 3 的后训练做得不好，它可能会产生一些错误信息——但然后你修复了后训练。这是一个严重的错误，但这是一个可以修复的错误，对吧？

**NS**: 对。

**DP**: 然而，如果你有这种反馈循环动态，这是一种可能性，那么导致这种智能爆炸的错误的产物是不对齐的，它没有尝试编写你认为它正在尝试编写的代码，而是优化了其他一些目标。

在这个持续几年（也许更短）的非常快速的过程的另一端，你拥有接近 Jeff Dean 或超越水平，或 Noam Shazeer 或超越水平的东西。然后你有数百万个 Jeff Dean 级别的程序员的副本，总之，这似乎是一个更难恢复的错误。

**NS**: 随着这些系统变得越来越强大，你必须越来越小心。

**JD**: 我想说的一件事是，两端都有极端的观点。有一种观点是，“天哪，这些系统在所有事情上都会比人类好得多，我们将不知所措。”还有一种观点是，“这些系统将是惊人的，我们根本不必担心它们。”

我想我介于两者之间。我是一篇名为“塑造 AI”的论文的合著者，你知道，这两种极端的观点通常有点将我们的角色视为自由放任，就像我们将让 AI 沿着它所走的道路发展。

我认为实际上有一个很好的论点可以说，我们将尝试塑造和引导 AI 在世界上部署的方式，以便它在我们在教育、我提到的一些领域（医疗保健）中想要获取和受益的领域中获得最大的好处。

尽可能地引导它远离——也许通过政策相关的事情，也许通过技术措施和保障措施——远离“计算机会接管并对其可以做的事情拥有无限的控制权”。所以我认为这是一个工程问题：你如何设计安全的系统？

我认为这有点像我们在旧式软件开发中所做的现代等价物。就像如果你看看飞机软件开发，那在如何严格开发安全可靠的系统来完成一项相当危险的任务方面有很好的记录？

**DP**: 那里的困难在于，没有某种反馈循环，737，你把它放在一个盒子里，里面有一些计算，几年后，它出来了 1000 版本。

**NS**: 我认为好消息是分析文本似乎比生成文本更容易。所以我相信语言模型分析语言模型输出并找出什么是问题或危险的能力实际上将成为许多这些控制问题的解决方案。

我们肯定正在研究这些东西。我们在 Google 有一群聪明人在研究这个问题。我认为这只会变得越来越重要，不仅是从“为人们做一些好事”的角度来看，而且从商业的角度来看，很多时候，你能够部署的东西是有限的，基于保持事物安全。

因此，在这方面做得非常好变得非常非常重要。

---

### **1:20:51 - 一百万个邪恶的 Jeff Dean**

**DP**: 是的，显然，我知道你们认真对待这里潜在的收益和成本，这真的非常了不起。我知道你们为此获得了赞誉，但还不够。我认为，你们已经提出了很多不同的应用来使用这些模型来改善你谈到的不同领域。

但是，我确实认为……再次，如果你有这样一种情况，理论上存在某种反馈循环过程，在另一端，你有一个与 Noam Shazeer 一样好，与 Jeff Dean 一样好的模型。

如果有一个邪恶版本的你在四处游荡，假设有一百万个，我认为这真的非常糟糕。这可能比任何其他风险都糟糕得多，也许除了核战争或其他什么。想想看，就像一百万个邪恶的 Jeff Dean 或类似的东西。

**JD**: 我们从哪里获得训练数据？

**DP**: 但是，如果你认为这是某个快速反馈循环过程的可能输出，那么你的计划是什么，好吧，我们有 Gemini 3 或 Gemini 4，我们认为它正在帮助我们更好地训练未来的版本，它正在为我们编写大量的训练代码。从这一点开始，我们只是查看它，验证它。

即使是你谈到的查看这些模型输出的验证器最终也将由你制造的 AI 训练，或者很多代码将由你制造的 AI 编写。在我们让 Gemini 4 帮助我们进行 AI 研究之前，我们真正想确定什么，我们想在它为我们编写 AI 代码之前对它运行这个测试。

**JD**: 我的意思是，我认为让系统探索算法研究想法似乎仍然是人类负责的事情。就像，它正在探索这个空间，然后它会得到一堆结果，我们将做出决定，就像，我们是否要将这个特定的学习算法或对系统的更改纳入核心代码库？

所以我认为你可以设置这样的保障措施，使我们能够获得系统的好处，这些系统可以在人工监督下进行改进或自我改进，而不必让系统在没有任何人查看它在做什么的情况下进行全面的自我改进，对吧？这就是我所说的工程保障措施，你想查看你正在部署的系统的特征，不要部署那些在某些措施和某些方面有害的系统，并且你了解它的能力是什么以及它在某些情况下可能会做什么。所以，你知道，我认为这绝不是一个容易的问题，但我确实认为有可能使这些系统安全。

**NS**: 是的。我的意思是，我认为我们还将使用这些系统来检查自己，检查其他系统。即使作为一个人，识别某事也比生成它更容易。

**JD**: 我想说的一件事是，如果你通过 API 或人们与之交互的用户界面公开模型的功能，我认为你就可以控制如何使用它并在它能做什么上设置一些界限。我认为这是确保它将要做的事情在某种程度上可以被你设定的一套标准所接受的工具之一。

**NS**: 是的。我的意思是，我认为目标是赋予人们权力，但在大多数情况下，我们应该主要让人们用这些系统做有意义的事情，并尽可能少地关闭空间。但是，是的，如果你让某人拿走你的东西并创建一百万个邪恶的软件工程师，那么这并不能赋予人们权力，因为他们会用一百万个邪恶的软件工程师伤害他人。

所以我是反对的。

**JD**: 我也是。我会继续。

---
### **1:24:22 - 在 Google 的有趣时光**

**DP**: 好吧，让我们再谈谈一些有趣的话题。让它轻松一点。在过去的 25 年里，最有趣的时光是什么？你对哪段时间最怀念？

**JD**: 你是说在工作中吗？

**NS**: 是的。在工作中。是的。

**JD**: 我认为在 Google 的最初四五年里，我是少数几个从事搜索、爬取和索引系统工作的人之一，我们的流量增长非常快。我们试图扩大我们的索引大小，并使其每分钟更新一次，而不是每个月更新一次，或者如果出现问题，则每两个月更新一次。

看到我们的系统使用量的增长真的让我个人感到满意。构建一个每天有 20 亿人使用的东西是非常了不起的。

但我也要说同样令人兴奋的是今天与 Gemini 团队的人一起工作。我认为在过去一年半的时间里，我们在这些模型可以做的事情上取得的进展真的很有趣。人们真的很敬业，对我们正在做的事情感到非常兴奋。

我认为模型在处理相当复杂的任务方面变得越来越好。就像如果你向 20 年前使用计算机的人展示这些模型的能力，他们不会相信。甚至五年前，他们也可能不相信。这非常令人满意。

我认为我们将看到这些模型的使用量和对世界的影响出现类似的增长。

**NS**: 是的，我同意你。早期非常有趣。部分原因是认识每个人和社交方面，以及你正在构建数百万甚至数十亿人正在使用的东西。

今天也是一样。我们有整个漂亮的微型厨房区，在那里你可以看到很多人在那里闲逛。我喜欢在现场，与一群优秀的人一起工作，并构建正在帮助数百万到数十亿人的东西。还有什么比这更好的呢？

**DP**: 这个微型厨房是什么？

**JD**: 哦，我们在我们所在的建筑里有一个微型厨房区。它是新的，被称为 Gradient Canopy。它以前被称为 Charleston East，我们决定我们需要一个更令人兴奋的名字，因为它有很多机器学习研究人员和 AI 研究在那里进行。

有一个微型厨房区，我们设置了，通常它只是一个浓缩咖啡机和一堆零食，但这个特别的厨房有很多空间。所以我们在那里设置了大约 50 张桌子，所以人们只是在那里闲逛。这有点吵，因为人们总是在磨豆子和煮浓缩咖啡，但你也会得到很多面对面的想法交流，比如，“哦，我试过那个。你有没有想过在你的想法中尝试这个？”或者，“哦，我们下周要发布这个东西。负载测试怎么样了？”有很多反馈发生。

然后我们有我们的 Gemini 聊天室，供那些不在那个微型厨房的人使用。我们在世界各地都有一个团队，我可能加入了 120 个与 Gemini 相关的聊天室。在这个特定的非常专注的话题中，我们有七个人在研究这个问题，伦敦的同事们分享了令人兴奋的结果。

当你醒来时，你会看到那里发生了什么，或者它是一大群专注于数据的人，那里发生了各种各样的问题。这很有趣。

---

### **1:27:51 - 2030 年的世界计算需求**

**DP**: 我发现你们做出的一些预测非常了不起，你们预测了对计算的需求水平，这在当时并不明显或显而易见。TPU 就是一个著名的例子，或者第一个 TPU 就是一个例子。

如果你今天这样想，你估计，看，我们将拥有这些模型，它们将成为我们服务的基础，我们将不断地对它们进行推理。我们将训练未来的版本。你考虑一下到 2030 年我们需要多少计算量来适应所有这些用例，费米估计会得到什么结果？

**JD**: 是的，我的意思是，我认为你将需要大量的推理。计算是这些有能力的模型的最粗略的、最高级别的视图，因为如果提高其质量的技术之一是扩大你使用的推理计算量，那么突然之间，目前生成一些 token 的一个请求现在变得在计算上密集 50 倍、100 倍或 1000 倍，即使它产生相同数量的输出。

而且你还会看到这些服务的使用量大幅增加，因为世界上并不是每个人都发现了这些基于聊天的对话界面，在那里你可以让它们做各种惊人的事情。世界上可能有 10% 或 20% 的计算机用户发现了这一点。随着这一比例接近 100%，人们更频繁地使用它，这将是另一个数量级或两个数量级的增长。

所以你现在将从那里得到两个数量级，从那里得到两个数量级。模型可能会更大，你将从那里得到另一个数量级或两个数量级。你需要大量的推理计算。所以你需要非常高效的硬件来进行你关心的模型的推理。

**DP**: 在 flops 方面，2030 年全球总推理量？

**NS**: 我认为更多总是会更好。如果你只是考虑一下，好吧，到那时人们会决定将世界 GDP 的多少用于 AI？然后，就像，好吧，AI 系统是什么样子的？

好吧，也许它是一种个人助理式的东西，它在你的眼镜里，可以看到你周围的一切，并且可以访问你所有的数字信息和世界的数字信息。也许就像你是 Joe Biden，你在内阁里有一个耳机，可以实时为你提供关于任何事情的建议，为你解决问题，并给你有用的提示。或者你可以和它交谈，它想分析它在你周围看到的任何东西，以了解它对你可能产生的任何有用的影响。

所以我的意思是，我可以想象，好吧，然后说它就像你的私人助理或你的私人内阁之类的东西，每次你花费 2 倍的计算费用，这个东西就会变得更聪明 5、10 个智商点或类似的东西。好吧，你愿意每天花 10 美元并拥有一个助理，还是每天花 20 美元并拥有一个更聪明的助理？它不仅是生活中的助手，而且是帮助你更好地完成工作的助手，因为现在它使你从一个 10 倍的工程师变成一个 100 倍或 1000 万倍的工程师？

好吧，让我们看看：从第一性原理出发，对吧？所以人们会想把世界 GDP 的一部分花在这个东西上。由于我们有所有这些人工智能工程师致力于改进事物，世界 GDP 几乎肯定会大幅增长，比现在高两个数量级。

到那时，我们可能已经解决了无限能源和碳问题。所以我们应该能够拥有大量的能源。我们应该能够拥有数百万到数十亿个机器人为我们建造数据中心。让我们看看，太阳是多少，10 的 26 次方瓦或类似的东西？

我猜用于帮助每个人的 AI 计算量将是天文数字。

**JD**: 我要补充一点。我不确定我是否完全同意，但这是一个非常有趣的思想实验，可以朝着这个方向发展。即使你只完成了一部分，也肯定会有大量的计算。

这就是为什么拥有一个尽可能便宜的硬件平台来使用这些模型并将它们应用于 Noam 描述的问题非常重要，这样你就可以让每个人都能以某种形式访问它，并尽可能降低访问这些能力的成本。

我认为通过专注于硬件和模型共同设计之类的事情，我们可以使这些东西比现在更有效率，这是可以实现的。

**DP**: Google 未来几年的数据中心建设计划是否足够积极，考虑到你预计的这种需求增长？

**JD**: 我不会评论我们未来的资本支出，因为我们的 CEO 和 CFO 可能不希望我这样做。但我会说，你可以看看我们过去几年的资本支出，看到我们肯定在这个领域进行了投资，因为我们认为这很重要。

我们正在继续构建新的、有趣的、创新的硬件，我们认为这确实有助于我们在向越来越多的人部署这些系统方面具有优势，无论是训练它们，还是我们如何让人们能够使用它们进行推理？

---

### **1:34:37 - 回归模块化**

**DP**: 我听说你经常谈论的一件事是持续学习，这个想法是你可以拥有一个随着时间推移而改进的模型，而不是必须从头开始。这是否存在任何根本性的障碍？因为理论上，你应该能够不断地微调一个模型。你认为未来会是什么样子？

**JD**: 是的，我越来越多地思考这个问题。我一直是稀疏模型的忠实粉丝，因为我认为你希望模型的不同部分擅长不同的事情。我们有我们的 Gemini 1.5 Pro 模型，其他模型是混合专家风格的模型，你现在有模型的部分被激活用于某些 token，而部分没有被激活，因为你已经决定这是一个面向数学的东西，这部分擅长数学，这部分擅长理解猫的图像。所以，这让你能够拥有一个更有能力的模型，它在推理时仍然非常高效，因为它有非常大的容量，但你激活了它的一小部分。

但我认为目前的问题，好吧，我们今天所做的事情的一个限制是它仍然是一个非常规则的结构，其中每个专家的大小都相同。路径非常快地合并在一起。它们不会有很多不同的分支，用于数学方面的事情，这些分支不会与猫图像的事情合并在一起。

我认为我们可能应该在这些东西中拥有一个更有机的结构。我也希望模型的各个部分可以稍微独立地开发。就像现在，我认为我们有这个问题，我们要训练一个模型。所以，我们做了大量的准备工作，决定我们可以提出的最棒的算法和我们可以提出的最棒的数据组合。

但那里总是有权衡，比如我们很乐意包含更多的多语言数据，但这可能会以包含更少的编码数据为代价，所以模型在编码方面不太好，但在多语言方面更好，反之亦然。我认为如果我们能够有一小组关心特定语言子集的人去创建非常好的训练数据，训练一个模型的一个模块化部分，然后我们可以将它连接到一个更大的模型，以提高它在东南亚语言或推理 Haskell 代码或类似方面的能力，那就太好了。

然后，你还有一个很好的软件工程优势，你已经分解了问题，相比之下，我们今天所做的是，我们有一大堆人一起工作。但是，我们有一个单一的过程来开始对这个模型进行预训练。

如果我们能做到这一点，你就可以在 Google 周围有 100 个团队。你可以让世界各地的人们努力改进他们关心的语言或他们关心的特定问题，并共同努力改进模型。这是一种持续学习的形式。

**NS**: 那将是如此美好。你可以把模型粘合在一起，或者从模型中取出一些部分，把它们塞进其他……

**JD**: 升级这部分而不扔掉这个东西……

**NS**: ……或者你只需连接一个消防水管，然后你吸取这个模型中的所有信息，把它塞进另一个模型。我的意思是，那里有一个相反的兴趣，那就是科学，就，好吧，我们仍然处于快速进步的时期，所以，如果你想做一些受控实验，好吧，我想把这个东西和那个东西进行比较，因为这有助于我们弄清楚未来要构建什么。出于这种兴趣，通常最好从头开始，这样你就可以比较一个完整的训练运行和另一个完整的训练运行，在实践层面，因为这有助于我们弄清楚未来要构建什么。这不太令人兴奋，但确实会导致快速进步。

**JD**: 是的，我认为可能有办法通过模块化的版本系统获得很多好处。我有一个我的模型的冻结版本，然后我包含了一些特定模块的不同变体，我想比较它的性能或对它进行更多的训练。然后，我将它与这个东西的基线进行比较，现在这个特定的模块（执行 Haskell 解释）的版本是 N'。

**NS**: 实际上，这可能会导致更快的研究进展，对吧？你有一些系统，你做了一些事情来改进它。如果你正在做的改进它的事情相对于从头开始训练系统来说相对便宜，那么它实际上可以使研究更便宜、更快。

**JD**: 是的，而且我认为，在人与人之间更具并行性。

**NS**: 好吧，让我们弄清楚并接下来做这件事。

---
### **1:39:15 - 模块化与蒸馏**
**DP**: 好的，有几个问题。第一，假设Noam又有了新的突破，现在我们有了一个更好的架构。你是否只需要把每个部分都提炼成这个更好的架构？这就是它随着时间的推移不断改进的方式吗？

**JD**: 我确实认为蒸馏是一个非常有用的工具，因为它使你能够将其当前模型架构形式的模型转换为不同的形式。通常，你用它来获取一个非常有能力但又大又笨重的模型，并将其提炼成一个更小的模型，也许你想用非常好的、快速的延迟推理特性来服务。

但我认为你也可以将其视为发生在模块级别的事情。也许会有一个持续的过程，你有每个模块，它有几个不同的表示。它有一个非常大的。它有一个小得多的，它不断地提炼成小版本。

然后是小版本，一旦完成，你就删除大的，然后添加更多的参数容量。现在，开始学习蒸馏的小模型不知道的所有东西，通过在更多的数据上训练它，然后你有点重复这个过程。如果你在模块化模型中有一千个不同的地方在后台运行，这似乎会工作得相当好。

### **1:40:42 - 推理规模与模块调用**

**DP**: 这可能是一种进行推理扩展的方式，就像路由器决定你想要多大的。

**JD**: 是的，你可以有多个版本。哦，这是一个简单的数学问题，所以我要把它路由到非常小的数学蒸馏的东西。哦，这个真的很难，所以...

### **1:40:56 - 模块化与可解释性**

**DP**: 首先，至少从公开研究来看，似乎通常很难解码混合专家类型模型中每个专家在做什么。如果你有这样的东西，你如何强制执行那种对我们可见和可理解的模块化？

**NS**: 实际上，在过去，我发现专家相对容易理解。我的意思是，第一篇混合专家论文，你可以直接查看专家。

**DP**: “我不知道，我只是混合专家的发明者。”

**NS**: 是的——哦，什么？

**JD**: 是的，是的。

**NS**: 是的，你可以看到，好吧，这个专家，就像我们做的，一千、两千个专家。好吧，这个专家，正在获取指代圆柱形物体的单词。

**JD**: 这个非常擅长时间。

**NS**: 是的。

**JD**: 谈论时间。

**NS**: 是的，很容易做到。

并不是说你需要人类理解才能弄清楚如何在运行时使用这个东西，因为你只需要有一些学习的路由器，它正在查看示例。

**JD**: 我想说的一件事是，有很多关于模型的可解释性以及它们内部在做什么的工作。专家级可解释性是这个更广泛领域的一个子问题。我真的很喜欢我的前实习生 Chris Olah 和其他人在 Anthropic 所做的一些工作，他们训练了一个非常稀疏的自动编码器，并且能够推断出一个大型语言模型中的某个特定神经元具有什么特征，所以他们发现了一个金门大桥神经元，当你在谈论金门大桥时，它会被激活。我认为你可以在专家级别做到这一点，你可以在各种不同的级别做到这一点，并获得相当可解释的结果，而且有点不清楚你是否一定需要它。如果模型在某些方面非常出色，我们不一定关心 Gemini 模型中的每个神经元在做什么，只要整个系统的集体输出和特征是好的。这是深度学习的美妙之处之一，你不需要理解或手工设计每一个特征。

---
### **1:43:13 - “巨型MoE”与基础设施**
**DP**: 天哪，这有很多有趣的含义，我可以一直问你这个问题——我会后悔没有问你更多关于这个问题，所以我会继续。一个含义是，目前，如果你有一个具有数百亿个参数的模型，你可以在少数 GPU 上为它提供服务。

在这个系统中，任何一个查询可能只通过总参数的一小部分，但你需要将整个东西加载到内存中，Google 投资的这种特定类型的基础设施，这些 TPU 存在于数百或数千个 pod 中，将非常有价值，对吧？

**NS**: 对于任何类型的甚至是现有的混合专家，你都希望整个东西都在内存中。我想有一种误解在混合专家中流传，好吧，好处是你甚至不必遍历模型中的那些权重。

如果某个专家未被使用，这并不意味着你不必检索该内存，因为，实际上，为了提高效率，你正在以非常大的批处理大小进行服务。

**JD**: 独立请求。

**NS**: 对，独立请求。所以并不是说，好吧，在这一步，你要么查看这个专家，要么不查看这个专家。

因为如果是那样的话，那么当你确实查看专家时，你将以批处理大小 1 运行它，这是非常低效的。就像你拥有现代硬件，操作强度是几百。所以这不是正在发生的事情。你正在查看所有专家，但你只需要通过每个专家发送一小部分批次。

**JD**: 对，但你仍然在每个专家那里有一个较小的批次，然后通过。为了获得合理的平衡，当前模型通常做的一件事是让所有专家的计算成本大致相同，然后你通过它们运行大致相同大小的批次，以便传播你在推理时进行的非常大的批次并具有良好的效率。

但我认为你将来经常可能需要计算成本相差 100 倍或 1000 倍的专家。或者也许是在一种情况下经过许多层的路径，而在另一种情况下是单层甚至跳过连接的路径。在那里，我认为你仍然需要非常大的批次，但你将希望在推理时稍微异步地推送事物通过模型，这比训练时更容易。

这是 Pathways 设计支持的部分内容之一。你有这些组件，这些组件可以是可变成本的，你可以说，对于这个特定的例子，我想通过模型的这个子集，对于这个例子，我想通过模型的这个子集，并让系统协调它。

---
### **1:46:39 - “巨型MoE”与公司规模**
**DP**: 这也意味着公司需要达到一定的规模和成熟度才能……现在，任何人都可以训练足够小的模型。但如果最终这是训练未来模型的最佳方式，那么你需要一家基本上可以拥有一个数据中心来服务一个引号，“blob”或模型的公司。所以这在范式上也会是一个有趣的变化。

**NS**: 你肯定至少要有足够的 HBM 来放置你的整个模型。所以根据你的模型的大小，很可能这就是你至少想要的 HBM 量。

**JD**: 这也意味着你不一定需要将你的整个模型足迹增长到数据中心的大小。你可能希望它比这小一点。

然后可能有许多特定专家的复制副本，这些副本被大量使用，这样你就可以获得更好的负载平衡。这个被大量使用，因为我们得到了很多数学问题，而这个是塔希提舞蹈的专家，它很少被调用。

那个，也许你甚至可以分页到 DRAM 而不是把它放在 HBM 中。但是你希望系统根据负载特性来解决所有这些问题。

---
### **1:48:09 - Google整体融入一个模型**

**DP**: 对。现在，语言模型，显然，你输入语言，你得到语言输出。显然，它是多模态的。

但是 Pathways 的博客文章谈到了许多不同的用例，这些用例显然不是这种通过同一模型的自回归性质的。你能想象，基本上，Google 作为一家公司，产品就像 Google 搜索通过这个，Google 图片通过这个，Gmail 通过它？

就像整个服务器只是这个巨大的混合专家，专门化？

**JD**: 你开始通过在 Google 中大量使用 Gemini 模型来看到其中的一些，这些模型不一定是微调的。它们只是针对这个特定产品设置中的这个特定功能的这个特定用例给出了指令。

所以，我肯定看到了更多地共享底层模型在越来越多的服务中的能力。我认为这绝对是一个非常有趣的方向。

**DP**: 是的，我觉得听众可能没有意识到这是关于 AI 发展方向的一个多么有趣的预测。这就像在 2018 年让 Noam 上一个播客，然后说，“是的，所以我认为语言模型会成为一件事。”

就像，如果这是事情的发展方向，这实际上非常有趣。

**JD**: 是的，我认为你可能会看到这可能是一个很大的基础模型。然后你可能需要该模型的自定义版本，其中包含针对可能具有访问限制的不同设置添加的不同模块。

也许我们有一个供 Google 员工使用的内部版本，我们已经在内部数据上训练了一些模块，我们不允许其他人使用这些模块，但我们可以利用它。也许其他公司，你添加了其他对该公司设置有用的模块，并在我们的云 API 中提供服务。

---
### **1:50:09 - 模块化系统的瓶颈**
**DP**: 使这种系统可行的瓶颈是什么？是系统工程吗？是 ML 吗？

**JD**: 这是一种与我们当前的 Gemini 开发非常不同的操作方式。所以，我认为我们将探索这些领域并取得一些进展。

但我们需要真正看到证据表明这是正确的方法，它有很多好处。其中一些好处可能是提高质量，一些可能不那么具体可衡量，比如能够并行开发不同的模块。但这仍然是一个非常令人兴奋的改进，因为我认为这将使我们能够在改进模型在许多不同领域的性能方面取得更快的进展。

**NS**: 数据控制模块化的东西看起来也很酷，因为那样你就可以拥有只为我训练的模型的一部分。它知道我所有的私人数据。

**JD**: 就像你的个人模块会很有用。另一件事可能是你可以在某些设置中使用某些数据，但在其他设置中不能使用。

也许我们有一些 YouTube 数据，这些数据只能在 YouTube 产品表面使用，但不能在其他设置中使用。所以，我们可以有一个针对该特定目的在该数据上训练的模块。

**DP**: 是的。

**NS**: 我们将需要一百万个自动化研究人员来发明所有这些东西。

**JD**: 这将会很棒。

**DP**: 是的，好吧，这东西本身，你构建这个 blob，它告诉你如何让这个 blob 更好。

**JD**: Blob 2.0。或者也许它们甚至不是版本，它就像一个逐渐增长的 blob。

---
### **1:51:56 - 模块化：受生物启发的有机结构**
**DP**: 是的，这非常有趣。好吧，Jeff，从大局出发，激励我一下：为什么这是一个好主意？为什么这是下一个方向？

**JD**: 是的，这种有机的、不那么精心数学构建的机器学习模型的概念已经伴随我一段时间了。我觉得在神经网络的发展中，人工神经元，从生物神经元中获得灵感是一个很好的灵感，并且在深度学习领域为我们提供了很好的服务。

我们已经能够取得很大的进步。但我觉得我们不一定像我们可能的那样多地关注真实大脑所做的其他事情，这并不是说我们应该完全模仿它，因为硅和湿件具有非常不同的特征和优势。但我认为我们可以从中获得更多灵感的一件事是拥有不同的专业部分的概念，模型的大脑区域擅长不同的事情。

我们在混合专家模型中有一些这样的东西，但它仍然非常结构化。我觉得这种更自然的专业知识增长，当你想要更多的专业知识时，你在那里向模型添加一些容量，让它学习更多关于那种东西。

此外，使模型的连接适应硬件的连接是一个好主意。我认为你希望在同一芯片和同一 HBM 中的人工神经元之间建立极其密集的连接，因为这不会花费你太多。但是，你希望与附近的神经元建立较少数量的连接。所以，就像一个芯片之外，你应该有一些连接，然后，像许多芯片之外，你应该有较少数量的连接，在那里你发送一个非常有限的瓶颈式的东西：这部分模型正在学习的最重要的东西，供模型的其他部分使用。甚至在多个 TPU pod 之间，你也希望发送更少的信息，但最显著的表示。然后跨都市区域，你希望发送更少的信息。

**DP**: 是的，然后它会自然地出现。

**JD**: 是的，我希望它能自然地出现。你可以手工指定这些特征，但我认为你不知道这些连接的正确比例是什么，所以你应该让硬件稍微决定一些事情。就像如果你在这里通信，而这些数据总是很早就出现，你应该添加更多的连接，然后它会花费更长的时间并在正确的时间出现。

---
### **1:54:48 - 推理规模与“blob”中的模式**
**DP**: 哦，这是另一个有趣的含义：现在，我们认为 AI 使用的增长是一种水平的——所以，假设你就像，Google 将有多少 AI 工程师为它工作？你考虑的是一次将有多少个 Gemini 3 实例在工作。

如果你有这个，无论你想怎么称呼它，这个 blob，它可以有机地决定激活自己的多少，那么它更多的是，如果你想要 10 个工程师的产出，它只是激活一个不同的模式或一个更大的模式。如果你想要 100 个工程师的产出，这不像调用更多的代理或更多的实例，它只是调用不同的子模式。

**JD**: 我认为有一个概念是你想要在这个特定的推理上花费多少计算，这应该根据非常简单的事情和非常困难的事情而变化 10,000 倍，甚至可能是一百万倍。它可能是迭代的，你可能通过模型进行一次传递，得到一些东西，然后决定你现在需要调用模型的其他部分。

我想说的另一件事是，这听起来部署起来非常复杂，因为它是一个奇怪的、不断发展的东西，可能没有超级优化的方法来在各个部分之间进行通信，但你总是可以从中提炼。如果你说，“这是我真正关心的那种任务，让我从这个巨大的有机事物中提炼出一些我知道可以非常有效地服务的东西，”你可以随时进行这种提炼过程，一天一次，一小时一次。这似乎会很好。

**NS**: 是的，我们需要更好的蒸馏。

**JD**: 是的。

**NS**: 任何发明了惊人的蒸馏技术的人，可以立即从一个巨大的 blob 蒸馏到你的手机上，那将是美妙的。

---
### **1:56:43 - 蒸馏的改进方向**
**DP**: 你如何描述当前蒸馏技术缺少什么？

**NS**: 嗯，我只是希望它能更快地工作。

**JD**: 一个相关的事情是，我觉得我们需要在预训练期间采用有趣的学习技术。我不确定我们是否从我们看到的每个 token 中提取了最大的价值，使用当前的训练目标。也许我们应该对某些 token 更加努力地思考。

当你到达“答案是”时，也许模型应该在训练时做更多的工作，而不是当它到达“the”时。

**NS**: 对。一定有某种方法可以从相同的数据中获得更多，让它向前和向后学习。

**JD**: 以及各种方式。以这种方式隐藏一些东西，以那种方式隐藏一些东西，让它从部分信息中推断。我认为人们已经在视觉模型中这样做了很长时间。你扭曲模型或隐藏它的某些部分，并试图让它从图像的这个上角或左下角猜测出鸟。

这使得任务更难，我觉得对于更多的文本或与编码相关的数据有一个类似的东西，你想强迫模型更努力地工作。你会从中得到更有趣的观察结果。

**NS**: 是的，图像领域的人没有足够的标记数据，所以他们必须发明所有这些东西。

**JD**: 他们发明了——我的意思是，dropout 是在图像上发明的，但我们并没有真正将它用于文本。这是你可以让一个更大规模的模型进行更多学习而不会过拟合的一种方法，只需对世界上的文本数据进行 100 个 epoch，并使用 dropout。

但这在计算上非常昂贵，但这确实意味着我们不会用完。即使人们说，“哦，不，我们快用完文本数据了，”我并不真的相信这一点，因为我认为我们可以从现有的文本数据中获得更多有能力的模型。

**NS**: 我的意思是，一个人已经看到了十亿个 token。

**JD**: 是的，他们在很多事情上都非常出色。

---

### **1:58:54 - 数据效率：人类 vs. LLM**

**DP**: 所以显然人类的数据效率为如何...或者我想是上限，其中一个，也许不是，设定了一个下限。

**JD**: 这是一个有趣的数据点。

**DP**: 是的。所以这里有一个肯定前件，否定后件的事情。一种看待它的方式是，看，LLM 还有很长的路要走，因此我们预测样本效率会有数量级的提高，只要它们能匹配人类。另一种是，也许它们正在做一些明显不同的事情，考虑到数量级的差异。你认为需要什么才能使这些模型像人类一样具有样本效率？

**JD**: 是的，我认为我们应该考虑稍微改变一下训练目标。仅仅从你看到的以前的 token 中预测下一个 token 似乎不是人们学习的方式。我认为这与人们的学习方式有点相关，但不完全是。一个人可能会读完一本书的整章，然后尝试回答后面的问题，这是一种不同的事情。

我也认为我们并没有从视觉数据中学到很多东西。我们正在对视频数据进行一些训练，但我们肯定还没有开始考虑对你可以获得的所有视觉输入进行训练。所以你有我们还没有真正开始训练的视觉数据。

然后我认为我们可以从我们看到的每一位数据中提取更多的信息。我认为人们如此高效地利用样本的原因之一是他们探索世界并在世界中采取行动并观察会发生什么。你可以看到非常小的婴儿捡起东西并放下它们；他们从中学习重力。当你没有发起行动时，这是一件更难学习的事情。

我认为拥有一个可以采取行动作为其学习过程的一部分的模型会比仅仅被动地观察一个巨大的数据集要好得多。

**DP**: 那么 Gato 是未来吗？

**JD**: 模型可以观察并采取行动并观察相应结果的东西似乎非常有用。

**NS**: 我的意思是，人们可以从甚至不涉及额外输入的思想实验中学到很多东西。爱因斯坦从思想实验中学到了很多东西，或者像牛顿进入隔离区，一个苹果掉在他头上或类似的事情，并发明了重力。还有像数学家——数学没有任何额外的输入。

国际象棋，好吧，你让这个东西自己下棋，它就擅长下棋了。那是 DeepMind，但它所需要的只是国际象棋的规则。所以实际上你可能可以做很多学习，即使没有外部数据，然后你可以在你关心的领域中做到这一点。当然，有些学习需要外部数据，但也许我们可以让这个东西与自己交谈并使自己更聪明。

---

### **2:02:03 - 开放研究的利弊**

**DP**: 所以这是我的问题。你在过去一个小时里所说的可能只是 AI 的下一个重大范式转变。这是一个非常有价值的见解，可能。Noam，在 2017 年，你发布了 Transformer 论文，其他公司的数百亿美元市值是基于这篇论文的，更不用说 Google 发布的其他所有研究了，你们一直比较慷慨。

回想起来，当你想到泄露这些对你的竞争对手有帮助的信息时，回想起来，你会说，“是的，我们仍然会这样做”，还是你会说，“啊，我们没有意识到 Transformer 有多重要。我们应该把它保密。”你是如何看待这个问题的？

**NS**: 这是一个好问题，因为我认为我们可能确实需要看到机会的大小，这通常反映在其他公司正在做的事情上。而且它不是一个固定的馅饼。当前的世界状况几乎是你能想象到的最远离固定馅饼的情况。

我认为我们将看到 GDP、健康、财富以及你能想到的任何其他东西都有数量级的改善。所以我认为 Transformer 能够传播开来肯定是件好事。

**JD**: 这是变革性的。

**NS**: 哇。谢天谢地，Google 也做得很好。所以现在我们确实发布的东西少了一点。

**JD**: 总是存在这种权衡：我们应该立即发布我们正在做的事情吗？我们应该把它放在研究的下一个阶段，然后把它推广到生产 Gemini 模型中，而不是完全发布它？还是有一些中间点？

例如，在我们 Pixel 相机的计算摄影工作中，我们经常决定开发有趣的新技术，比如能够在低光照情况下进行超级好的夜视，或者其他什么，把它放到产品中，然后在产品发布后发布一篇关于这个系统的真正的研究论文。

不同的技术和发展有不同的处理方式。有些我们认为非常关键的东西我们可能不会发布。有些我们认为非常有趣但对改进我们的产品很重要；我们会把它们放到我们的产品中，然后做出决定：我们发布这个吗？还是我们给出一个轻量级的讨论，但也许不是每一个细节？

其他东西我认为我们公开并尝试推进该领域和社区，因为这是我们所有人从参与中受益的方式。我认为参加像上周 NeurIPS 这样的会议非常棒，有 15,000 人都在分享很多很多很棒的想法。我们像过去一样在那里发表了很多论文，看到这个领域的进步非常令人兴奋。

---

### **2:05:29 - Google 的 AI 历程**

**DP**: 你如何解释……所以显然 Google 在内部很早就有了所有这些见解，包括顶级研究人员。现在 Gemini 2 出来了。我们没有太多机会谈论它，但人们知道这是一个非常棒的模型。

**JD**: 如此好的模型。正如我们在微型厨房周围所说，“如此好的模型，如此好的模型”。

**DP**: 所以它在 LMSYS Chatbot Arena 中名列前茅。所以现在 Google 处于领先地位。但是，你如何解释在几年内提出了所有伟大的见解？尽管如此，其他竞争对手的模型在一段时间内更好。

**JD**: 我们一直在研究语言模型很长时间。Noam 在 2001 年早期在拼写纠正方面的工作，2007 年在翻译、非常大规模的语言模型方面的工作，以及 seq2seq 和 word2vec，以及最近的 Transformers 和 BERT。

像内部 Meena 系统这样的东西实际上是一个基于聊天机器人的系统，旨在让人们参与有趣的对话。实际上，在 ChatGPT 出现之前，我们有一个内部聊天机器人系统，Googlers 可以玩。实际上，在大流行期间，很多 Googlers 会喜欢花时间，你知道，每个人都被锁在家里，所以他们喜欢在午餐时花时间和 Meena 聊天，因为它就像一个很好的午餐伙伴。

我认为我们有点不确定的一件事是，从搜索的角度来看，这些模型会产生很多幻觉，它们在很多时候——或者有时——都不能正确地完成事情，这意味着它们没有它们可能的那样有用，所以我们想让它们变得更好。从搜索的角度来看，你希望 100% 的时间都能得到正确的答案，理想情况下，并且在事实性方面非常高。这些模型还没有达到这个标准。

我认为我们不太确定的是，它们非常有用。哦，它们还有各种各样的安全问题，比如它们可能会说冒犯性的东西，我们必须在这方面努力，并让它达到我们觉得可以发布模型的程度。但我认为我们没有完全意识到的是，对于你不会问搜索引擎的事情，它们是多么有用，对吧？比如，帮我给我的兽医写一张便条，或者，你能把这段文字给我做一个快速的总结吗？我认为这是我们看到人们真正涌向聊天机器人的那种用途，作为惊人的新功能，而不是作为一个纯粹的搜索引擎。

所以我想我们花了一些时间，达到了我们实际上发布了相当有能力的聊天机器人的程度，并且一直在通过 Gemini 模型对它们进行改进。我认为这实际上不是一条糟糕的道路。我们希望更早发布聊天机器人吗？也许吧。但我认为我们有一个非常棒的聊天机器人，它拥有非常棒的 Gemini 模型，这些模型一直在变得更好。这很酷。

---

### **2:09:58 - 坚持到底**

**DP**: 好的，最后一个问题。所以我们讨论了你们在过去 25 年里所做的一些事情，而且有很多不同的领域，对吧？你从搜索和索引开始，到分布式系统，到硬件，到 AI 算法。真的，还有一千个，只需访问他们中的任何一个的 Google Scholar 页面或类似的东西。拥有这种水平的技巧是什么，不仅是职业生涯的持久性，你有很多年的突破，而且还有不同领域的广度，你们俩，按任何顺序，职业生涯持久性和广度的技巧是什么？

**JD**: 我喜欢做的一件事是了解一个全新的、有趣的领域，而做到这一点的最佳方法之一是关注正在发生的事情，与同事交谈，关注正在发表的研究论文，并观察研究领域的演变。

愿意说，“哦，芯片设计。我想知道我们是否可以在这方面使用强化学习。”能够深入到一个新的领域，与那些了解不同领域或 AI 医疗保健或其他什么的人一起工作。我与临床医生一起做了一些工作，了解真正的问题是什么，AI 如何提供帮助？这对这件事没有那么有用，但对这件事非常有用。

获得这些见解，并且通常与一组五六个同事一起工作，他们拥有与你不同的专业知识。这使你们能够共同做一些你们任何人都无法单独做到的事情。然后他们的一些专业知识会影响你，你的一些专业知识会影响他们，现在你拥有了更多的工具，作为一名工程研究人员，你可以去解决下一个问题。

我认为这是在工作中不断学习的美妙之处之一。这是我所珍视的。我真的很喜欢深入研究新事物并看看我们能做些什么。

**NS**: 我想说可能很大的一件事是谦逊，就像我会说我是最谦虚的。但说真的，要说我刚才所做的与我能做的或可以做的相比算不了什么。并且能够在看到更好的东西时立即放弃一个想法，比如你或其他人有更好的想法，你看到你正在考虑的东西，他们正在考虑的东西或完全不同的东西可能会更好。

我认为在某种意义上有一种动力说，“嘿，我刚发明的东西很棒，给我更多的芯片。”特别是如果有很多自上而下的资源分配。但我认为我们也需要激励人们说，“嘿，我正在做的这件事根本不起作用。让我完全放弃它并尝试其他东西。”

我认为 Google Brain 做得很好。我们有非常自下而上的 UBI 芯片分配。

**DP**: 你们有 UBI？

**NS**: 是的，基本上每个人都有一个信用额度，你可以把它们集中起来。

Gemini 主要是自上而下的，这在某种意义上非常好，因为它导致了更多的协作和人们一起工作。你不太可能有五组人都在构建相同的东西或构建可互换的东西。

但另一方面，它确实会导致一些激励说，“嘿，我正在做的事情很棒。”然后，作为一个领导者，你听到数百个小组，一切都是，“所以你应该给他们更多的芯片。”不太有动力说，“嘿，我正在做的事情实际上效果不是很好。让我尝试一些不同的东西。”

所以我想在未来，我们将有一些自上而下，一些自下而上，以激励这两种行为：协作和灵活性。我认为这两件事都会带来很多创新。

**JD**: 我认为阐明你认为我们应该发展的有趣方向也很好。我有一个内部幻灯片，叫做“Go, Jeff, Wacky Ideas”。我认为这些都是更面向产品的东西，比如，“嘿，我认为现在我们有了这些能力，我们可以做这 17 件事。”

我认为这是一件好事，因为有时人们会对这件事感到兴奋，并想开始与你合作其中的一个或多个。我认为这是一个很好的方式来引导我们应该去哪里，而不必命令人们，“我们必须去这里。”

**DP**: 好的，这太棒了。

**JD**: 是的。

**DP**: 谢谢你们。

**JD**: 感谢你抽出时间，很高兴和你聊天。这太棒了。


