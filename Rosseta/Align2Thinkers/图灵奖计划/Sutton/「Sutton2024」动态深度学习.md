
讲座主题：[Dynamic Deep Learning (动态深度学习)](https://www.youtube.com/watch?v=75jr5E4OzEE&list=WL&index=6)
演讲者：Richard Sutton

## 导言

视频内容为强化学习领域的先驱人物，Richard Sutton 教授，所做的关于“动态深度学习 (Dynamic Deep Learning)”的精彩报告。

Sutton 教授是 Keen Technologies 的研究科学家，阿尔伯塔大学计算科学系教授，阿尔伯塔机器智能研究所 (Amii) 的首席科学顾问，同时也是伦敦皇家学会、加拿大皇家学会等多个学术机构的会士。他 1984 年在马萨诸塞大学获得了计算机科学博士学位，是强化学习领域经典教材《Reinforcement Learning: An Introduction》的合著者。

在本次报告中，Sutton 教授将深入探讨一个当前深度学习领域面临的关键挑战：**如何在持续变化的环境中保持学习能力？** 尽管深度学习在许多领域取得了巨大成功，但现有的方法在面对需要持续学习的任务时，往往会表现出“可塑性丧失”的现象，即学习速度逐渐减慢，最终丧失学习新知识的能力。

Sutton 教授将通过一系列精心设计的实验，包括在 ImageNet、CIFAR-100 等经典数据集上的持续学习实验，以及在 MuJoCo 环境下的蚂蚁运动强化学习实验，来系统地展示这一现象，并分析其背后的原因。更重要的是，他将介绍一种名为“持续反向传播 (Continual Backprop)”的创新方法，该方法通过选择性地重新初始化网络中的部分神经元，能够有效地维持网络的可塑性，使其在持续学习环境中保持良好的性能。

本次报告不仅是对现有深度学习方法局限性的深刻反思，更是对未来深度学习发展方向的重要探索。相信 Sutton 教授的分享，将为我们理解和解决持续学习问题提供宝贵的启示。让我们以热烈的掌声欢迎 Sutton 教授！请仔细阅读这份根据对话整理的视频脚本，相信您会对动态深度学习有更深入的理解。


```
Dynamic Deep Learning 讲座纲要

├── 背景与动机：
│   ├── 人工智能研究的宏大目标：理解并创造超越人类的智能。
│   ├── 这一目标对经济、社会的影响。
│   ├── 大型语言模型 (LLMs) 的局限性：缺乏对输入流的预测和控制。
│   ├── 强化学习是通往智能体的关键。
│   └── 当前深度学习方法在持续学习方面的不足：部署后停止学习，成为“瞬时学习”系统。
├── 强化学习的标准架构（题外话）：
│   ├── 四个主要部分：
│   │   ├── 感知 (Perception)：事件流 → 状态表示（特征向量）。
│   │   ├── 策略 (Policy)：基于状态选择动作。
│   │   ├── 价值函数 (Value Function)：评估状态好坏，通过TD误差调整策略。
│   │   └── 转换模型 (Transition Model)：模拟世界的动态变化，用于规划。
│   └── 智能体通过多个策略和价值函数学习子任务和技能。
├── 主要信息：
│   ├── 深度学习在持续学习中“失效”：学习速度减慢，丧失可塑性 (loss of plasticity)。
│   ├── 标准人工神经网络方法（不含 replay buffers）针对非持续学习设计。
│   └── 针对持续学习的更好算法是存在的，需要主动寻找。
├── 实验演示（Loss of Plasticity）：
│    ├── 实验一：持续 ImageNet (Continual ImageNet)
│    │   ├── 数据集修改：每次呈现一对类别供网络学习区分。
│    │   ├── 网络结构：每次学习新类别对时，替换最后两层输出神经元。
│    │   ├── 结果：标准反向传播 (Backprop) 在持续学习后性能显著下降，低于线性网络。
        │    ├── 原因分析：初始阶段学到的特征可能有用，但随着任务增多，网络丧失可塑性。
│    │   └── 改进方法：
│    │       ├── L2 正则化：鼓励权重保持较小。
│    │       ├── Shrink and Perturb：L2 正则化 + 随机权重扰动。
│    │       └── 持续反向传播 (Continual Backprop)：选择性重新初始化最不“有用”的单元。
│    │           ├── 效用 (Utility)：单元对网络行为的贡献程度。
│    │           └── 重新初始化细节：输入权重随机，输出权重为零，考虑单元“年龄”。
│    ├── 实验二：持续 CIFAR-100 + 残差网络 (ResNet)
│    │   ├── 数据集：CIFAR-100，逐步增加类别数量（5、10、15...）。
│    │   ├── 网络：残差网络（快捷连接）。
│    │   ├── 性能指标：与从头训练相同数量类别的网络比较。
│    │   ├── 结果：
│    │   │   ├── 标准反向传播（即使有 L2）性能显著低于从头训练。
│    │   │   ├── Shrink and Perturb 稍好，但仍有损失。
│    │   │   └── 持续反向传播保持与从头训练相当的性能，初期有优势。
│    │   └── 网络内部观察：
│    │       ├── 标准反向传播 → 大量“休眠”单元 (inactive units)。
│    │       ├── 标准反向传播 → 特征表示多样性 (diversity) 降低。
│    │       └── 持续反向传播保持低休眠单元比例和高特征多样性。
│    └── 实验三：强化学习 - Ant Locomotion
│        ├── 任务：MuJoCo 蚂蚁运动，奖励是向前速度，动作控制 8 个关节。
│        ├── 非平稳环境：周期性改变脚与地面摩擦力。
│        ├── 结果：
│        │   ├── 标准 PPO 算法持续训练后性能下降，行为退化。
│        │   ├── 即使在平稳环境，PPO 仍因策略和 TD 目标变化而性能下降。
│        │   └── L2 正则化和持续反向传播可改善性能。
│        └── 网络内部观察：
│            ├── 标准反向传播 → 休眠单元增加、多样性降低、权重幅度增大。
│            └── L2 和持续反向传播可缓解这些问题。
├── 结论与未来方向：
│   ├── 深度学习网络针对一次性学习优化，持续学习中存在缺陷。
│   ├── 简单修改（如持续反向传播）可使其适应持续学习。
│   ├── 持续反向传播关键：根据效用选择性重新初始化。
│   └── 未来研究方向：
│       ├── 改进持续学习算法。
│       ├── 开发“元学习”算法（学习如何更好地泛化）。
│       └── 动态深度学习的三个层次：权重、步长参数（每个权重独立）、连接结构。
└── 问答环节 (Q&A)：
    ├── 关于橙色线（较小学习率）：仍属“失败”，远低于理想水平。
    ├── Continual Backprop 在没有 L2 的情况：某些情况下表现会变差。
    ├── Continual Learning 相对于普通深度学习的优势：主要针对问题的不同。
    ├── 与基线（baseline）训练比较效率的问题:持续学习的优势在于微调现有模型。
    ├── 增加类别实验中数据量的问题：确认实验中持续学习和从头训练的网络看到的数据总量相同。
    ├── 新初始化的单元的效用：输出权重为零，效用也为零；算法需跟踪单元“年龄”。
    └── 未来深度学习和强化学习的研究重点：可能仍需修改反向传播, 但需要超越反向传播, 探索元学习和动态调整网络结构。
    
```

**Dynamic Deep Learning (动态深度学习)**



大家好，非常荣幸今天能在这里和大家分享一些关于动态深度学习的想法。正如我们所知，人工智能研究的目标非常宏大，我们致力于理解智能的本质，并创造出超越人类智能的实体。这是一个激动人心的目标，一旦实现，将深刻地改变我们的经济和社会结构，甚至可以被视为人类技术进步的又一个里程碑。

当然，人工智能领域目前也存在一些炒作的成分。例如，大型语言模型取得了巨大的成功，这毋庸置疑。但有些人急于宣称它们已经可以比肩甚至超越人类智能，这其实并不利于科学的健康发展。真正的科学态度应该是严谨地分析我们取得了什么成就，还存在哪些不足，并专注于踏实地解决基本的问题。

**背景与动机：**

从我的角度来看，我致力于理解和创造尽可能智能的代理。我个人对智能的定义是：一个代理能够预测和控制其输入流的能力，特别是奖励信号。这个定义可能与某些人对大型语言模型的看法有所不同，因为目前的大型语言模型在运行时，实际上并没有一个需要预测和控制的输入流。我们所追求的智能，是能够主动影响环境，控制自身输入的能力。

我相信，创造出超智能代理，甚至是超智能增强人类，将为世界带来巨大的福祉。当然，这个过程也可能充满挑战，我们正处于一个所谓的“第四次转折”时期，社会变革的速度和深度都在加速。

通往智能代理的道路，我认为是基于强化学习的，而不是目前大家热衷的大型语言模型。当前，阻碍我们实现更宏大强化学习目标的最大瓶颈，是我们所依赖的深度学习方法本身。这些方法在应对持续学习的任务时显得力不从心。今天的讲座，我将重点探讨这个问题：**为什么现有的深度学习方法在动态的、需要持续学习的环境中会失效？**  它们在被部署到机器人或者与人类交互时，往往就停止了学习，变成了一种“瞬时学习”系统。

**强化学习的标准架构（题外话，但与演讲者研究相关）：**

在深入探讨动态深度学习之前，我想先简单回顾一下强化学习的标准架构，这有助于大家理解我研究的背景。一个典型的基于模型的强化学习代理（Model-Based Reinforcement Learning Agent, MBR），会接收观测 (observations)、执行动作 (actions)，并接收奖励 (reward)。它主要由四个部分组成：

- **感知 (Perception):**  负责接收来自环境的事件流，并将其转化为对当前情境的表示，通常是一个特征向量，我们称之为“状态 (state)”。

- **策略 (Policy):** 这是一个反应式策略，它接收状态表示作为输入，并根据当前的状态选择要执行的动作。感知和策略这两部分结合起来，就可以构成一个完整的、最基础的代理。

- **价值函数 (Value Function):**  为了让代理具备适应性和智能，我们需要评估当前状态的好坏。价值函数就是用来做这件事的。它评估当前状态的价值，并产生一个时间差分 (Temporal Difference, TD) 误差信号。这个 TD 误差信号被用来调整策略，图中从价值函数到策略的对角线箭头就表示这种调整，它不是信息传递，而是函数的改变。

- **转换模型 (Transition Model):**  一个完整的基于模型的代理还需要学习一个世界的转换模型。这个模型学习世界的状态如何转换。通过观察，代理可以学习到在某个状态下采取某个动作后，会转移到哪个新的状态。有了转换模型，代理就可以进行“假设性推理”，例如“如果我采取这个动作会怎么样？”。它可以利用模型预测未来的状态，并结合价值函数评估未来状态的价值，从而指导策略的改进。这种使用模型而非直接经验来调整策略的过程，我们称之为“规划 (Planning)”。

此外，为了更高级的功能，我们可能不只有一个策略和一个价值函数，而是一组策略和价值函数。这就像图中堆叠在策略和价值函数背后的那些方框所暗示的。这代表我们可以为代理设定子任务，让它学习各种技能，在追求最终奖励的同时，也关注如何实现这些子目标。

这些都是一些背景知识，更详细的内容大家可以参考我列出的论文。现在，让我们回到今天的主题——动态深度学习。

**主要信息：**

今天我主要想传达一个核心信息：**目前的深度学习方法在持续学习方面是“失效”的。**  这里的“失效”指的是学习速度会显著降低，最终几乎完全丧失学习能力，也就是我所说的“可塑性丧失 (loss of plasticity)”。  我这里说的深度学习，指的是所有标准的、针对非持续或瞬时学习而优化的传统人工神经网络方法，并且**不包括使用 replay buffers 的方法**，因为我认为 replay buffers 本身就承认了传统深度学习在持续学习方面的不足。

但是，**专门为持续学习设计的、更优秀的学习算法是存在的，而且它们可以应用于深度学习网络。**  我认为问题不在于神经网络本身，而在于我们目前使用的学习算法。我们需要开始寻找更好的算法，而这正是我们研究的重点。

我的主要观点是，深度学习在持续学习中会丧失可塑性。接下来，我将通过一系列实验演示来证实这个观点，并展示我们如何尝试维持可塑性。这些实验主要在经典的监督学习问题上进行，例如 ImageNet 和 CIFAR-100，使用的网络架构是残差网络 (ResNet)。虽然我个人更关注强化学习，但监督学习的实验结果同样具有重要意义，因为我之前提到的强化学习代理的四个组成部分都需要持续学习。我们需要在不断变化的世界中持续适应，对世界进行建模，而世界本身是无限复杂和不断变化的，我们需要持续学习才能应对。

这项研究的成果最近发表在《自然》杂志上，这让我感到非常兴奋。但这篇论文的发表过程其实经历了漫长的曲折，长达四年半的时间投稿屡次被拒。这其实是科研常态，尤其是当你的研究成果比较新颖，挑战了现有主流观点时，发表的难度往往会更大。所以，如果你在发表论文时遇到困难，请不要灰心，坚持下去，也许最终会获得认可。

“可塑性 (plasticity)” 指的是学习能力，“可塑性丧失 (loss of plasticity)” 就是指丧失持续学习新知识的能力。在人工智能领域，我们希望所有的模块都能持续学习，因此维持可塑性至关重要。

其实，可塑性丧失的问题并非全新的发现。它与“灾难性遗忘 (catastrophic forgetting)” 现象密切相关。灾难性遗忘指的是深度学习网络在学习新知识时，会迅速遗忘之前学到的知识。而可塑性丧失则更进一步，它指的是网络在持续学习的过程中，逐渐丧失学习新知识的能力，最终变得无法学习任何新东西。这与灾难性遗忘有所不同，可以看作是灾难性遗忘的升级版——“灾难性可塑性丧失 (catastrophic loss of plasticity)”。

早期的神经网络研究，包括联结主义 (connectionism) 文献中，已经出现了一些关于这个问题的暗示。例如，Warm Starting 实验就表明，预先在部分数据上训练过的神经网络，在后续数据上的学习速度反而更慢，这被视为一种失败。此外，Primacy bias, NicAin capacity loss, CLA ly 等现象，也在强化学习中有所体现。

然而，以往的研究大多只是零星的暗示，缺乏系统性的论证。我们这项工作的意义在于，我们对可塑性丧失现象进行了全面而深入的实验验证，排除了各种可能的干扰因素，最终得到了一个相当确凿的结论。

**实验演示（Loss of Plasticity）：**

接下来，我将通过一系列实验来演示深度学习中的可塑性丧失现象，并展示我们为了维持可塑性所做的一些尝试。

**实验一：持续 ImageNet (Continual ImageNet)**

首先，我们来看第一个实验，我们在经典的 ImageNet 数据集上进行了研究。ImageNet 包含数百万张图像，分为 1000 个类别，每个类别至少有 700 张图像。这是一个被广泛使用的图像识别数据集。

为了将其改造成持续学习问题，我们做了一个最小的改动，我们称之为 “持续 ImageNet (Continual ImageNet)”。  对于每个类别，我们将其中的 600 张图像划分为训练集，100 张划分为测试集。然后，我们将类别两两配对，例如第一对可能是“鳄鱼 vs 吉他”。  我们的任务是训练一个神经网络来区分这对类别。

网络每次只面对二分类任务，输出层只需要两个神经元，分别对应“类别A”和“类别B”。 当网络在这个二分类任务上表现良好，即学习完“鳄鱼 vs 吉他”这对类别后，我们会替换掉网络的最后两层输出神经元，因为我们假设之后不会再问关于“鳄鱼和吉他”的问题了。  然后，我们引入第二对类别，例如“游戏手柄 vs 鱼”。网络需要学习区分这对新的类别。以此类推，我们不断地替换输出层，并引入新的类别对。由于 ImageNet 有 1000 个类别，我们可以构建非常多的类别对，理论上可以无限地进行下去。

性能指标方面，我们使用测试集上的准确率 (percent correct)。  每次学习完一对类别后，我们会在预留的测试集上评估网络的性能，然后将多次实验结果进行平均。在多次实验中，我们会随机改变类别的配对方式，以消除由于类别本身难度差异带来的影响。

网络结构方面，我们使用了比较标准的卷积神经网络结构，但为了适应每次只进行二分类任务的情况，网络结构相对较窄。  网络的最后一层只有两个输出神经元。我们使用了 mini-batch 训练、epochs 等常用训练技巧。

一个关键点是，**网络的权重只在最开始的第一个任务之前进行一次标准的随机初始化。**  之后，当面对新的任务时，我们不会重新初始化任何权重。 我们的想法是，当我们学习了第一个任务后，网络学到的一些特征可能对后续任务也有帮助。例如，在第一个任务中学到的特征提取器，可能能够提供更好的特征表示，从而加速后续任务的学习。

我们首先尝试了最基础的反向传播算法，使用了动量优化器 (momentum)、交叉熵损失函数 (cross-entropy loss)、ReLU 激活函数等常用设置。当然，我们也尝试了各种变体，但结果都具有代表性。

现在，我想问问大家，你们认为在持续学习多个任务之后，网络的性能会如何变化？ 你们觉得在第一个任务上的表现会更好，还是在后续的任务上会更好？

*（现场观众互动，有人认为性能会提升）*

有人认为性能会提升，这确实是一个合理的推测。 也许在学习了第一个任务之后，网络能够学到一些通用的特征，这些特征可以帮助它更快地学习后续的任务。这正是我们所期望的理想的学习系统应该有的表现。

好，接下来我给大家展示一下实验结果，首先是训练的初期阶段。  这张图的横轴是任务编号，纵轴是测试集准确率。

图中红色的曲线对应的是一个学习率 (alpha) 相对较高的反向传播算法，这个学习率是经过调整的，目标是在第一个任务上尽可能快地达到最佳性能。可以看到，它在第一个任务上很快就达到了大约 89% 的准确率。 橙色的曲线对应的是一个学习率相对较慢的反向传播算法，它的初始学习速度较慢，但在经过一段时间的训练后，最终可能达到更高的准确率。

图中还有一条蓝色的虚线，代表的是线性基线 (linear baseline)。  这个基线模型没有使用任何深度网络结构，仅仅是将像素作为输入，学习一个从像素到类别的线性映射。 线性基线的性能其实并不完全是随机的，对于二分类问题，随机猜测的准确率是 50%，而线性基线可以达到大约 77% 的准确率。  阴影区域表示线性基线的一个标准误差范围。  线性基线之所以表现不佳，是因为它没有利用深度网络的特征提取能力。

在实验的初期阶段，红线和橙线似乎都在提升，特别是橙线，看起来在不断进步。 这印证了刚才有观众提出的观点，即早期的任务可以帮助网络学习到一些有用的特征，这些特征可以迁移到后续的任务中。

但是，真正的问题是，**网络的性能在长期持续学习之后会如何变化？** 这才是我们真正关心的问题。

这张图展示的是长期持续学习的结果。红线和橙线与之前的图是相同的，只是横轴的任务数量扩展到了更多。

我们先来看红线，它在第一个任务上达到了 89% 的准确率，但实际上，图中的第一个数据点是对前 50 个任务的性能取平均值的结果，因为为了消除曲线的锯齿状波动，我们对最初的 50 个任务的性能进行了平均。  在前 50 个任务的平均性能约为 84%，已经开始出现下降趋势。随着任务数量的增加，性能持续下降，最终甚至低于线性基线的水平。  橙线代表的是学习率较慢的情况，性能也同样下降，最终稳定在一个略高于线性基线的水平。  图中还有一条更慢学习率的曲线，结果也类似，性能同样很差。

这就是我们在持续 ImageNet 实验中观察到的**可塑性灾难性丧失**现象。 无论如何调整超参数，我们都无法避免性能的最终下降。  事实上，我们尝试了各种常见的改进方法，例如 Adam 优化器、Dropout 正则化等等，但结果都只会让性能变得更差。

总结一下，对于标准的反向传播算法，即使我们调整到最佳的超参数，网络的性能也会随着任务数量的增加而持续下降，最终接近甚至低于单层线性网络的水平。 这就是我所说的“可塑性灾难性丧失”。

*（观众提问：为什么学习率较小的橙线最终性能会比红色线好一些？）*

这是一个很好的问题。我们没有特别强调橙色线有多好，因为它的最终性能仍然很差，也属于失败。  关于橙色线为什么比红色线最终性能稍好，我还没有仔细思考过这个问题，也没有和进行这些实验的学生深入讨论过。  也许是因为学习率较慢的网络，能够更充分地利用早期任务学习到的特征，从而在一定程度上延缓可塑性丧失的进程。 这只是我的一个初步猜测，还需要进一步研究。

*（观众提问：为什么在初始的几个任务中，橙色线和棕色线的性能是上升的？）*

是的，正如这位观众所观察到的，橙色线和棕色线在最初的几个任务中确实是上升的。这也印证了之前那位观众的观点，即网络在早期任务中学习到的特征，可以帮助它更好地完成后续的任务。  我们的理解是，网络在早期任务中学习到了一些通用的特征提取器，这些特征在后续任务中仍然有用。 特别是当学习率较慢时，这种特征迁移的效果会更加明显。  学习率慢的网络可能需要更长的时间才能找到好的特征，但一旦找到，这些特征的通用性也可能更高，从而带来跨任务的性能提升。

正如我之前提到的，像 Dropout 这样的改进方法，实际上会使性能更糟。 但也存在一些算法可以部分地解决这个问题。  图中红色的曲线仍然是之前看到的标准反向传播的结果。  我们尝试了一些方法，例如 Shrink and Perturb 和 L2 正则化。L2 正则化鼓励权重保持较小，有助于维持可塑性。  Shrink and Perturb 方法在 L2 正则化的基础上，还对权重进行了随机扰动。  可以看到，这两种方法都比标准的反向传播要好一些，能够部分地缓解可塑性丧失的问题。

持续反向传播 (Continual Backprop) 是我们自己提出的一种算法。它本质上仍然是随机梯度下降 (SGD)，但增加了一个关键的修改：**选择性地重新初始化一部分神经元。**  我们只选择网络中一小部分最不“有用”的神经元进行重新初始化，比例非常小，例如每个样本只重新初始化不到一个单元。  我们设计了一个“效用 (utility)” 指标来衡量神经元的有用程度。 那些效用值最低的神经元，将被选为重新初始化的候选对象。  这个微小的改动，却能够显著地改善持续学习的性能。

*（在线观众提问：如何选择要重新初始化的单元？）*

这是一个关键的问题，稍后我会详细解释如何选择要重新初始化的单元。  关键在于设计一个合理的“效用 (utility)” 指标。 正如我们将看到的，在标准的深度网络中，实际上有很多神经元并没有发挥太大的作用，这就是反向传播算法的一个特点。

**持续反向传播 (Continual Backprop) 的细节：**

持续反向传播仍然是随机梯度下降，但增加了选择性重新初始化的步骤。 重新初始化的频率由一个超参数控制，这个参数控制着每一步重新初始化的单元比例，我们发现这个比例应该非常小。  我们发现，根据效用指标进行选择性重新初始化，比随机重新初始化效果更好。

Shrink and Perturb 方法也使用了随机扰动权重的思想，但这是一种更早期的想法，可以追溯到 20 世纪 60 年代的 Pandemonium 模型。  持续反向传播的关键在于，它将这种思想扩展到了通用的多层神经网络中。

**实验二：持续 CIFAR-100 (Continual CIFAR-100) + 残差网络 (Residual Networks)**

接下来，我们来看第二个实验，我们使用了残差网络 (ResNet)，这是一种更现代的网络架构。残差网络拥有更深的网络层数，例如 ResNet-18 就有 18 层，并且引入了 shortcut connections (快捷连接)，使得网络不再是严格的层状结构。

在数据集方面，我们使用了 CIFAR-100，并修改了问题的设置。  我们不再像之前那样使用类别对，而是**逐步增加类别数量。**  我们从 5 个类别开始，首先训练网络学习这 5 个类别。  然后，我们增加 5 个新的类别，总共变成 10 个类别。  在训练时，我们会同时呈现旧的 5 个类别和新的 5 个类别的数据。  之后，我们再增加 5 个类别，总共 15 个类别，以此类推，直到增加到 100 个类别。

大家可以思考一下，在这种逐步增加类别数量的设置下，后续的任务会比之前的任务更难，因为类别数量在不断增加。 例如，第一个任务只需要区分 5 个类别，而第二个任务需要区分 10 个类别，以此类推。  因此，我们预期整体的准确率会随着任务数量的增加而下降。

为了更清晰地观察性能的下降，我们没有直接比较准确率，而是**比较增量学习与从头开始学习的性能差异。**  具体来说，对于 50 个类别的情况，我们可以有两种训练方式：

1. **从头开始训练 (Train from scratch):**  一次性使用所有 50 个类别的数据进行训练。
2. **增量式学习 (Incremental learning):**  按照 5 个、10 个、15 个... 50 个类别的顺序逐步学习。

我们想知道，增量式学习的方式，相比于一次性学习，是会带来性能的提升还是下降？  我们比较的是在类别数量达到 50 个时，这两种训练方式的性能差异。  纵轴表示的是增量式学习的性能减去从头开始训练的性能，正值表示增量学习更好，负值表示增量学习更差。

实验结果表明，在实验的初期，增量式学习确实带来了一定的性能提升，大约有 2 个百分点的提升，这是一个相当显著的效果。  但随着任务数量的增加，这种优势逐渐消失。  使用标准反向传播算法 (Backprop) 在学习到大约 40-45 个类别后，增量学习的性能就变得与从头开始训练几乎没有差别，之后就开始出现可塑性丧失现象，性能明显低于从头开始训练。  Shrink and Perturb 方法表现稍好，但仍然存在可塑性丧失的问题。  只有持续反向传播 (Continual Backprop) 能够有效地维持性能，在整个学习过程中，增量学习的性能都与从头开始训练相当，并且在初期还保持一定的优势。

*（观众提问：持续反向传播在长期运行中保持了相同的性能，那么它是否比从头开始训练更快地达到这个性能水平？）*

这是一个很好的问题。  我的理解是，我们并没有明确地衡量“更快”这个指标。  你的意思是，当我们学习到比如 50 个类别时，持续反向传播是否比从头开始训练 50 个类别的网络更快地达到相同的性能水平？

在增量式学习的设置下，当我们学习到 50 个类别时，实际上我们已经逐步学习了之前的 5 个、10 个、15 个... 45 个类别，对于大部分类别，网络已经学习了一段时间。  所以，理论上，持续反向传播应该能够更快地达到最终性能。

*（观众追问：我的意思是，在类别数量为 50 的情况下，持续反向传播达到最终性能所需的训练步数，是否比从头开始训练 50 个类别的网络更少？）*

我想我们没有直接比较训练时间，而是比较达到相同性能所需的训练步数。  我们确保在两种训练方式下，网络看到的样本总数是相同的。

为了更深入地理解网络内部发生了什么，我们进一步分析了网络的内部状态。 这张图展示了随着类别数量的增加，网络中“休眠单元 (dormant units)” 的比例。  “休眠单元”指的是那些激活值低于 1% 的单元，也就是几乎不被激活的单元。

在训练的初期，网络权重是随机初始化的，几乎所有的单元都是活跃的。  但随着使用标准反向传播进行训练，休眠单元的比例不断增加，当学习到 100 个类别时，超过一半的单元都变成了休眠单元。  Shrink and Perturb 方法在这方面表现要好得多，休眠单元的比例相对较低。  而持续反向传播几乎没有产生休眠单元。

我们还测量了特征表示的多样性 (diversity)。  多样性指的是网络学习到的特征的独立性。  在深度网络中，有时会出现冗余特征，即不同的神经元学习到了非常相似的特征，这会降低特征表示的效率。  我们使用线性秩 (linear rank) 来衡量特征表示的多样性，并将其归一化到 0 到 1 之间。  实验结果表明，使用标准反向传播，特征表示的多样性会显著下降。  而使用 Shrink and Perturb 和持续反向传播，可以有效地保持特征表示的多样性。

为了验证结论的鲁棒性，我们在不同的网络架构、激活函数（不仅限于 ReLU，还包括其他类型的激活函数）和超参数设置下，都进行了实验。  结果都显示，标准的深度监督学习方法在持续学习环境中会显著丧失可塑性。  L2 正则化可以在一定程度上缓解这个问题，Shrink and Perturb 方法的效果更进一步，而持续反向传播在维持可塑性方面表现最佳。  持续反向传播只有一个超参数，即重新初始化的频率，但我们发现性能对这个超参数并不敏感，即使设置一个很小的重新初始化频率，也能取得很好的效果。

**实验三：强化学习 - Ant Locomotion (蚂蚁运动)**

最后，我们来看一个强化学习的实验，我们选择了蚂蚁运动 (Ant Locomotion) 任务。  大家可能看过这个任务的演示视频。  我们的目标是训练一个虚拟的蚂蚁尽可能快地向前奔跑。  奖励函数被设置为向前运动的速度。  蚂蚁有 8 个关节，我们可以通过控制这些关节的力矩来控制蚂蚁的运动。 这是一个标准的强化学习 benchmark 任务。

在这个实验中，我们考虑一个非平稳环境 (non-stationary environment)，我们周期性地改变蚂蚁脚与地面之间的摩擦力。  每隔 200 万步，我们会随机改变摩擦力的大小，有时地面会很滑，有时会很粘，有时摩擦力适中。  这种环境变化使得任务变得更加具有挑战性，需要智能体能够持续适应环境的变化。

这张图展示了实验结果，横轴是训练步数，纵轴是每个 episode 的平均奖励 (rewards per episode)。  这种锯齿状的曲线，我们称之为 “鳄鱼尾巴图 (alligator graph)”，因为曲线的形状很像鳄鱼的尾巴。  当环境发生变化时，性能会突然下降，然后又逐渐恢复。

首先，我们来看标准的 PPO (Proximal Policy Optimization) 算法。  在训练的初期，PPO 算法表现良好，奖励值不断上升。  但训练到一定程度后，性能开始停滞 (plateau)。  如果我们继续训练，性能甚至会开始下降，出现我之前提到的“行为退化”现象。  原本跑得很快的蚂蚁，可能会变得像视频中展示的那样，原地打转，无法向前移动。  性能实际上会变得比训练初期还要差。  很多人可能也观察到过这种现象，但通常的做法是提前停止训练，避免性能下降。  但一个真正的持续学习系统，应该能够应对这种性能下降，并持续地学习和适应环境变化。

如果我们仔细调整 PPO 算法的超参数，可以获得更好的初始性能，并延缓性能下降的时间，但最终仍然无法避免可塑性丧失的问题。  相比之下，L2 正则化和持续反向传播都能够显著地改善性能，在整个训练过程中都保持了较高的奖励值，有效地解决了可塑性丧失的问题。

*（在线观众提问：如果改变蚂蚁的质量而不是摩擦力，是否也会观察到类似的结果？）*

是的，我们相信改变蚂蚁的质量也会观察到类似的结果。  只要环境发生变化，导致任务的性质发生改变，就可能会诱发可塑性丧失现象。

为了进一步分析原因，我们再次观察了网络内部的状态。  和之前的监督学习实验类似，我们发现使用标准 PPO 算法，休眠单元的比例会不断增加，最终超过一半。  特征表示的多样性也持续降低。  此外，我们还发现，标准 PPO 算法训练的网络，权重幅值 (weight magnitude) 会越来越大。  这意味着网络的权重变得越来越难以改变，这可能是导致可塑性丧失的一个重要原因。  L2 正则化通过限制权重的大小，可能有助于维持可塑性。  持续反向传播也有效地控制了休眠单元的比例、保持了特征多样性，并抑制了权重幅值的增长。

**结论与未来方向：**

通过以上实验，我们可以得出结论：**深度学习网络本质上是为一次性学习而优化的，在持续学习环境中会表现出严重的可塑性丧失问题。**  但通过简单的修改，例如持续反向传播，就可以使深度学习网络有效地进行持续学习。

持续反向传播的关键在于，**根据神经元的效用对其进行排序，并优先保留那些对网络功能至关重要的神经元，而对那些不重要的神经元进行选择性地重新初始化。**

我认为，深度学习在持续学习领域还有巨大的发展潜力。  特别是对于强化学习，持续学习能力至关重要，因为强化学习本身就是一个持续学习的过程。  对于基于模型的强化学习，持续学习能力更是至关重要，因为我们需要不断地学习和更新模型的各个组成部分。

展望未来，我们希望能够开发出更好的持续学习算法。  我个人认为，理想的流式深度学习或动态深度学习应该能够在三个层次上进行自适应调整：

1. **权重 (Weights):**  这是传统的反向传播算法主要关注的层面，调整神经元之间的连接强度。
2. **步长参数 (Step Sizes):**  每个权重都应该有自己独立的步长参数，也就是学习率。  我认为，根据不同的权重分配不同的学习率，可以更好地雕琢网络的泛化能力。  对于那些已经学习得很好的、非常可靠的特征，我们应该使用较小的学习率，甚至停止学习。  而对于那些需要快速适应新知识的特征，我们可以使用较大的学习率。  通过这种方式，我们可以更精细地控制网络的学习过程，提高泛化能力。
3. **连接结构 (Connection Structure):**  除了调整权重和步长参数，我们还应该考虑动态地调整网络自身的连接结构，例如增加或删除神经元和连接。

这三个层面的自适应调整，是我对未来更优秀的深度学习算法的一个初步设想。  当然，这只是一个开始，还有很多问题需要研究和探索。

**问答环节 (Q&A):**

*（观众提问：你认为深度学习和强化学习未来的研究方向是什么？你认为未来的重点会是像这篇论文一样修改反向传播算法，还是会有其他方向？）*

我认为，未来的研究方向肯定不会仅仅局限于修改反向传播算法，而是要超越反向传播。  持续反向传播可以被看作是对反向传播的一种 “加强版”，它仍然延续了反向传播的精神。  但我认为，为了实现更高级的持续学习能力，例如我之前提到的“元学习 (meta-learning)” 和更好的泛化能力，我们需要探索远超反向传播的新方法。  我们希望未来的算法不仅能够维持可塑性，还能够学习如何更好地泛化，学习如何更有效地学习。  持续学习的独特之处在于，它提供了一个学习如何学习的机会。  当我们持续地学习新的任务时，我们可以观察不同的学习策略的效果，并从中学习和改进我们的学习方法。  这是传统的一次性学习无法做到的。  所以，我认为未来的研究方向会更加多元化，不仅会改进现有的反向传播算法，还会探索全新的学习范式和网络结构。

非常感谢大家的参与和提问。 如果大家还有其他问题，欢迎继续交流。