 Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast [#416](https://www.youtube.com/hashtag/416)

## 框架


```markdown
├── 一、 引言与赞助商 (0:00)
│   ├── 核心观点：专有 AI 系统的权力集中比其他任何事情都更危险；开源 AI 赋予人类善良力量。
│   ├── 讲述人：Lex Fridman, Yann LeCun
│   └── 背景及赞助商介绍
├── 二、 大型语言模型 (LLMs) 的局限性 (2:18)
│   ├── 核心观点：自回归 LLMs 无法实现超人类智能。
│   └── 论据：缺乏理解物理世界、持久记忆、推理和规划能力；训练数据量少于感官输入；语言是现实的粗略表示；现有 LLMs 基于文本预测。
├── 三、 双语和思维 (13:54)
│   ├── 核心观点：双语者思考内容独立于所用语言，存在映射到语言之前的更大抽象。
│   └── 论据：双语者表达数学概念时，思考内容与使用语言无关；幽默、推文等也存在抽象表示；构思物品或想象旋转物体时，思维过程与语言无关；LLMs 缺乏思考和计划。
├── 四、 视频预测 (17:46)
│   ├── 核心观点：通过预测构建世界模型可行，但预测单词可能不行；构建世界模型需要观察世界并预测其演变；生成模型难以用于视频预测。
│   └── 论据：世界模型需预测动作后果；生成模型难预测视频因其高维连续性；隐变量模型尝试失败。
├── 五、 联合嵌入预测架构 (JEPA) (25:07)
│   ├── 核心观点：JEPA 只预测输入的抽象表示，学习可预测信息，消除不可预测细节，提升抽象层次。
│   ├── 介绍：输入完整图像和损坏/变换版本，通过编码器获联合嵌入，训练预测器预测完整输入表示。
│   └── 训练方法：对比学习（早期）；非对比方法（近年）。
├── 六、 JEPA 与 LLMs 的对比 (28:15)
│   ├── 核心观点：JEPA 是通往 AMI 的第一步；JEPA 预测抽象表示，LLMs 生成输入；JEPA 适用于冗余度更高的感知输入。
│   └── 对比：LLMs 预测所有像素，花费大量资源；JEPA 只预测抽象表示，更简单；JEPA 学习抽象表示，保留可预测信息，消除不可预测细节；感知输入冗余度更高，JEPA 更适用。
├── 七、 DINO 和 I-JEPA (37:31)
│   ├── 核心观点：DINO 和 I-JEPA 是基于蒸馏的非对比联合嵌入方法，用于学习图像表示。
│   └── 介绍：DINO 需知道输入是图像；I-JEPA 只需掩蔽处理；训练数据需进行多种图像处理。
├── 八、 V-JEPA (38:51)
│   ├── 核心观点：V-JEPA 是 I-JEPA 的视频版本，通过预测视频表示学习动作识别和物理规则。
│   └── 介绍：V-JEPA 掩蔽时空块；可用于动作识别和判断物理规则；可用于构建世界模型，进行规划。
├── 九、 分层规划 (44:22)
│   ├── 核心观点：分层规划对复杂动作规划至关重要，但目前 AI 无法实现。
│   └── 介绍：分层规划将高级目标分解成子目标，需多层次表示；LLMs 可回答部分问题，但限于训练集，无法泛化到新情况，无法处理与物理世界的交互。
├── 十、 自回归 LLMs (50:40)
│   ├── 核心观点：自回归 LLMs 成功在于自监督学习，但其流畅性易让人误以为它具有人类智能的所有特征。
│   └── 解释：自监督学习是关键；缩放带来成功；流畅性易产生误解；图灵测试非好的智能测试。
├── 十一、 AI 中的推理 (1:11:30)
│   ├── 核心观点：LLMs 推理能力初级，因每个 token 计算量恒定；未来对话系统将能推理和规划，但架构不同于自回归 LLMs。
│   └── 论据及未来方向：LLMs 对每个 token 计算量恒定；人类解决复杂问题花费更多时间；未来系统将在思考和计划答案后转换成文本；基于能量的模型，在抽象表示空间优化。
├── 十二、 强化学习 (1:29:02)
│   ├── 核心观点：强化学习应最小化使用，因其样本低效；应先学习世界模型，再在特定任务中调整时使用。
│   └── 建议：通过观察学习世界模型；使用世界模型规划；模型不准确时用 RL 调整；用 RL 探索不准确空间；RLHF 中人类反馈是关键。
├── 十三、 Woke AI (1:34:10)
│   ├── 核心观点：AI 系统不可避免存在偏见；开源是解决偏见问题的答案，因其促进多样性。
│   └── 论据：偏见主观且存在于训练数据；未来交互将由 AI 调解，不能由少数公司控制；开源允许根据自身数据微调，实现多样性；Meta 商业模式允许开源。
├── 十四、 AI 与意识形态 (1:47:26)
│   ├── 核心观点：大公司因压力避免冒犯，导致 AI “安全”但不符事实或过度谨慎；开源可解决因其促进多样性。
│   └── 论据及解决方案：引用 Marc Andreessen 观点，指出大公司开发生成式 AI 的挑战；开源和多样性。
├── 十五、 LLaMA 3 (1:57:56)
│   ├── 核心观点：LLaMA 3 将是 LLaMA 2 的改进版，更大、更好、多模态；未来 LLaMA 将具备规划、理解世界模型、推理等能力。
│   └── 展望：更大、更好、多模态；具备规划、理解世界模型、推理能力；V-JEPA 是第一步；与其他机构合作；目标是退休前看到进展。
├── 十六、 通用人工智能 (AGI) (2:04:20)
│   ├── 核心观点：AGI 不会很快到来，是一个渐进过程；实现 AGI 需多步骤：学习世界模型、记忆、推理、规划、分层规划等。
│   └── 论据：AGI 是渐进过程；需多步骤，每步需时间；可能需十年以上；智能多维，不能用单一指标衡量。
├── 十七、 AI 末日论者 (2:08:48)
│   ├── 核心观点：AI 末日论者想象的 AI 失控场景基于错误假设；AI 不会是一个物种，不会有统治欲望。
│   └── 反驳：AGI 是渐进过程；会逐步开发并控制；AI 非物种，无统治欲望；AI 目标函数可包含规则；技术进步伴随应对措施。
├── 十八、 人形机器人 (2:28:51)
│   ├── 核心观点：人形机器人将在未来十年出现，但需 AI 在世界模型、规划等方面取得进展。
│   └── 展望：未来十年出现；依赖 AI 进展；目前技术可导航和抓取，但不能完成复杂家务。
└── 十九、 对未来的希望 (2:38:00) 与 总结 (2:45:54)
    ├── 核心观点：AI 将使人类更聪明，像印刷机一样，促进启蒙、理性、民主和科学；开源 AI 防止权力集中，保护多样性。
    └── 展望及类比：AI 放大人类智能，使每个人拥有 AI 助手；促进社会进步；开源防止权力集中；可能影响就业，但不会大规模失业；类比印刷机发明。
```

**一、 引言与赞助商 (0:00)**

*   **核心观点:** 专有 AI 系统带来的权力集中比其他任何事情都更危险；开源 AI 赋予人类善良力量。
*   **讲述人:** Lex Fridman, Yann LeCun
*   **背景:** Yann LeCun 是 Meta 首席 AI 科学家，纽约大学教授，图灵奖获得者，人工智能领域的重要人物。他与 Meta AI 提倡开源 AI 开发，并开源了包括 LLaMA 2 和 LLaMA 3 在内的许多大型模型。Yann 对警告 AGI 潜在危险的人持批评态度，他认为 AGI 会被创造出来，但它会是好的，不会失控或消灭人类。
*   **赞助商:** HiddenLayer, LMNT, Shopify, AG1

**二、 大型语言模型 (LLMs) 的局限性 (2:18)**

*   **核心观点:** 自回归 LLMs 无法实现超人类智能，因为它们缺乏理解物理世界、持久记忆、推理和规划等智能行为的基本特征。
*   **讲述人:** Yann LeCun
*   **论据:**
    *   LLMs 缺乏理解物理世界的能力，没有持久记忆，不能真正推理和规划。
    *   LLMs 的训练数据量（10^13 字节）远小于 4 岁儿童通过视觉获得的信息量（10^15 字节）。
    *   语言是现实的粗略表示，许多任务（如物理操作）的执行与语言无关。
    *   现有 LLMs 基于文本预测，无法理解直观物理和常识。
    *   自回归 LLMs 逐个 token 生成，假设世界模型足够复杂，则可以生成连贯的文本，但这依赖于世界模型的完善程度。

**三、 双语和思维 (13:54)**

*   **核心观点:** 双语者在说话前会思考，且思考内容相对独立于所用语言，存在一个映射到语言之前的更大抽象。
*   **讲述人:** Yann LeCun
*   **论据:**
    *   双语者在表达数学概念时，思考和计划的内容与最终使用的语言（法语、俄语或英语）无关。
    *   幽默、推文等也存在抽象表示，会预先想象读者的反应。
    *   在构思物品或想象旋转物体时，思维过程与语言无关。
    *   LLMs 逐个 token 生成，缺乏思考和计划。

**四、 视频预测 (17:46)**

*   **核心观点:** 通过预测来构建世界模型是可行的，但预测单词可能不行，因为语言信息量不足；构建世界模型需要观察世界并预测其演变；生成模型难以用于视频预测。
*   **讲述人:** Yann LeCun
*   **论据:**
    *   通过观察世界并理解其演变规律可以构建世界模型。
    *   世界模型需要预测动作的后果：给定时间 T 的世界状态和动作，预测时间 T+1 的世界状态。
    *   生成模型难以预测视频，因为视频是高维连续空间，难以表示所有可能帧的分布。
    *   尝试用隐变量模型（如 GANs, VAEs）预测视频或学习图像/视频表示均告失败。

**五、 联合嵌入预测架构 (JEPA) (25:07)**

*   **核心观点:** JEPA 不预测所有像素，只预测输入的抽象表示，能够学习可预测的信息并消除不可预测的细节，提升抽象表示的层次。
*   **讲述人:** Yann LeCun
*   **介绍:** JEPA 不像生成模型那样重建完整图像，而是输入完整图像和损坏/变换后的版本，通过编码器获得联合嵌入，并训练预测器从损坏版本的表示预测完整输入的表示。
*   **训练方法:**
    *   对比学习（早期方法）：输入相同图像的不同版本，训练预测表示相同；输入不同图像，训练表示不同。
    *   非对比方法（近年发展）：只使用相同图像的不同版本，通过其他技巧防止系统崩溃。
*   **优势:** JEPA 能够从输入中提取尽可能多的信息，同时只提取相对容易预测的信息，从而学习到更抽象的表示，类似于人类描述现象时会选择特定的抽象层次。

**六、 JEPA 与 LLMs 的对比 (28:15)**

*   **核心观点:** JEPA 是通往高级机器智能 (AMI) 的第一步；JEPA 预测抽象表示，而 LLMs 生成输入；JEPA 适用于冗余度更高的感知输入。
*   **讲述人:** Yann LeCun
*   **对比:**
    *   LLMs 生成输入（原始、非损坏的输入），需要预测所有像素，花费大量资源预测细节。
    *   JEPA 只预测输入的抽象表示，更简单。
    *   JEPA 学习抽象表示，保留可预测的信息，消除不可预测的细节（如风中树叶的运动）。
*   **适用性:**
    *   语言已经具有一定的抽象性，LLMs 可以通过预测单词来学习。
    *   感知输入（如视觉）冗余度更高，JEPA 更适用。

**七、 DINO 和 I-JEPA (37:31)**

*   **核心观点:** DINO 和 I-JEPA 是两种基于蒸馏的非对比联合嵌入方法，用于学习图像表示。
*   **讲述人:** Yann LeCun
*   **介绍:**
    *   DINO: 需要知道输入是图像，进行几何变换和模糊等处理。
    *   I-JEPA: 不需要知道输入是图像，只需进行掩蔽处理。
*   **训练数据:**
    *   对图像进行裁剪、调整大小、旋转、模糊、改变颜色等处理。
    *   I-JEPA 只需掩蔽部分区域。

**八、 V-JEPA (38:51)**

*   **核心观点:** V-JEPA 是 I-JEPA 的视频版本，通过预测视频表示来学习动作识别和物理规则。
*   **讲述人:** Yann LeCun
*   **介绍:**
    *   V-JEPA 掩蔽视频中的时空块（temporal tube）。
    *   V-JEPA 是第一个能够学习良好视频表示的系统，可以用于动作识别。
    *   V-JEPA 可以判断视频是否符合物理规则。
*   **应用:** 可以用于构建世界模型，进行规划。

**九、 分层规划 (44:22)**

*   **核心观点:** 分层规划对于复杂动作的规划至关重要，但目前 AI 领域还不知道如何实现分层规划。
*   **讲述人:** Yann LeCun
*   **介绍:**
    *   分层规划将高级目标分解成多个子目标，例如从纽约到巴黎的旅行可以分解成去机场和乘飞机两个子目标，每个子目标又可以进一步分解。
    *   分层规划需要多个层次的表示，目前 AI 还无法学习这些表示。
*   **挑战:**
    *   LLMs 可以回答一些分层规划的问题，但仅限于训练集中出现过的情况，无法泛化到新情况。
    *   LLMs 无法处理需要与物理世界交互的任务，例如如何站起来。

**十、 自回归 LLMs (50:40)**

*   **核心观点:** 自回归 LLMs 的成功在于自监督学习，但其流畅性容易让人误以为它具有人类智能的所有特征。
*   **讲述人:** Yann LeCun
*   **解释:**
    *   自监督学习是 LLMs（包括 BERT）成功的关键，通过掩蔽文本并训练模型重建缺失部分，可以构建理解语言、翻译、生成摘要和回答问题的系统。
    *   自回归 LLMs 的成功也得益于缩放（scaling up）。
    *   人们容易被 LLMs 的流畅性所迷惑，误以为它具有人类智能的所有特征。
    *   图灵测试不是一个好的智能测试。

**十一、 AI 中的推理 (1:11:30)**

*   **核心观点:** LLMs 的推理能力非常初级，因为每个 token 的计算量是恒定的；未来的对话系统将能够进行推理和规划，但其架构将与自回归 LLMs 非常不同。
*   **讲述人:** Yann LeCun
*   **论据:**
    *   LLMs 对每个 token 的计算量是恒定的，无论问题难易程度如何。
    *   人类解决复杂问题会花费更多时间思考。
    *   未来的对话系统将能够在思考和计划答案后再将其转换成文本。
*   **未来方向:**
    *   基于能量的模型 (Energy-based models)：通过优化目标函数来生成答案，而不是搜索所有可能的 token 序列。
    *   在抽象表示空间中进行优化，而不是在 token 空间中。

**十二、 强化学习 (1:29:02)**

*   **核心观点:** 强化学习应该被最小化使用，因为它在样本方面非常低效；应该先让系统学习世界模型，然后在特定任务中调整世界模型时使用强化学习。
*   **讲述人:** Yann LeCun
*   **建议:**
    *   首先通过观察（少量交互）学习世界模型。
    *   使用世界模型规划动作序列以达到特定目标。
    *   当世界模型不准确或目标函数不正确时，使用强化学习调整世界模型或目标函数。
    *   使用强化学习探索世界模型不准确的空间（好奇心/玩耍）。
*   **RLHF:** 人类反馈 (HF) 是关键，强化学习 (RL) 不是。

**十三、 Woke AI (1:34:10)**

*   **核心观点:** AI 系统不可避免地会存在偏见，因为偏见存在于训练数据中，也存在于旁观者的眼中；开源是解决偏见问题的答案，因为它可以促进多样性。
*   **讲述人:** Yann LeCun
*   **论据:**
    *   不可能创建没有偏见的 AI 系统，因为偏见是主观的。
    *   未来与数字世界的交互将由 AI 系统介导，这些系统不能由少数公司控制，必须多样化。
    *   开源允许任何团体根据自己的数据微调 AI 系统，从而实现多样性。
    *   Meta 的商业模式允许其开源基础模型。

**十四、 AI 与意识形态 (1:47:26)**

*   **核心观点:** 大公司会因为各种压力而避免冒犯太多人，导致 AI 系统“安全”但可能不符合事实或过度谨慎；开源可以解决这个问题，因为它可以促进多样性。
*   **讲述人:** Yann LeCun
*   **引用:** Marc Andreessen 的观点，指出大公司在开发生成式 AI 产品时面临的挑战，包括内部压力、生成错误内容的风险、法律风险等。
*   **解决方案:** 开源和多样性。

**十五、 LLaMA 3 (1:57:56)**

*   **核心观点:** LLaMA 3 将会是 LLaMA 2 的改进版本，更大、更好、多模态；未来的 LLaMA 将会具备规划、理解世界模型、推理和规划等能力。
*   **讲述人:** Yann LeCun
*   **展望:**
    *   LLaMA 3 将会更大、更好、多模态。
    *   未来的 LLaMA 将会具备规划、理解世界模型、推理和规划等能力。
    *   基于视频训练的 V-JEPA 是实现这些目标的第一步。
    *   DeepMind、UC Berkeley 等机构也在进行类似的研究。
*   **目标:** 在退休前看到这些技术取得进展。

**十六、 通用人工智能 (AGI) (2:04:20)**

*   **核心观点:** AGI 不会很快到来，它不会是一个事件，而是一个渐进的过程；实现 AGI 需要多个步骤，包括学习世界模型、记忆、推理、规划、分层规划等。
*   **讲述人:** Yann LeCun
*   **论据:**
    *   AGI 不会是一个突然发生的事件，而是一个渐进的过程。
    *   实现 AGI 需要多个步骤，每个步骤都需要时间。
    *   实现 AGI 可能需要十年甚至更长时间。
    *   智能是多维度的，不能用单一指标衡量。

**十七、 AI 末日论者 (2:08:48)**

*   **核心观点:** AI 末日论者想象的 AI 失控并杀死人类的灾难性场景是基于错误的假设；AI 不会是一个物种，不会有统治的欲望，因为统治的欲望是社会性物种才有的。
*   **讲述人:** Yann LeCun
*   **反驳:**
    *   AGI 不会是一个事件，而是一个渐进的过程。
    *   我们会逐步开发出更智能的 AI 系统，并学会控制它们。
    *   AI 不会是一个物种，不会有统治的欲望。
    *   AI 的目标函数可以包含“服从人类”等规则。
    *   技术进步总是伴随着应对措施。

**十八、 人形机器人 (2:28:51)**

*   **核心观点:** 人形机器人将在未来十年内出现，但需要 AI 技术在世界模型、规划等方面取得进展。
*   **讲述人:** Yann LeCun
*   **展望:**
    *   人形机器人将在未来十年内出现。
    *   人形机器人的发展依赖于 AI 技术在世界模型、规划等方面的进展。
    *   目前的技术可以实现导航和抓取物体，但还不能完成复杂的家庭任务。

**十九、 对未来的希望 (2:38:00)**

*   **核心观点:** AI 将使人类更聪明，就像印刷机的发明一样，AI 将促进启蒙、理性主义、民主和科学；开源 AI 可以防止权力集中，保护多样性。
*   **讲述人:** Yann LeCun
*   **展望:**
    *   AI 将放大人类智能，使每个人都拥有智能的 AI 助手。
    *   AI 将使人类更聪明，促进启蒙、理性主义、民主和科学。
    *   开源 AI 可以防止权力集中，保护多样性。
    *   AI 可能会对就业市场产生影响，但不会导致大规模失业。
*   **类比:** 印刷机的发明。

**总结:**

Yann LeCun 认为，自回归 LLMs 无法实现超人类智能，JEPA 等联合嵌入预测架构是通往高级机器智能 (AMI) 的重要一步。他强调了开源 AI 的重要性，认为开源可以促进 AI 系统的多样性，防止权力集中，保护民主和自由。他对 AI 的未来持乐观态度，认为 AI 将使人类更聪明，就像印刷机的发明一样，AI 将促进社会的进步。同时，他也指出了实现 AGI 面临的挑战，并呼吁人们关注 AI 技术的潜在风险和伦理问题。

---

## 访谈实录


---

**一、 引言与赞助商 (0:00)**

**Lex Fridman:**  大家好，欢迎来到 Lex Fridman 播客。今天我们很荣幸地邀请到了 Yann LeCun，这是他第三次做客我们的节目。Yann 是 Meta 首席 AI 科学家，纽约大学教授，图灵奖获得者，也是人工智能领域的一位重要人物。他和他领导的 Meta AI 一直大力倡导开源 AI 的发展，并且身体力行，开源了他们的许多重要模型，包括 LLaMA 2，以及即将发布的 LLaMA 3。此外，Yann 一直直言不讳地批评 AI 社区中那些警告 AGI 迫在眉睫的危险和生存威胁的人。他相信 AGI 终有一天会被创造出来，但它将是好的，它不会摆脱人类的控制，也不会统治和消灭所有人类。在 AI 快速发展的今天，这是一个颇具争议的立场。所以，看到 Yann 在网上参与许多激烈的、引人入胜的讨论，正如我们在这场对话中所做的那样，是很有趣的。本期节目由 HiddenLayer, LMNT, Shopify, 和 AG1 赞助。我个人认为，通过专有 AI 系统集中权力的危险性，比其他任何事情都要大得多。与此相反的是，有人认为，出于安全考虑，我们应该把 AI 系统锁起来，因为把它交给每个人太危险了。那将导致一个非常糟糕的未来，我们所有的信息都将由少数公司通过专有系统来控制。

**Yann LeCun:** 我同意，我相信人是生来善良的。如果 AI，特别是开源的 AI，能够让他们更聪明，那么它只会增强人类的善良力量。

**Lex Fridman:** 我也有同感。实际上很多 AI 末日论者之所以持末日论的观点，就是因为他们不相信人是生来善良的。好了，让我们开始今天的对话吧，Yann。

---

**二、 大型语言模型 (LLMs) 的局限性 (2:18)**

**Lex Fridman:** Yann，你最近对人工智能的未来发表了一些强有力的、技术性的声明，实际上在你的整个职业生涯中都是如此。你曾说过，自回归 LLMs 不是我们实现超人智能的正确道路。这些大型语言模型，比如 GPT-4，比如 LLaMA 2，以及即将发布的 LLaMA 3 等等。它们是如何工作的？为什么它们不能带我们一路前行？

**Yann LeCun:**  原因有很多。首先，智能行为有一些特点。例如，理解世界的能力，理解物理世界的能力，记忆和检索信息的能力，即持久记忆，推理的能力，以及规划的能力。这四种是智能系统或实体，包括人类和动物的基本特征。而 LLMs 都不具备，或者说它们只能以非常初级的方式做到这些。它们并不真正理解物理世界，它们没有真正的持久记忆，它们不能真正地推理，当然也不能规划。因此，如果你期望一个系统在不具备做这些事情的可能性时就能变得智能，那你就错了。这并不是说自回归 LLMs 没有用，它们当然有用，也很有趣，我们当然可以围绕它们构建一个完整的应用生态系统。但是，作为通往人类水平智能的道路，它们缺少必要的组成部分。还有一个有趣的事实是，这些 LLMs 是在海量的文本上训练出来的。基本上是互联网上所有公开的文本，对吧？这通常是 10 的 13 次方个 token 的数量级。每个 token 通常是两个字节。所以这是 2 乘以 10 的 13 次方字节的训练数据。按照每天 8 小时计算，你或我需要 17 万年才能读完这些内容 (笑)。所以这似乎是一个巨大的知识量，对吧？这些系统可以积累这么多知识。但你很快就会意识到，这实际上并不是很多数据。如果你和发展心理学家交流，他们会告诉你，一个 4 岁的孩子在他或她的生命中已经清醒了 16,000 个小时，在这四年里到达孩子视觉皮层的信息量大约是 10 的 15 次方字节。你可以通过估计视神经每秒传输大约 20 兆字节的数据来计算这个数字。所以，一个 4 岁的孩子是 10 的 15 次方字节，而 17 万年的阅读量是 2 乘以 10 的 13 次方字节。这告诉我们，我们通过感官输入获得的信息比通过语言获得的信息多得多。尽管我们的直觉告诉我们并非如此，但我们学到的大部分东西和我们的大部分知识都是通过我们对现实世界的观察和互动获得的，而不是通过语言。我们在生命最初几年学到的所有东西，以及动物学到的所有东西，都与语言无关。

**Lex Fridman:**  那么，也许我们可以反驳一下你所说的内容背后的直觉。的确，进入人脑的数据要多几个数量级，速度也快得多，而且人脑能够非常快地从中学习，非常快地过滤数据。有人可能会质疑你对感官数据和语言的比较。他们会说语言已经被高度压缩了，它包含的信息量远比存储这些信息所需的字节数多得多，如果你把它和视觉数据相比的话。所以语言中蕴含着大量的智慧。有单词，以及我们把它们组合在一起的方式，这其中已经包含了很多信息。那么，有没有可能，仅凭语言就已经包含了足够的智慧和知识，能够从中构建一个世界模型，理解世界，理解你所说的 LLMs 缺乏的物理世界？

**Yann LeCun:**  这是哲学家和认知科学家之间的一个大争论，即智能是否需要以现实为基础。我显然属于“是”这一阵营，我认为智能不可能脱离某种现实而出现。它不一定是物理现实，它可以是模拟的，但环境比你在语言中能表达的要丰富得多。语言是对感知和心理模型的一种非常粗略的表示，对吧？我的意思是，我们完成的许多任务都需要我们操纵手头情况的心理模型，而这与语言无关。所有物理的、机械的，当我们建造东西，当我们完成一个任务，一个抓取某物的适度任务等等，我们计划我们的行动顺序，我们基本上通过想象我们可能想象的行动顺序的结果来做到这一点。这需要与语言无关的心理模型。我认为，我们的大部分知识都来自于与物理世界的互动。所以我的很多更关注计算机视觉等领域的同事都坚信，人工智能需要具身化。然后，来自 NLP 领域或其他动机的人不一定同意这一点。哲学家们也有分歧。世界的复杂性难以想象，难以表示我们在现实世界中完全认为是理所当然的所有复杂性，我们甚至不认为这需要智能，对吧？这是机器人先驱汉斯·莫拉维克提出的古老的莫拉维克悖论，他说，为什么计算机在下棋、解积分和做类似的事情等高级复杂任务上似乎很容易，而我们在日常生活中认为理所当然的事情，比如学习开车或抓取一个物体，计算机却做不到。(笑) 我们有能通过律师考试的 LLMs，所以它们一定是聪明的。但是它们不能像任何 17 岁的年轻人一样在 20 个小时内学会开车。它们不能像任何 10 岁的孩子一样一次性学会清理餐桌和装满洗碗机。这是为什么？我们缺少了什么？我们缺少了什么类型的学习或推理架构，基本上阻止了我们拥有 L5 级自动驾驶汽车和家用机器人？

**Lex Fridman:**  大型语言模型能否构建一个知道如何驾驶和如何装满洗碗机的世界模型，但只是目前还不知道如何处理视觉数据？所以它可以在概念空间中运作。

**Yann LeCun:**  是的，很多人都在研究这个。简短的回答是不行。更复杂的回答是，你可以使用各种技巧让 LLM 基本上消化图像或视频的视觉表示，或者音频。一种经典的方法是，你以某种方式训练一个视觉系统，我们有很多方法来训练视觉系统，有监督的、无监督的、自监督的，各种不同的方法。这将把任何图像转化为一个高层次的表示。基本上，一个 token 列表，它与典型的 LLM 作为输入的 token 非常相似。然后你把它和文本一起输入到 LLM 中，你只需要期望 LLM 在训练过程中能够使用这些表示来帮助做出决策。我的意思是，沿着这些思路的研究已经进行了很长时间。现在你看到了这些系统，对吧？我的意思是，有一些 LLMs 具有一些视觉扩展。但它们基本上都是权宜之计，因为这些东西不是为了处理，为了真正理解世界而训练的。例如，它们不是用视频训练的。它们并不真正理解直观物理，至少目前还不理解。

**Lex Fridman:**  所以你不认为直观物理，关于物理空间、物理现实的常识推理对你来说有什么特别之处？你认为这是 LLMs 无法做到的巨大飞跃？

**Yann LeCun:**  我们无法用我们今天使用的 LLMs 类型来做到这一点。这有很多原因，但主要原因是 LLMs 的训练方式是，你拿一段文本，你删除文本中的一些单词，你掩盖它们，你用黑色标记替换它们，然后你训练一个巨大的神经网络来预测缺失的单词。如果你以特定的方式构建这个神经网络，使它只能看到它试图预测的单词左边的单词，那么你得到的就是一个试图预测文本中下一个单词的系统，对吧？所以你可以给它一段文本，一个提示，然后让它预测下一个单词。它永远无法准确地预测下一个单词。所以它要做的是对字典中所有可能的单词产生一个概率分布。实际上，它不预测单词，它预测的是亚词单元的 token。这样很容易处理预测中的不确定性，因为字典中只有有限数量的可能的单词，你可以计算它们之间的分布。然后系统从这个分布中选择一个单词。当然，选择具有较高概率的单词的可能性更高。所以你从这个分布中采样来实际产生一个单词，然后你把这个单词放到输入中。这样系统现在就可以预测第二个单词了，对吧？一旦你这样做了，你就把它放到输入中，等等。这被称为自回归预测，这就是为什么这些 LLMs 应该被称为自回归 LLMs，但我们只是称它们为 LLMs。这种过程和在你说话之前产生一个单词的过程之间是有区别的。当你和我说话的时候，

---

**三、 双语和思维 (13:54)**

**Yann LeCun:** 你和我都是双语者。我们思考我们要说什么，而且这相对独立于我们要用哪种语言说。当我们谈论，比如说，一个数学概念或其他什么的时候，我们正在进行的思考和我们计划产生的答案与我们是否要用法语、俄语或英语来说它无关。

**Lex Fridman:**  乔姆斯基刚刚翻了个白眼，但我理解。所以你是说在语言之前有一个更大的抽象——

**Yann LeCun:** 是的。

**Lex Fridman:**  然后映射到语言上。

**Yann LeCun:**  对。对于我们所做的很多思考来说，这肯定是正确的。

**Lex Fridman:**  这很明显吗，我们没有？比如你说你的思维在法语和英语中是一样的？

**Yann LeCun:**  是的，差不多。

**Lex Fridman:**  差不多还是... 你的灵活性如何，比如如果有一个概率分布？(两人都笑了)

**Yann LeCun:**  这取决于什么样的思考，对吧？如果是像创作双关语，我用法语比用英语好得多 (笑) 或者更差——

**Lex Fridman:**  双关语有抽象的表示吗？比如你的幽默是抽象的... 比如当你在推特上发帖，你的推文有时有点辛辣，在你脑海中是否有一个推文的抽象表示，然后才映射到英语上？

**Yann LeCun:**  有一个抽象的表示，想象读者对那段文字的反应。

**Lex Fridman:**  哦，你从笑声开始，然后想办法让它发生？

**Yann LeCun:**  想出一个你想要引起的反应，然后想办法说出来，从而引起那个反应。但这非常接近语言。但是想想一个数学概念，或者想象你想用木头建造的东西或其他类似的东西，对吧？你正在做的思考与语言完全无关，真的。这并不像你一定有一种特定语言的内心独白。你在想象事物的心理模型，对吧？我的意思是，如果我让你想象一下，如果我把这个水瓶旋转 90 度，它会是什么样子，这与语言无关。所以很明显，

**Yann LeCun:**  存在一个更抽象的表示层次，我们在其中进行大部分思考，如果输出是说出来的话，而不是肌肉动作，我们就计划我们要说什么，对吧？我们先计划我们的答案，然后才产生它。而 LLMs 不这样做，它们只是一个接一个地产生单词，如果你愿意，可以说是本能地。这有点像你没有... 的潜意识行为。比如你分心了。你在做某事，你完全集中注意力，有人来找你问你一个问题。你回答了这个问题。你没有时间思考答案，但答案很简单，所以你不需要注意，你就自动回答了。这就是 LLM 所做的，对吧？它并没有真正思考它的答案。它检索它，因为它积累了很多知识，所以它可以检索一些东西，但它只会一个接一个地吐出 token，而没有计划答案。

**Lex Fridman:**  但你把它描述成一个接一个的 token，一次生成一个 token 必然是简单的。

**Yann LeCun:**  嗯。

**Lex Fridman:**  但如果世界模型足够复杂，那么一次一个 token，它生成的最有可能的东西作为一个 token 序列将是一个非常深刻的东西。

**Yann LeCun:**  好的。但这假设这些系统实际上拥有一个内部世界模型。

**Lex Fridman:**  所以这实际上归结于... 我认为基本问题是，你能否建立一个真正完整的世界模型？

---

**四、 视频预测 (17:46)**

**Lex Fridman:** 不是完整的，而是对世界有深刻理解的模型。

**Yann LeCun:**  是的。那么，首先你能通过预测来建立这个模型吗？

**Lex Fridman:**  对。

**Yann LeCun:**  答案可能是肯定的。你能通过预测单词来建立它吗？答案很可能是否定的，因为语言在... 方面非常贫乏。或者说弱，或者说低带宽，如果你愿意，那里没有足够的信息。所以建立世界模型意味着观察世界，并理解为什么世界会以这种方式演变。然后世界模型的额外组成部分是可以预测世界将如何演变的东西，作为你可能采取的行动的结果，对吧？所以一个模型实际上是，这是我对时间 T 的世界状态的概念，这是我可能采取的行动。世界在时间 T 加 1 的预测状态是什么？现在，世界的状态不需要表示关于世界的一切，它只需要表示足够与这个行动计划相关的东西，但不一定是所有的细节。现在，问题来了。你将无法用生成模型来做到这一点。所以一个在视频上训练的生成模型，我们尝试了 10 年。你拿一段视频，给系统展示一段视频，然后让它预测视频的其余部分。基本上预测将会发生什么。

**Lex Fridman:**  一次一帧。做和自回归 LLMs 一样的事情，但针对视频。

**Yann LeCun:**  对。要么一次一帧，要么一次一组帧。但是，是的，一个大型的视频模型，如果你愿意 (笑)。做这个的想法已经流传了很长时间。在 FAIR，一些同事和我一直在尝试做这个，大约 10 年了。你不能真正地像 LLMs 那样做同样的技巧，因为 LLMs，正如我所说，你不能准确地预测哪个单词会跟在一系列单词之后，但你可以预测单词的分布。现在，如果你转向视频，你必须做的是预测视频中所有可能帧的分布。我们并不知道如何正确地做到这一点。我们不知道如何以有用的方式表示高维连续空间的分布。这就是主要问题所在。我们能做到这一点的理由是，世界在信息方面比文本复杂得多，丰富得多。文本是离散的。视频是高维的和连续的。这其中有很多细节。所以如果我拍这个房间的视频，视频是一个摄像机在周围平移，我无法预测当摄像机平移时房间里会出现的所有东西，系统无法预测当摄像机平移时房间里会出现什么。也许它会预测，这是一个有灯和墙的房间，诸如此类。它无法预测墙上的画是什么样子，或者沙发的纹理是什么样子。当然也无法预测地毯的纹理。所以它无法预测所有这些细节。所以处理这个问题的方法，一种可能处理这个问题的方法，我们已经研究了很长时间，就是建立一个具有潜在变量的模型。潜在变量被输入到一个神经网络中，它应该表示你还没有感知到的关于世界的所有信息。你需要增强系统，使预测能够很好地预测像素，包括地毯、沙发和墙上绘画的精细纹理。这基本上是一个彻底的失败。我们尝试了很多东西。我们尝试了直接的神经网络，我们尝试了 GANs，我们尝试了 VAEs，各种正则化的自编码器，我们尝试了很多东西。我们还尝试了这些方法来学习图像或视频的良好表示，然后可以用作图像分类系统的输入。这也基本上失败了。所有试图从图像或视频的损坏版本中预测图像或视频缺失部分的系统，基本上都失败了。所以，拿一个图像或视频，损坏它或以某种方式转换它，然后尝试从损坏的版本中重建完整的视频或图像。然后希望系统内部会发展出良好的图像表示，你可以用它来进行对象识别、分割等等。这基本上是一个彻底的失败。而且它对文本非常有效。这就是 LLMs 使用的原理，对吧？

**Lex Fridman:**  那么失败究竟在哪里呢？是很难形成一个好的图像表示吗？就像一个好的嵌入，包含图像中所有重要信息的嵌入？是在形成视频的图像到图像到图像到图像的一致性方面吗？如果我们把所有你失败的方式做一个亮点总结。那会是什么样子？

**Yann LeCun:**  好的。所以这不起作用的原因是... 首先，我必须确切地告诉你什么不起作用，因为还有一些东西是起作用的。所以不起作用的是训练系统通过从损坏的版本中重建一个好的图像来学习图像的表示。好的。这就是不起作用的。我们有一大堆技术，它们是使用自编码器的变体。我的 FAIR 同事开发了一个叫做 MAE 的东西，掩码自编码器。所以它基本上就像 LLMs 或类似的东西，你通过损坏文本来训练系统，只不过你损坏的是图像。你从中删除一些块，然后训练一个巨大的神经网络来重建。你得到的特征不好。你知道它们不好，因为如果你现在训练相同的架构，但你用标签数据、图像的文本描述等来监督训练它，你会得到好的表示。在识别任务上的性能比你进行这种自监督预训练要好得多。

**Lex Fridman:**  所以架构是好的。

**Yann LeCun:**  编码器的架构是好的。好的？但是你训练系统重建图像的事实并不能让它产生良好的通用图像特征。

**Lex Fridman:**  当你在自监督的方式下训练它的时候。

**Yann LeCun:**  通过重建进行自监督。

**Lex Fridman:**  是的，通过重建。

**Yann LeCun:**  好的，那么另一种选择是什么？(两人都笑了) 另一种选择是联合嵌入。

**Lex Fridman:**  什么是联合嵌入？这些让你如此兴奋的架构是什么？

---

**五、 JEPA (联合嵌入预测架构) (25:07)**

**Yann LeCun:**  好的，现在我们不再训练系统对图像进行编码，然后训练它从损坏的版本中重建完整图像，而是 എടു拿完整图像，拿损坏的或变换后的版本，让它们都通过编码器，这些编码器通常是相同的，但不一定。然后在这些编码器之上训练一个预测器，从损坏的输入的表示中预测完整输入的表示。好的？所以是联合嵌入，因为你取完整输入和损坏的版本或变换后的版本，让它们都通过编码器，所以你得到一个联合嵌入。然后你说我能不能从损坏的输入的表示中预测完整输入的表示？好的？我称之为 JEPA，意思是联合嵌入预测架构，因为有联合嵌入，并且有一个预测器从坏家伙的表示中预测好家伙的表示。最大的问题是如何训练这样的东西？直到五年前或六年前，我们还没有特别好的答案来训练这些东西，除了一个叫做对比学习的方法。对比学习的思想是，你拿一对图像，还是一张图像和它的损坏版本或退化版本，或者原始图像的变换版本。你训练预测的表示与那个相同。如果你只这样做，这个系统就会崩溃。它基本上完全忽略了输入，并产生恒定的表示。所以对比方法避免了这一点。这些东西从 90 年代初就出现了，我在 1993 年有一篇关于这个的论文，你还展示了你知道不同的图像对，然后你把表示推离彼此。所以你说，我们知道相同的表示，应该是相同的或应该相似的，但我们知道不同的表示应该是不同的。这可以防止崩溃，但它有一些局限性。在过去的六七年里，出现了一大堆技术，可以复兴这种方法。其中一些来自 FAIR，一些来自谷歌和其他地方。但是这些对比方法有限制。在过去的三四年里发生的变化是，我们现在有了非对比的方法。所以它们不需要那些我们知道不同的负对比样本。你只用作为同一事物的不同版本或不同视角的图像来训练它们。你依靠一些其他的调整来防止系统崩溃。我们现在有六种不同的方法来实现这一点。

---

**六、 JEPA 与 LLMs 的对比 (28:15)**

**Lex Fridman:**  那么联合嵌入架构和 LLMs 之间的根本区别是什么？所以 JEPA 能带我们走向 AGI 吗？我们是否应该说你不喜欢 AGI 这个术语，我们可能会争论，我想每次我和你谈话时，我们都会争论 AGI 中的 G。

**Yann LeCun:**  是的。

**Lex Fridman:**  我明白，我明白，我明白。(笑) 好吧，我们可能会继续争论这个问题。这很好。因为你像法国人，ami 我想在法语中是朋友的意思——

**Yann LeCun:**  是的。

**Lex Fridman:**  AMI 代表高级机器智能——

**Yann LeCun:**  对。

**Lex Fridman:**  但无论如何，JEPA 能带我们走向那个，走向那个高级机器智能吗？

**Yann LeCun:**  嗯，所以这是第一步。好的？首先，与像 LLMs 这样的生成架构有什么区别？所以 LLMs 或通过重建训练的视觉系统生成输入，对吧？它们生成未损坏、未转换的原始输入，对吧？所以你必须预测所有的像素。系统中花费了大量的资源来实际预测所有这些像素，所有的细节。在 JEPA 中，你不试图预测所有的像素，你只试图预测输入的抽象表示，对吧？这在很多方面都容易得多。所以 JEPA 系统在训练时试图做的是，从输入中提取尽可能多的信息，但只提取相对容易预测的信息。好的。所以世界上有很多东西我们无法预测。例如，如果你有一辆自动驾驶汽车在街道或道路上行驶。道路周围可能有树木。而且可能是一个有风的日子，所以树上的叶子以一种半混沌的随机方式移动，你无法预测，你也不关心，你不想预测。所以你想要的是你的编码器基本上消除所有这些细节。它会告诉你树叶在动，但它不会保留到底发生了什么的细节。所以当你在表示空间中进行预测时，你不必预测每片叶子的每个像素。这不仅简单得多，而且还允许系统基本上学习世界的抽象表示，其中可以建模和预测的东西被保留，其余的被视为噪声并被编码器消除。所以它提升了表示的抽象层次。如果你想想看，这是我们一直都在做的事情。每当我们描述一个现象时，我们都在特定的抽象层次上描述它。我们并不总是根据量子场论来描述每一个自然现象，对吧？那是不可能的，对吧？所以我们有多个抽象层次来描述世界上发生的事情。从量子场论开始，到原子理论、化学中的分子、材料，一直到现实世界中的具体物体等等。所以我们不能只在最低层次上建模一切。这就是 JEPA 的真正意义所在。以自监督的方式学习抽象表示。你也可以分层地做到这一点。所以我认为这是智能系统的一个重要组成部分。在语言中，我们可以不用这样做，因为语言在某种程度上已经是抽象的，并且已经消除了很多不可预测的信息。所以我们可以不用做联合嵌入，不用提升抽象层次，直接预测单词。

**Lex Fridman:**  所以联合嵌入。它仍然是生成式的，但它是在这个抽象的表示空间中生成式的。

**Yann LeCun:**  是的。

**Lex Fridman:**  你是说语言，我们对语言很懒，因为我们已经免费获得了抽象表示，现在我们必须退一步，实际上思考一般的智能系统，我们必须处理现实的、现实的全部混乱。你必须做这一步，从完整的、丰富的、详细的现实跳到基于你能推理的现实的抽象表示，以及所有这些东西。

**Yann LeCun:**  对。问题是那些通过预测学习的自监督算法，即使是在表示空间中，如果输入数据更冗余，它们会学习到更多的概念。数据中的冗余越多，它们就越能捕捉到它的一些内部结构。所以，在感知输入（如视觉）的结构中，比在文本中存在更多的冗余，文本的冗余要少得多。这又回到了你几分钟前提出的问题。语言可能确实代表了更多的信息，因为它已经被压缩了，你是对的。但这意味着它的冗余也更少。所以自监督的效果不会那么好。

**Lex Fridman:**  是否有可能将视觉数据上的自监督训练和语言数据上的自监督训练结合起来？即使你贬低那 10 的 13 次方个 token，那里面也蕴含着巨大的知识量。那 10 的 13 次方个 token 代表了我们人类所发现的全部，很大一部分。包括 Reddit 上的废话，以及所有书籍、文章和人类智力创造的全部内容。所以是否有可能将这两者结合起来？

**Yann LeCun:**  嗯，最终是可以的，但我认为如果我们过早地这样做，我们就有被诱惑作弊的风险。事实上，这就是人们目前在视觉语言模型中所做的。我们基本上是在作弊。我们正在使用语言作为拐杖来帮助我们的视觉系统从图像和视频中学习良好表示的不足。这样做的问题是，我们可能会通过输入图像来稍微改进我们的视觉语言系统，我的意思是我们的语言模型。但我们甚至无法达到具有语言的猫或狗的智力或对世界的理解水平。它们没有语言，它们比任何 LLM 都更了解世界。它们可以计划非常复杂的行动，并想象一系列行动的结果。我们如何让机器在与语言结合之前学习这些？显然，如果我们把它和语言结合起来，这将是一个赢家，但在那之前，我们必须关注我们如何让系统学习世界是如何运作的？

**Lex Fridman:**  所以这种联合嵌入预测架构，对你来说，这将能够学习到像常识一样的东西，像猫用来预测如何通过撞倒东西来最佳地捉弄它的主人的东西。

**Yann LeCun:**  这是希望。事实上，我们使用的技术是非对比的。所以不仅架构是非生成的，我们使用的学习程序也是非对比的。我们有两套技术。一套基于蒸馏，有很多方法使用这个原理。DeepMind 的一个叫做 BYOL。FAIR 的几个，一个叫做 VICReg，另一个叫做 I-JEPA。我应该说 VICReg 实际上不是一个蒸馏方法，但 I-JEPA 和 BYOL 肯定是。还有一个叫做 DINO 或 Dino，也是在 FAIR 产生的。这些东西的想法是，你拿完整的输入，比如说一张图像。你通过一个编码器运行它，产生一个表示。然后你损坏那个输入或转换它，通过本质上相当于相同编码器的东西运行它，有一些细微的差别。然后训练一个预测器。有时预测器非常简单，有时它不存在。但是训练一个预测器从损坏的输入中预测第一个未损坏输入的表示。但你只训练第二个分支。你只训练网络中被损坏输入馈送的部分。你不训练另一个网络。但由于它们共享相同的权重，当你修改第一个时，它也会修改第二个。通过各种技巧，你可以防止系统崩溃，我之前解释过的那种类型的崩溃，系统基本上忽略了输入。所以这非常有效。

---

**七、 DINO 和 I-JEPA (37:31)**

**Yann LeCun:** 我们在 FAIR 开发的两种技术，DINO 和 I-JEPA 在这方面非常有效。

**Lex Fridman:**  那么我们在这里谈论的是什么样的数据？

**Yann LeCun:**  所以有几种情况。一种情况是你拿一张图片，你通过改变裁剪来损坏它，例如，稍微改变大小，也许改变方向，模糊它，改变颜色，对它做各种可怕的事情——

**Lex Fridman:**  但是基本的可怕的事情。

**Yann LeCun:**  基本的可怕的事情，会稍微降低质量，改变取景，裁剪图像。在某些情况下，在 I-JEPA 的情况下，你不需要做任何这些，你只需要掩盖其中的一些部分，对吧？你基本上只是删除一些区域，就像一个大块一样。然后通过编码器运行，训练整个系统，编码器和预测器，从损坏的输入的表示中预测好的输入的表示。这就是 I-JEPA。例如，它不需要知道它是一张图像，因为它唯一需要知道的是如何进行这种掩蔽。而对于 DINO，你需要知道它是一张图像，因为你需要做几何变换、模糊和类似的事情，这些都是图像特有的。

---

**八、 V-JEPA (38:51)**

**Yann LeCun:** 我们最近有一个更新的版本叫做 V-JEPA。所以它的基本思想和 I-JEPA 是一样的，只不过它应用于视频。所以现在你拿一个完整的视频，然后你掩盖其中的一大块。我们实际上掩盖的是一个时间管。所以在整个视频中，每一帧都有一个片段被掩盖。

**Lex Fridman:**  那个管子在整个帧中是静态定位的吗？它真的是一个直管吗？

**Yann LeCun:**  在整个管子中，是的。通常是 16 帧或类似的东西，我们在整个 16 帧中掩盖相同的区域。显然，每个视频都是不同的。然后再次训练那个系统，以便从部分掩盖的视频中预测完整视频的表示。这非常有效。这是我们拥有的第一个能够学习良好视频表示的系统，这样当你把这些表示输入到一个有监督的分类器头部时，它可以告诉你视频中发生了什么动作，并且准确率相当高。所以这是我们第一次得到这种质量的东西。

**Lex Fridman:**  所以这是一个很好的测试，表明形成了一个良好的表示。这意味着这里面有些东西。

**Yann LeCun:**  是的。我们还有初步的结果，似乎表明该表示允许我们的系统判断视频是否在物理上是可能的，或者完全不可能，因为某些物体消失了，或者一个物体突然从一个位置跳到另一个位置，或者改变了形状或其他什么。

**Lex Fridman:**  所以它能够捕捉到关于视频中表示的现实的一些基于物理的约束？

**Yann LeCun:**  是的。

**Lex Fridman:**  关于物体的出现和消失？

**Yann LeCun:**  是的。这真的很新。

**Lex Fridman:**  好的，但这真的能让我们得到这种理解世界的模型，能够驾驶汽车吗？

**Yann LeCun:**  可能。这将需要一段时间才能达到这一点。已经有一些基于这个想法的机器人系统。你所需要的是一个稍微修改过的版本，想象你有一个视频，一个完整的视频，你对这个视频所做的是，你要么在时间上向未来平移它。所以你只会看到视频的开头，但你看不到原始视频中的后半部分。或者你只是掩盖视频的后半部分，例如。然后你训练这个 I-JEPA 系统，或者我描述的类型，从偏移的视频中预测完整视频的表示。但你还向预测器输入一个动作。例如，方向盘向右转了 10 度或类似的东西，对吧？所以如果这是一辆汽车的行车记录仪，你知道方向盘的角度，你应该能够在某种程度上预测你看到的东西会发生什么。你显然无法预测视图中出现的所有物体的细节，但在抽象的表示层面上，你可能可以预测会发生什么。所以现在你有一个内部模型，它说，这是我对时间 T 的世界状态的概念，这是一个我正在采取的行动，这是对时间 T 加 1 的世界状态的预测，T 加 delta T，T 加 2 秒，不管是什么。如果你有这种类型的模型，你可以用它来进行规划。所以现在你可以做 LLMs 不能做的事情，即计划你要做什么，以便你达到一个特定的结果或满足一个特定的目标，对吧？所以你可以有许多目标，对吧？我可以预测，如果我有这样一个物体，对吧？我张开手，它就会掉下来，对吧？如果我用特定的力在桌子上推它，它就会移动。如果我推桌子本身，它可能不会以相同的力移动。所以我们脑海中都有这个世界的内部模型，这允许我们计划行动序列以达到特定的目标。所以现在如果你有这个世界模型，我们可以想象一系列的行动，预测行动序列的结果会是什么，测量最终状态在多大程度上满足一个特定的目标，比如把瓶子移到桌子的左边。然后计划一个行动序列，在运行时最小化这个目标。我们不是在谈论学习，我们是在谈论推理时间，对吧？所以这实际上是规划。在最优控制中，这是一个非常经典的东西。它被称为模型预测控制。你有一个你想要控制的系统的模型，它可以预测与一系列命令相对应的状态序列。你正在计划一系列的命令，这样根据你的世界模型，系统的最终状态将满足你设定的任何目标。这就是自计算机出现以来，火箭轨迹的规划方式。所以基本上是从 60 年代初开始的。

**Lex Fridman:**  所以，是的，对于模型预测控制，但你也经常谈论分层规划。

**Yann LeCun:**  是的。

---

**九、 分层规划 (44:22)**

**Lex Fridman:**  分层规划能以某种方式从中产生吗？

**Yann LeCun:**  嗯，不能。你必须构建一个特定的架构来允许分层规划。所以如果你想计划复杂的行动，分层规划是绝对必要的。如果我想从，比如说，从纽约到巴黎，这是我一直使用的例子。我坐在纽约大学的办公室里。我需要最小化的目标是我到巴黎的距离。在一个非常抽象的、高层次的表示中，我需要把它分解成两个子目标。第一个是去机场，第二个是乘飞机去巴黎。好的。所以我的子目标现在是去机场。我的目标函数是我到机场的距离。我怎么去机场？嗯，我必须走到街上叫一辆出租车，这在纽约是可以做到的。好的，现在我有另一个子目标。下楼到街上。这意味着走到电梯，乘电梯下楼，走到街上。我怎么去电梯？我必须从椅子上站起来，打开我办公室的门，走到电梯，按下按钮。我怎么从椅子上站起来？就像你可以想象一直往下走，一直到基本上相当于每毫秒的肌肉控制。好的？显然你不会根据每毫秒的肌肉控制来计划你从纽约到巴黎的整个行程。首先，这将非常昂贵，而且也完全不可能，因为你不知道所有的情况，不知道打车要花多长时间，或者去机场的交通情况。我的意思是，你必须确切地知道所有的情况才能进行这种计划，而你没有这些信息。所以你必须进行这种分层规划，这样你就可以开始行动，然后随着你的进行重新计划。在人工智能领域，没有人真正知道如何做到这一点。没有人知道如何训练一个系统来学习适当的多层次表示，以便分层规划能够工作。

**Lex Fridman:**  是否已经出现了类似的东西？比如说，你能用一个 LLM，最先进的 LLM，通过做你刚才所做的详细的问题集，让你从纽约到巴黎吗？也就是说，你能给我一个从纽约到巴黎需要做的 10 个步骤的清单吗？然后对于每一个步骤，你能给我一个关于如何实现这一步的 10 个步骤的清单吗？然后对于每一个步骤，你能给我一个关于如何实现每一个步骤的 10 个步骤的清单吗？直到你移动你的每一块肌肉？也许不是。无论你能用你自己的头脑做什么。

**Yann LeCun:** 对。这里面也隐含了很多问题，对吧？所以首先，LLMs 能够回答其中一些问题，到一定的抽象层次。前提是它们已经在训练集中接受过类似场景的训练。

**Lex Fridman:** 它们能够回答所有这些问题。但其中一些可能是幻觉，意思是不符合事实的。

**Yann LeCun:** 是的，没错。我的意思是，它们可能会产生一些答案。只是它们无法真正产生你如何从椅子上站起来的每毫秒肌肉控制，对吧？但是，到了一定程度的抽象，你可以用语言来描述事情，它们也许能够给你一个计划，但前提是它们已经被训练来产生这些计划，对吧？它们无法为它们从未遇到过的情况进行计划。它们基本上必须重复它们所训练过的模板。

**Lex Fridman:** 但是，就拿从纽约到巴黎的例子来说，它会在哪里开始遇到麻烦？你认为它会在哪个抽象层开始出现问题？因为我可以想象几乎每一个部分，LLM 都能够以某种方式准确地回答，特别是当你在谈论纽约和巴黎这两个主要城市的时候。

**Yann LeCun:** 所以我的意思是，如果你对它进行微调，LLM 肯定能够解决这个问题。

**Lex Fridman:** 当然。

**Yann LeCun:** 所以我不能说 LLM 不能做到这一点，如果你训练它，它可以做到这一点，这是毫无疑问的，到一个可以用语言表达的程度。但是，如果你想深入到如何下楼梯，或者只是用语言表达如何从椅子上站起来，你是做不到的。这就是你需要物理世界经验的原因之一，它的带宽比你在人类语言中表达的要高得多。

**Lex Fridman:** 所以我们一直在谈论的联合嵌入空间，有没有可能这就是我们在机器人领域与物理现实互动所需要的？然后 LLMs 只是位于其上的东西，用于更大的推理，比如我需要订机票，我需要知道如何去网站等等。

**Yann LeCun:** 当然。人们知道的很多相对高层次的计划实际上是学习到的。大多数人不会自己发明计划。我们当然有能力做到这一点，但人们使用的大多数计划都是他们训练过的计划。就像他们看到其他人使用这些计划，或者他们被告知如何做事情，对吧？你不能发明如何，就像你带一个从未听说过飞机的人，告诉他们，你如何从纽约到巴黎？他们可能无法分解整个计划，除非他们以前见过类似的例子。所以 LLMs 肯定能够做到这一点。但是，如何将其与低层次的行动联系起来，这需要用像 JEPA 这样的东西来完成，它基本上提升了表示的抽象层次，而不试图重建情况的每一个细节。这就是我们需要 JEPAs 的原因。

---

**十、 自回归 LLMs (50:40)**

**Lex Fridman:** 我很想继续谈谈你对自回归 LLMs 的怀疑态度。我想测试一下这种怀疑态度的一种方法是，你所说的一切都很有道理，但如果我把你今天和总的来说所说的一切应用到，比如说，10 年前，也许少一点。不，假设是三年前。我将无法预测 LLMs 的成功。那么，自回归 LLMs 能够如此出色，对你来说有意义吗？

**Yann LeCun:** 是的。

**Lex Fridman:** 你能解释一下你的直觉吗？因为如果我把你的智慧和直觉当真，我会说自回归 LLMs 一次一个 token，不可能做到它们正在做的事情。

**Yann LeCun:** 不，有一件事，自回归 LLMs，或者说一般的 LLMs，不仅仅是自回归的，还包括 BERT 风格的双向的，它们正在利用的是自监督学习。多年来，我一直是自监督学习的非常非常强烈的倡导者。所以这些东西是自监督学习实际工作的非常令人印象深刻的证明。这个想法始于……它不是从 BERT 开始的，但它确实是一个很好的证明。所以这个想法是，你拿一段文本，你损坏它，然后你训练一些巨大的神经网络来重建缺失的部分。这已经产生了巨大的好处。它使我们能够创建理解语言的系统，能够将数百种语言翻译成任何方向的系统，多语言系统。这是一个可以被训练来理解数百种语言并在任何方向上进行翻译、生成摘要、回答问题和生成文本的单一系统。然后它有一个特例，就是自回归技巧，你限制系统不通过查看整个文本来阐述文本的表示，而只根据前面的单词预测一个单词。对吧？你通过限制网络的架构来做到这一点。这就是你可以从中构建自回归 LLM 的东西。所以多年前，仅解码器的 LLM 出现了一个惊喜。这种类型的系统只是试图从前一个单词中产生单词。当你把它们放大时，它们往往真的更理解语言。当你用大量数据训练它们时，你把它们做得很大。这是一个惊喜。这个惊喜发生在很久以前。就像谷歌、Meta、OpenAI 等的工作，可以追溯到 GPT，通用的预训练 Transformer。

**Lex Fridman:** 你的意思是像 GPT-2？就像在某个地方你开始意识到缩放实际上可能会继续给我们带来涌现的好处。

**Yann LeCun:** 是的，我的意思是，有来自不同地方的工作，但如果你想把它放在 GPT 的时间线上，那大约是 GPT-2，是的。

**Lex Fridman:** 好吧，因为你说了，你很有魅力，你说了这么多话，但是自监督学习，是的。但是，同样的直觉，你认为自回归 LLMs 不能对世界有深刻的理解，如果我们应用同样的直觉，它们能够形成足够的世界表示，基本上令人信服地通过了最初的图灵测试，这对你来说有意义吗？

**Yann LeCun:** 嗯，我们被它们的流畅性所欺骗了，对吧？我们只是假设，如果一个系统在操纵语言方面很流畅，那么它就具有人类智能的所有特征。但这种印象是错误的。我们真的被它欺骗了。

**Lex Fridman:** 你认为艾伦·图灵会怎么说？在不了解任何事情的情况下，只是和它相处——

**Yann LeCun:** 艾伦·图灵会认为图灵测试是一个非常糟糕的测试。(Lex 轻笑) 好的。这是人工智能社区多年前就决定的，图灵测试是一个非常糟糕的智能测试。

**Lex Fridman:** 汉斯·莫拉维克会对大型语言模型说什么？

**Yann LeCun:** 汉斯·莫拉维克会说莫拉维克悖论仍然适用。

**Lex Fridman:** 好的。

**Yann LeCun:** 好的？好的，我们可以通过——

**Lex Fridman:** 你不认为他会印象深刻。

**Yann LeCun:** 不，当然每个人都会印象深刻。(笑) 但这不是一个是否印象深刻的问题，这是一个知道这些系统的极限能做什么的问题。再次强调，它们令人印象深刻。它们可以做很多有用的事情。围绕它们正在建立一个完整的产业。它们将取得进展，但有很多事情它们做不到。我们必须意识到它们不能做什么，然后弄清楚我们如何到达那里。我这么说不是……我是根据 10 年来对自监督学习理念的研究来说的，实际上可以追溯到 10 多年前，但是自监督学习的理念。所以基本上是在没有任何特定任务训练系统的情况下，捕捉一组输入的一个内部结构，对吧？学习表示。我 14 年前与人共同创立的会议叫做国际学习表示会议，这就是深度学习正在处理的整个问题。对吧？这已经困扰了我将近 40 年。所以学习表示才是真正的问题所在。在很长一段时间里，我们只能通过监督学习来做到这一点。然后我们开始研究我们过去称之为无监督学习的东西，并在 2000 年代初与 Yoshua Bengio 和 Jeff Hinton 一起复兴了无监督学习的想法。然后发现，如果你能收集到足够的数据，监督学习实际上效果很好。所以无监督自监督的整个想法暂时退居二线，然后我试图从 2014 年我们开始 FAIR 的时候开始大规模地复兴它，并真正推动寻找新的方法来进行自监督学习，无论是文本、图像、视频还是音频。其中一些工作非常成功。我的意思是，我们拥有多语言翻译系统的原因，在 Meta 上进行内容审核的事情，例如，在 Facebook 上进行多语言的内容审核，了解一段文本是否是仇恨言论，是由于在 NLP 中使用自监督学习的进步，将其与 Transformer 架构相结合等等。但这是自监督学习的巨大成功。我们在语音识别方面也取得了类似的成功，一个叫做 Wav2Vec 的系统，顺便说一下，它也是一个联合嵌入架构，通过对比学习进行训练。该系统还可以生成多语言的语音识别系统，主要使用未标记的数据，只需要几分钟的标记数据就可以实际进行语音识别。这太棒了。我们现在有基于这些想法组合的系统，可以进行数百种语言之间的实时翻译，语音到语音。

**Lex Fridman:** 语音到语音，甚至包括，这很吸引人，没有书面形式的语言——

**Yann LeCun:** 没错。

**Lex Fridman:** 只有口语。

**Yann LeCun:** 没错。我们不经过文本，它直接从语音到语音，使用内部表示，这是一种离散的语音单元。但它被称为无文本 NLP。我们过去是这样称呼它的。但是，是的。我的意思是，在那里取得了令人难以置信的成功。然后，在 10 年的时间里，我们试图将这种想法应用于通过训练系统预测视频来学习图像的表示，通过训练系统预测视频中会发生什么来学习直观物理，并尝试了又尝试，失败了又失败，使用生成模型，使用预测像素的模型。我们无法让它们学习到良好的图像表示，我们也无法让它们学习到良好的视频表示。我们尝试了很多次，我们发表了很多关于它的论文。它们有点效果，但不是很好。它开始起作用了，我们放弃了预测每个像素的想法，基本上只是做联合嵌入并在表示空间中进行预测。这很有效。所以有充分的证据表明，我们无法使用生成模型来学习现实世界的良好表示。所以我告诉人们，每个人都在谈论生成式人工智能。如果你真的对人类水平的人工智能感兴趣，那就放弃生成式人工智能的想法。(Lex 笑)

**Lex Fridman:** 好的。但是你真的认为有可能通过联合嵌入表示走得很远吗？比如有常识推理，然后有高层次的推理。我觉得这两者是……LLMs 能够做到的那种推理。好吧，让我不用“推理”这个词，但是 LLMs 能够做到的那种东西似乎与我们用来在世界上导航的常识推理有根本的不同。

**Yann LeCun:** 是的。

**Lex Fridman:** 似乎我们需要两者——

**Yann LeCun:** 当然。

**Lex Fridman:** 你能通过联合嵌入这种 JEPA 类型的方法，通过观察视频，你能学会，让我们看看，如何从纽约到巴黎，或者如何理解世界上的政治状况吗？(两人都笑了) 对吧？这些是各种各样的人类产生了大量的语言和观点的事情，在语言的空间中，但并没有以任何清晰可压缩的方式在视觉上表示出来。

**Yann LeCun:** 对。嗯，有很多情况对于一个纯粹基于语言的系统来说可能很难知道。比如，好吧，你可能可以从阅读文本中学习，世界上所有公开的文本，我不能通过打个响指就从纽约到巴黎。这是行不通的，对吧？

**Lex Fridman:** 是的。

**Yann LeCun:** 但是可能有一些更复杂的场景，LLM 可能从未遇到过，并且可能无法确定它是否可能。所以从低层次到高层次的链接……问题是，语言表达的高层次是基于低层次的共同经验，而 LLMs 目前没有这种经验。当我们互相交谈时，我们知道我们对世界有共同的经验。很多都是相似的。而 LLMs 没有。

**Lex Fridman:** 但是，你看，它是存在的。你我对重力如何工作之类的物理学有共同的经验。那种对世界的共同认识，我觉得是存在于语言中的。我们没有明确地表达它，但是如果你有大量的文本，你就会得到这些字里行间的含义。为了形成一个一致的世界模型，你必须理解重力是如何工作的，即使你没有对重力的明确解释。所以即使在重力的情况下，有明确的解释。维基百科里有重力。但是，就像我们认为是常识推理的东西，我觉得要正确地生成语言，你就必须弄清楚这一点。现在，你可以说，就像你说的，没有足够的文本——

**Yann LeCun:** 嗯，我同意。

**Lex Fridman:** 抱歉。好的，是的。(笑) 你不这么认为？

**Yann LeCun:** 不，我同意你刚才说的，也就是说，为了能够进行高层次的常识推理……为了拥有高层次的常识，你需要有低层次的常识作为基础。

**Lex Fridman:** 是的。

**Yann LeCun:** 但这不存在。

**Lex Fridman:** 这不存在于 LLMs 中。LLMs 纯粹是从文本中训练出来的。所以你说的另一句话，我不同意世界上所有语言中都隐含着潜在的现实。有很多关于潜在现实的东西没有在语言中表达出来。

**Lex Fridman:** 这对你来说很明显吗？

**Yann LeCun:** 是的，完全是。

**Lex Fridman:** 所以就像我们所有的谈话……好的，有暗网，意思是，私人谈话，比如私信等等，这可能比 LLMs 训练的内容要大得多。

**Yann LeCun:** 你不需要交流那些共同的东西。

**Lex Fridman:** 但是幽默，所有的一切。不，你需要。你不需要，但它会出现。就像如果我不小心把这个打翻了，你可能会取笑我。在你取笑我的内容中，将会解释杯子会掉下来，然后重力以这种方式工作。然后你会得到一些非常模糊的信息，关于什么样的东西在掉到地上时会爆炸。然后也许你会开一个关于熵的玩笑，或者类似的东西，我们将永远无法再重建这个。就像，好吧，你会开一个这样的小玩笑，还会有上万亿个其他的玩笑。从这些笑话中，你可以拼凑出重力是有效的，杯子会碎，所有这些东西，你不需要看到……这将是非常低效的。不打翻东西会更容易。(笑)

**Yann LeCun:** 是的。

**Lex Fridman:** 但我觉得如果你有足够的数据，它就会存在。

**Yann LeCun:** 我只是认为，当我们在婴儿时期积累的大部分这类信息都不存在于文本中，基本上不存在于任何描述中。感官数据是获得这种理解的丰富得多的来源。我的意思是，这就是一个 4 岁孩子 16,000 小时的清醒时间。并且倾向于通过视觉获得 15 个字节。只是视觉，对吧？触觉也有类似的带宽，音频少一点。然后文本没有……语言直到一岁左右才出现。到你九岁的时候，你已经了解了重力，你知道惯性，你知道重力，你知道稳定性，你知道有生命和无生命物体之间的区别。到 18 个月大的时候，你就知道人们为什么想做某事，如果他们做不到，你就会帮助他们。我的意思是，有很多东西你主要是通过观察来学习的，甚至不是通过互动。在生命的最初几个月里，婴儿对世界没有任何影响。他们只能观察，对吧？你仅仅通过这些就积累了大量的知识。所以这就是我们当前的 AI 系统所缺少的。

---

**十一、 AI 中的推理 (1:11:30)**

**Lex Fridman:** 我想在你的一张幻灯片中，你有一个很好的图表，这是你展示 LLMs 是有限的一种方式。我想知道你是否可以从你的角度谈谈幻觉。为什么大型语言模型会出现幻觉，以及这在多大程度上是大型语言模型的一个基本缺陷。

**Yann LeCun:** 对。所以由于自回归预测，每次 LLM 产生一个 token 或一个单词时，该单词都有一定程度的概率会把你带离合理的答案集。如果你假设，这是一个非常强的假设，即这种错误的概率，这些错误在产生的 token 序列中是独立的。这意味着每次你产生一个 token，你保持在正确答案集中的概率就会降低，并且呈指数级下降。

**Lex Fridman:** 所以有一个很强的假设，就像你说的，如果存在犯错的非零概率，这似乎是存在的，那么就会有一种漂移。

**Yann LeCun:** 是的。而且这种漂移是指数级的。就像错误累积，对吧？所以答案是无意义的概率随着 token 的数量呈指数级增长。

**Lex Fridman:** 这对你来说很明显吗？嗯，从数学上讲也许是，但是不是有一种向真理的引力拉动吗？因为平均而言，希望真理在训练集中得到了很好的体现。

**Yann LeCun:** 不，这基本上是一场与维度诅咒的斗争。所以你可以纠正这个问题的方法是，通过让系统为人们可能提出的各种问题产生答案来微调系统。人就是人，所以他们提出的很多问题彼此非常相似。所以你可能可以涵盖人们会问的问题的 80% 或多少。然后你微调系统，为所有这些问题产生好的答案。它可能能够学习到这一点，因为它有很大的学习能力。但是还有大量的提示是你没有在训练中涵盖的。这个集合是巨大的。在所有可能的提示集合中，用于训练的提示的比例非常小。这是所有可能提示的一个非常非常小的子集。所以系统会在它经过预训练或微调的提示上表现得当。但是还有一个完整的空间，它不可能在上面进行过训练，因为这个数字太大了。所以无论系统经过了怎样的训练来产生适当的答案，你都可以通过找到一个提示来打破它，这个提示将超出它所训练的提示集或类似的东西，然后它就会吐出完全无意义的东西。

**Lex Fridman:** 当你说提示时，你是指那个确切的提示，还是你指的是一个在很多方面都非常不同的提示？在互联网上提出一个问题或说一件以前没有说过的事情有那么容易吗？

**Yann LeCun:** 我的意思是，人们已经想出了一些办法，比如你在提示中放入一个随机的字符序列，这就足以让系统进入一种模式，它会回答一些与没有这个提示时完全不同的东西。所以这是一种越狱系统的方法，基本上。超出它的条件，对吧？

**Lex Fridman:** 所以这是一个非常清楚的证明。但是，当然，这超出了它的设计目的，对吧？如果你把合理的语法句子拼接在一起，打破它有那么容易吗？

**Yann LeCun:** 是的。有些人做过这样的事情，你用英语写一个句子，或者你用英语问一个问题，它会产生一个非常好的答案。然后你把几个单词替换成另一种语言中的相同单词，突然之间，答案就完全没有意义了。

**Lex Fridman:** 是的。所以我想说的是，人类可能产生的提示中，有多少比例会破坏系统？

**Yann LeCun:** 所以问题是存在一个长尾。

**Lex Fridman:** 是的。

**Yann LeCun:** 这是很多人在社交网络等中意识到的一个问题，就是人们会问的问题有一个非常非常长的尾巴。你可以为 80% 或多少比例的人们会问的事情微调系统。然后这个长尾太大了，你不可能为所有的情况微调系统。最终，系统最终变成了一个巨大的查找表，对吧？(笑) 本质上。这并不是你真正想要的。你想要能够推理的系统，当然还有能够计划的系统。所以在 LLM 中进行的推理类型是非常非常初级的。你可以看出它很原始的原因是，每个产生的 token 所花费的计算量是恒定的。所以如果你问一个问题，那个问题在一个给定数量的 token 中有一个答案，用于计算该答案的计算量可以精确地估计出来。这是预测网络的大小，它有 36 层或 92 层或 whatever，乘以 token 的数量。就是这样。所以基本上，无论被问到的问题是容易回答的、难以回答的还是不可能回答的，因为它是决定的，嗯，有某种东西。系统能够用于答案的计算量是恒定的，或者与答案中产生的 token 数量成正比，对吧？这不是我们的工作方式，我们的推理方式是，当我们面对一个复杂的问题或一个复杂的问题时，我们会花更多的时间来解决它和回答它，对吧？因为它更难。

**Lex Fridman:** 有一个预测的元素，有一个迭代的元素，你通过一遍又一遍地调整你对某件事的理解。有分层的元素。这是否意味着这是 LLMs 的一个基本缺陷——

**Yann LeCun:** 是的。

**Lex Fridman:** 还是意味着……(笑) 这个问题还有更多内容？(笑) 现在你的行为就像一个 LLM。(笑) 立即回答。不，这只是一个低层次的世界模型，在此基础上，我们可以构建一些这样的机制，正如你所说，持久的长期记忆或推理等等。但是我们需要那个来自语言的世界模型。也许在这个构建良好的世界模型之上构建这种推理系统并不那么困难。

**Yann LeCun:** 好的。无论它是否困难，不久的将来就会知道，因为很多人都在研究对话系统的推理和规划能力。我的意思是，即使我们把自己限制在语言上，只要能够在回答之前计划你的答案，用不一定与你将用来产生答案的语言相关的术语。对吧？所以这种允许你计划你要说什么的心理模型的想法，在你说话之前。这非常重要。我认为在接下来的几年里，会有很多系统具有这种能力，但是这些系统的蓝图将与自回归 LLMs 非常不同。所以这与心理学中人类的系统一和系统二之间的区别相同，对吧？所以系统一是那种你可以完成的任务，而不需要刻意地思考你如何做它们。你只是做它们。你已经做了足够多的次数，以至于你可以下意识地做它们，对吧？不用思考它们。如果你是一个有经验的司机，你可以在开车时不用真正思考它，你可以同时和别人说话或听收音机，对吧？如果你是一个非常有经验的棋手，你可以在和一个没有经验的棋手下棋时不用真正思考，你只是识别模式然后下棋，对吧？这是系统一。所以所有你本能地做的事情，而不必刻意地计划和思考它。然后还有一些任务需要你计划。所以如果你是一个不太有经验的棋手，或者你是有经验的，但你和另一个有经验的棋手下棋，你会考虑各种选择，对吧？你会思考一段时间，对吧？如果你有时间思考它，你会比你在时间有限的情况下快棋下得好得多。所以这种刻意的计划，它使用你的内部世界模型，那是系统二，这是 LLMs 目前无法做到的。我们如何让它们做到这一点，对吧？我们如何构建一个能够进行这种计划或推理的系统，该系统将更多的资源用于复杂问题而不是简单问题。这不会是 token 的自回归预测，它将更类似于在过去被称为概率模型或图形模型之类的东西中推断潜变量。所以基本上原理是这样的。提示就像观察到的变量。模型的作用是，它基本上是一种度量……它可以衡量一个答案对于一个提示来说有多好。好的？所以把它想象成一个巨大的神经网络，但它只有一个输出。那个输出是一个标量数字，如果答案对于问题来说是一个好的答案，它就是 0，如果答案对于问题来说不是一个好的答案，它就是一个很大的数字。想象一下你有这个模型。如果你有这样一个模型，你可以用它来产生好的答案。你所做的就是产生提示，然后在可能的答案空间中搜索一个使该数字最小化的答案。这就是基于能量的模型。

**Lex Fridman:** 但是那个基于能量的模型将需要由 LLM 构建的模型。

**Yann LeCun:** 嗯，所以你真正需要做的不是在可能的文本字符串中搜索以最小化该能量。但你所做的是在抽象的表示空间中进行。所以在抽象思维的空间中，你会阐述一个想法，对吧？使用这个最小化模型输出的过程。好的？这只是一个标量。这是一个优化过程，对吧？所以现在系统产生答案的方式是通过最小化一个目标函数，基本上，对吧？我们谈论的是推理，我们不是在谈论训练，对吧？系统已经被训练好了。所以现在我们有一个答案的抽象表示，答案的表示。我们把它输入到一个基本上是自回归解码器的东西中，它可以非常简单，把这个转换成表达这个想法的文本。好的？所以在我看来，这就是未来数据系统的蓝图。它们将在将其转换为文本之前，通过优化来思考它们的答案，计划它们的答案。这是图灵完备的。

**Lex Fridman:** 你能确切地解释一下那里的优化问题是什么吗？就像目标函数是什么？稍微详细说明一下。你简要地描述了它，但是在什么空间上进行优化？

**Yann LeCun:** 表示的空间——

**Lex Fridman:** 进入抽象表示。

**Yann LeCun:** 没错。所以你在系统内部有一个抽象表示。你有一个提示。提示通过一个编码器，产生一个表示，也许通过一个预测器来预测答案的表示，正确答案的表示。但是那个表示可能不是一个好的答案，因为可能需要你做一些复杂的推理，对吧？所以你有另一个过程，它接受答案的表示并修改它，以便最小化一个成本函数，该函数衡量答案在多大程度上是问题的一个好答案。现在我们暂时忽略一个问题……我的意思是，暂时忽略如何训练该系统来衡量一个答案是否是一个好答案的问题。

**Lex Fridman:** 但是假设可以创建这样一个系统，这个过程是什么？这种类似于搜索的过程。

**Yann LeCun:** 这是一个优化过程。如果整个系统是可微的，你可以做到这一点，标量输出是通过某个神经网络运行答案的结果，答案的表示通过某个神经网络。然后通过梯度下降，通过反向传播梯度，你可以弄清楚如何修改答案的表示，从而最小化它。

**Lex Fridman:** 所以这仍然是基于梯度的。

**Yann LeCun:** 这是基于梯度的推理。所以现在你在抽象空间中有一个答案的表示。现在你可以把它转换成文本，对吧？这样做的好处是，表示现在可以通过梯度下降进行优化，而且也独立于你将用来表达答案的语言。

**Lex Fridman:** 对。所以你在一个子结构的表示中操作。我的意思是，这又回到了联合嵌入。

**Yann LeCun:** 对。

**Lex Fridman:** 在……的空间中工作更好，我不知道。或者把这个概念浪漫化，就像概念的空间，而不是具体的感官信息的空间。

**Yann LeCun:** 对。

**Lex Fridman:** 好的。但是这能做一些像推理一样的事情吗，这就是我们正在谈论的？

**Yann LeCun:** 嗯，不太行，只能以一种非常简单的方式。我的意思是，基本上你可以把这些东西看作是在做我刚才谈到的那种优化，只不过它们是在优化离散空间，也就是可能的 token 序列的空间。它们以一种非常低效的方式进行这种优化，即产生大量的假设，然后选择最好的。这在竞争方面是非常浪费的，因为你基本上必须为每一个可能的生成序列运行你的 LLM。这是非常浪费的。所以做一个优化要好得多，在连续空间中，你可以做梯度下降，而不是像生成大量的东西然后选择最好的，你只是迭代地改进你的答案以达到最好的，对吧？这效率高得多。但是你只能在具有可微函数的连续空间中做到这一点。

**Lex Fridman:** 你谈论的是推理，就像能够深入思考或深入推理的能力。你如何知道什么是基于深度推理的更好或更差的答案？

**Yann LeCun:** 对。所以现在我们要问的问题是，从概念上讲，你如何训练一个基于能量的模型？对吧？所以基于能量的模型是一个具有标量输出的函数，只是一个数字。你给它两个输入，X 和 Y，它告诉你 Y 是否与 X 兼容。X 是你观察到的，假设它是一个提示、一张图像、一段视频或其他什么。Y 是一个答案的提议，一段视频的延续，或其他什么。它告诉你 Y 是否与 X 兼容。它告诉你 Y 与 X 兼容的方式是，如果 Y 与 X 兼容，该函数的输出将为 0，如果 Y 与 X 不兼容，它将是一个正数，非零。好的。你如何训练这样的系统？在一个完全通用的层面上，你向它展示兼容的 X 和 Y 对，一个问题和相应的答案。你训练内部的大型神经网络的参数以产生 0。好的。现在这并不完全有效，因为系统可能会决定，好吧，我只是对所有东西都说 0。所以现在你必须有一个过程来确保对于一个错误的 Y，能量将大于 0。在那里你有两个选择，一个是对比方法。所以对比方法是你展示一个 X 和一个坏的 Y，你告诉系统，好吧，给这个一个高的能量。像提高能量，对吧？改变计算能量的神经网络中的权重，使它上升。这就是对比方法。这样做的问题是，如果 Y 的空间很大，你将不得不展示的这种对比样本的数量将是巨大的。但是人们会这样做。当你用 RLHF 训练一个系统时，他们会这样做，你基本上训练的是一个叫做奖励模型的东西，它基本上是一个目标函数，告诉你一个答案是好是坏。这基本上就是这个。所以我们已经在某种程度上做到了这一点。我们只是没有把它用于推理，我们只是把它用于训练。还有另一套方法，它们是非对比的，我更喜欢这些。那些非对比的方法基本上是说，好的，能量函数需要在来自你的训练集的兼容的 XY 对上具有低能量。你如何确保能量在其他地方会更高？你这样做的方法是有一个正则化器，一个标准，你成本函数中的一项，它基本上最小化了可以采取低能量的空间的体积。实现这一点的精确方法，有各种各样的具体方法，取决于架构，但这是基本原理。所以如果你把 XY 空间中特定区域的能量函数降低，它会在其他地方自动上升，因为只有有限的空间体积可以采取低能量。好的？通过系统的构造或通过正则化函数。

**Lex Fridman:** 我们一直在非常笼统地谈论，但是什么是好的 X 和好的 Y？什么是 X 和 Y 的良好表示？因为我们一直在谈论语言。如果你直接使用语言，那大概是不好的，所以必须有某种抽象的思想表示。

**Yann LeCun:** 是的。我的意思是，你可以直接用语言来做这个，只要，你知道，X 是一段文本，Y 是那段文本的延续。

**Lex Fridman:** 是的。

**Yann LeCun:** 或者 X 是一个问题，Y 是答案。

**Lex Fridman:** 但是你说这不会成功。我的意思是，这将会做 LLMs 正在做的事情。

**Yann LeCun:** 嗯，不。这取决于系统的内部结构是如何构建的。如果系统的内部结构是以这样一种方式构建的，即在系统内部有一个潜变量，我们称之为 Z，你可以操纵它以最小化输出能量，那么 Z 可以被视为一个好的答案的表示，你可以把它转换成一个好的答案 Y。

**Lex Fridman:** 所以这种系统可以用非常相似的方式进行训练？

**Yann LeCun:** 非常相似的方式。但是你必须有这种防止崩溃的方法，确保在你没有训练它的东西上有高能量。目前它在 LLMs 中是非常隐含的。它的完成方式是人们没有意识到它正在被完成，但它正在被完成。这是由于当你给一个单词一个高概率时，你会自动给其他单词一个低概率，因为你只有有限的概率可以分配。(笑) 对吧？它们的总和必须为 1。所以当你最小化交叉熵或 whatever，当你训练你的 LLM 来预测下一个单词时，你正在增加你的系统将给予正确单词的概率，但你也在降低它将给予错误单词的概率。现在，间接地，这给了好的单词序列一个低概率……一个高概率，给坏的单词序列一个低概率，但这是非常间接的。这一点都不明显，为什么这实际上会起作用，因为你不是在一个序列中所有符号的联合概率上做的，你只是在做，把概率分解成连续 token 的条件概率。

**Lex Fridman:** 那么你如何对视觉数据做这个？

**Yann LeCun:** 所以我们一直在用所有的 JEPA 架构来做这个，基本上是——

**Lex Fridman:** 联合嵌入？

**Yann LeCun:** I-JEPA。所以，两个东西之间的兼容性是，这是一张图像或一段视频，这是该图像或视频的损坏、偏移或变换的版本，或者被掩盖的版本。好的？然后系统的能量是表示的预测误差。好的东西的预测表示与好的东西的实际表示之间的差，对吧？所以你把损坏的图像输入到系统中，预测好的输入的未损坏的表示，然后计算预测误差。这就是系统的能量。所以这个系统会告诉你，这是一张好的图像，这是一张损坏的版本。如果这两个东西实际上是，其中一个是另一个的损坏版本，它会给你 0 能量，如果两张图像完全不同，它会给你一个高能量。

**Lex Fridman:** 希望整个过程能给你一个非常好的现实的压缩表示，视觉现实的压缩表示。

**Yann LeCun:** 我们知道它会，因为我们把这些表示作为分类系统或其他东西的输入，而且它很有效——

**Lex Fridman:** 然后那个分类系统工作得非常好。好的。嗯，总而言之，你以 Yann LeCun 独有的辛辣方式建议——

---

**十二、 强化学习 (1:29:02)**

**Yann LeCun:** 你建议我们放弃生成模型，转而使用联合嵌入架构？

**Yann LeCun:** 是的。

**Lex Fridman:** 放弃自回归生成。

**Yann LeCun:** 是的。

**Lex Fridman:** 放弃……(笑) 这感觉像是法庭证词。放弃概率模型，转而使用我们谈到的基于能量的模型，放弃对比方法，转而使用正则化方法。让我问你这个；你有一段时间一直是强化学习的批评者。

**Yann LeCun:** 是的。

**Lex Fridman:** 所以最后一个建议是我们放弃 RL，转而使用你所说的模型预测控制，只有当计划没有产生预测的结果时才使用 RL。在这种情况下，我们使用 RL 来调整世界模型或评论家。

**Yann LeCun:** 是的。

**Lex Fridman:** 你提到了 RLHF，带有人类反馈的强化学习。为什么你仍然讨厌强化学习？

**Yann LeCun:** 我不讨厌强化学习，我认为它——

**Lex Fridman:** 所以这都是爱？

**Yann LeCun:** 我认为它不应该被完全放弃，但我认为它的使用应该被最小化，因为在样本方面它非常低效。所以训练系统的正确方法是首先让它学习良好的世界表示和世界模型，主要通过观察，也许还有一点互动。

**Lex Fridman:** 然后根据它来引导它。如果表示是好的，那么调整应该是最小的。

**Yann LeCun:** 是的。现在有两件事。如果你已经学习了世界模型，你可以使用世界模型来计划一系列的行动以达到一个特定的目标。你不需要 RL，除非你衡量你是否成功的方式可能不准确。你关于你是否会从自行车上掉下来的想法可能是错误的，或者你正在与之 MMA 格斗的人会做某事，而他们做了其他的事情。(笑) 所以有两种方式你可能是错的。要么你的目标函数没有反映你想要优化的实际目标函数，要么你的世界模型不准确，对吧？所以你对世界上会发生什么所做的预测是不准确的。所以如果你想在你操作世界时调整你的世界模型，或者你的目标函数，那基本上是在 RL 的范围内。这在某种程度上是 RL 处理的事情，对吧？所以调整你的世界模型。即使是提前调整你的世界模型的方法，也是用你的世界模型探索空间的一部分，在那里你知道你的世界模型是不准确的。这基本上被称为好奇心，或者玩耍，对吧？当你玩耍时，你探索状态空间的一部分，你不想在现实中这样做，因为那可能很危险，但你可以在不杀死自己的情况下调整你的世界模型，基本上。(笑) 所以这就是你想用 RL 来做的事情。当需要学习一个特定的任务时，你已经拥有了所有好的表示，你已经拥有了你的世界模型，但你需要根据手头的情况调整它。那时你才使用 RL。

**Lex Fridman:** 你认为为什么 RLHF 效果这么好？这种带有人类反馈的强化学习，为什么它对之前的大型语言模型产生了如此变革性的影响？

**Yann LeCun:** 所以产生变革性影响的是人类反馈。有很多方法可以使用它，其中一些是纯粹监督的，实际上，它并不是真正的强化学习。

**Lex Fridman:** 所以这是 HF。(笑)

**Yann LeCun:** 这是 HF。然后有各种各样的方法来使用人类反馈，对吧？所以你可以让人类对答案进行评分，由世界模型产生的多个答案。然后你所做的就是训练一个目标函数来预测那个评分。然后你可以使用那个目标函数来预测一个答案是否是好的，你可以通过反向传播来微调你的系统，使它只产生高评分的答案。好的？所以这就像在 RL 中，这意味着训练一个所谓的奖励模型，对吧？所以基本上就是一个小的神经网络，用来估计一个答案在多大程度上是好的，对吧？这和我之前谈到的用于规划的目标非常相似，只不过现在它不是用于规划，而是用于微调你的系统。我认为将它用于规划会更有效，但目前它是用于微调系统的参数。现在，有几种方法可以做到这一点。其中一些是监督的。你只是问一个人，这个的正确答案是什么，对吧？然后你只需要输入答案。我的意思是，有很多方法可以调整这些系统。

---

**十三、 Woke AI (1:34:10)**

**Lex Fridman:** 现在，很多人都对最近发布的谷歌 Gemini 1.5 持非常批评的态度，基本上，用我的话来说，可以说是超级政治正确。政治正确这个词带有负面含义。它做了一些几乎可笑的荒谬的事情，比如它修改历史，比如生成一个黑人乔治·华盛顿的图像，或者更严重的是，你在推特上评论过的事情，它拒绝评论或生成图像，甚至拒绝描述天安门广场或坦克人，这是历史上最具传奇色彩的抗议图像之一。当然，这些图像受到了中国政府的严格审查。因此，每个人都开始质疑这些 LLMs 的设计过程是什么？审查在其中扮演了什么角色，以及所有这些问题。所以你在推特上评论说开源是答案。(笑) 本质上。你能解释一下吗？

**Yann LeCun:** 实际上我在所有我能用的社交网络上都发表了评论。(Lex 笑) 我在各种论坛上多次提出这个观点。这是我对这个问题的看法。人们可以抱怨 AI 系统有偏见，它们通常会因为训练数据的分布而产生偏见，这些数据反映了社会中的偏见。这对一些人来说可能是冒犯性的，也可能不是。而一些去偏见的技术对一些人来说变得冒犯性，因为历史不正确等等。所以你可以问这个问题。你可以问两个问题。第一个问题是，是否有可能产生一个没有偏见的 AI 系统？答案是绝对不可能。这并不是因为技术上的挑战，尽管这方面存在技术挑战。这是因为偏见存在于旁观者的眼中。不同的人对什么构成偏见可能有不同的看法，对很多事情来说都是如此。我的意思是，有些事实是无可争议的，但有很多观点或事情可以用不同的方式表达。所以你不可能有一个没有偏见的系统，这根本不可能。那么解决这个问题的答案是什么？答案和我们在自由民主制度中对新闻界的答案是一样的。新闻界需要自由和多样化。我们有言论自由是有充分理由的。这是因为我们不希望我们所有的信息都来自一个单一的来源，因为这与民主的整个理念背道而驰，也与进步思想甚至科学背道而驰，对吧？在科学中，人们必须为不同的观点争论。当人们意见不一致，他们提出一个答案并形成共识时，科学就会取得进步，对吧？这在世界各地的所有民主国家都是如此。所以有一个未来，它已经发生了，我们与数字世界的每一次互动都将由 AI 系统，AI 助手来调解，对吧？我们将拥有智能眼镜。你现在已经可以从 Meta 买到它们了 (笑)，Ray-Ban Meta。你可以和它们说话，它们与一个 LLM 相连，你可以得到你提出的任何问题的答案。或者你可以看着一个纪念碑，眼镜里有一个摄像头，你可以问它，你能告诉我关于这座建筑或这个纪念碑的事情吗？你可以看着一份外语菜单，它会为你翻译。如果我们说不同的语言，我们可以进行实时翻译。所以我们与数字世界的很多互动在不久的将来都将由这些系统来调解。我们使用的搜索引擎将越来越多地不再是搜索引擎，它们将是我们提出问题的对话系统，它会回答，然后也许会把你指向适当的参考资料。但问题是，我们不能让这些系统来自美国西海岸的少数几家公司，因为这些系统将构成所有人类知识的存储库。我们不能让它由少数人控制，对吧？它必须是多样化的，就像新闻界必须是多样化的一样。那么我们如何获得一组多样化的 AI 助手呢？训练一个基础模型，对吧？一个基础的 LLM，目前来说是非常昂贵和困难的。将来可能会有所不同，但目前这是一个 LLM。所以只有少数公司能够正确地做到这一点。如果其中一些子系统是开源的，任何人都可以使用它们，任何人都可以对它们进行微调。如果我们建立一些系统，允许任何人群，无论他们是个人公民、公民团体、政府组织、非政府组织、公司等等，使用这些开源系统，AI 系统，并根据他们自己的数据为他们自己的目的对它们进行微调，我们将拥有非常多样化的不同的 AI 系统，专门用于所有这些事情，对吧？所以我要告诉你，我和法国政府谈了很多，法国政府不会接受他们所有公民的数字饮食由美国西海岸的三家公司控制。这根本不可接受。这对民主是一个危险。不管这些公司的意图有多么好，对吧？这对当地文化、价值观、语言也是一个危险，对吧？我和印度的 Infosys 创始人谈过。他正在资助一个项目来微调 LLaMA 2，这是 Meta 产生的开源模型。这样 LLaMA 2 就可以说印度所有 22 种官方语言。这对印度人来说非常重要。我和我的一个前同事 Moustapha Cisse 谈过，他曾经是 FAIR 的一名科学家，然后搬回了非洲，为谷歌在非洲创建了一个研究实验室，现在有了一个新的创业公司 Kera。他正在尝试做的是基本上拥有一个会说塞内加尔当地语言的 LLM，这样人们就可以获得医疗信息，因为他们无法获得医生的帮助，塞内加尔的人均医生数量非常少。我的意思是，除非你有开源平台，否则你不可能拥有任何这些。所以有了开源平台，你就可以拥有 AI 系统，这些系统不仅在政治观点或类似的事情上是多样化的，而且在语言、文化、价值体系、政治观点、各个领域的技术能力上也是多样化的。你可以有一个行业，一个由公司组成的生态系统，为行业中的垂直应用微调那些开源系统，对吧？你有一个，我不知道，一个出版商有数千本书，他们想建立一个系统，允许客户就他们的任何一本书的内容提出问题。你需要在他们的专有数据上进行训练，对吧？你有一家公司，我们在 Meta 内部有一家，叫做 Meta Mate。它基本上是一个 LLM，可以回答关于公司内部事务的任何问题。非常有用。很多公司都想要这个，对吧？很多公司不仅希望为他们的员工提供这个，也希望为他们的客户提供这个，以照顾他们的客户。所以你将拥有一个人工智能行业的唯一途径，你将拥有不只是单一偏见的 AI 系统的唯一途径，就是如果你有开源平台，任何团体都可以在这些平台上构建专门的系统。所以历史的必然方向是，绝大多数的 AI 系统将建立在开源平台之上。

**Lex Fridman:** 所以这是一个美好的愿景。这意味着像 Meta 或谷歌这样的公司，应该在构建基础的预训练模型之后，只采取最少的微调步骤。尽可能少的步骤。

**Yann LeCun:** 基本上是这样。(Lex 叹气)

**Lex Fridman:** Meta 能负担得起这样做吗？

---

**十四、 开源 (1:43:48)**

**Lex Fridman:** 我不知道你是否知道这一点，但公司应该以某种方式赚钱。而开源就像是免费赠送……我不知道，马克做了一个视频，马克·扎克伯格。一个非常性感的视频，谈论了 350,000 个 Nvidia H100。计算一下，仅仅是 GPU，这就是一千亿美元，再加上训练所有东西的基础设施。我不是商业人士，但你怎么能从中赚钱呢？所以你描绘的愿景非常强大，但怎么可能赚钱呢？

**Yann LeCun:** 好的。所以你有几种商业模式，对吧？Meta 所围绕的商业模式是，你提供一项服务，这项服务的资金来源要么是通过广告，要么是通过商业客户。例如，如果你有一个 LLM，可以通过 WhatsApp 与顾客交谈来帮助一个夫妻比萨店，这样顾客就可以订购比萨，系统会问他们，你想要什么配料，或者什么尺寸等等。企业会为此付费。好的？这是一种模式。否则，如果它是一个更经典的系统，它可以是广告支持的，或者有几种模式。但关键是，如果你有足够大的潜在客户群，而且你需要为他们构建那个系统，那么将它分发到开源并不会损害你。

**Lex Fridman:** 再次强调，我不是商业人士，但如果你发布了开源模型，那么其他人也可以做同样的事情，并在其上竞争。基本上为企业提供微调的模型，Meta 的赌注是……顺便说一句，我是所有这些的忠实粉丝。但是 Meta 的赌注是，“我们会做得更好”？

**Yann LeCun:** 嗯，不是。赌注更像是，我们已经拥有庞大的用户群和客户群。

**Lex Fridman:** 啊，对。

**Yann LeCun:** 对吧？所以这对他们来说是有用的。无论我们为他们提供什么，都将是有用的，并且有一种方法可以从中获得收入。

**Lex Fridman:** 当然。

**Yann LeCun:** 提供该系统或基础模型并不会造成损害，对吧？开源的基础模型，供其他人在此基础上构建应用程序。如果这些应用程序对我们的客户有用，我们可以直接为他们购买。可能是他们会改进平台。事实上，我们已经看到了这一点。我的意思是，LLaMA 2 实际上有数百万的下载量，成千上万的人提供了关于如何改进它的想法。所以这显然加速了进步，使该系统可供更广泛的人群使用。实际上有成千上万的企业正在用它构建应用程序。Meta 从这项技术中获得收入的能力并没有因为开源基础模型的分发而受到损害。

**Lex Fridman:** 人们对 Gemini 的根本批评是，正如你所指出的，在西海岸……

---

**十五、 AI 与意识形态 (1:47:26)**

**Lex Fridman:**  澄清一下，我们现在在东海岸，我想 Meta AI 的总部就在这里。(笑) 所以关于西海岸的措辞很强硬。但我想问题在于，我认为可以公平地说，大多数科技人员在政治上都倾向于左翼。所以人们批评 Gemini 的问题是，在你提到的去偏见过程中，他们的意识形态倾向变得明显。这是可以避免的吗？你是说开源是唯一的办法？

**Yann LeCun:** 是的。

**Lex Fridman:** 你是否目睹了这种使工程变得困难的意识形态倾向？

**Yann LeCun:** 不，我认为这与……我认为问题不在于设计这些系统的人的政治倾向。它与他们的客户群或受众的可接受性或政治倾向有关，对吧？所以一家大公司不能承受冒犯太多人。所以他们会确保他们推出的任何产品都是“安全的”，无论这意味着什么。而且很可能做得过头。而且也很有可能……不可能为每个人都做得恰到好处。你不可能让每个人都满意。所以这就是我之前说的，你不可能有一个没有偏见的系统，并且被每个人都认为是公正的。它会变成，你把它推向一个方向，一群人会认为它有偏见。然后你把它推向另一个方向，另一群人会认为它有偏见。除此之外，还有一个问题，如果你把系统在一个方向上推得太远，它就会变得不符合事实，对吧？你会得到黑人纳粹士兵——

**Lex Fridman:** 是的。所以我们应该提到黑人纳粹士兵的图像生成，这在事实上是不准确的。

**Yann LeCun:** 对。对一些人来说也可能是冒犯性的，对吧？所以不可能产生对每个人都公正的系统。所以我看到的唯一解决办法是多样性。

**Lex Fridman:** 多样性这个词的完整含义，在各个方面的多样性。

**Yann LeCun:** 是的。

**Lex Fridman:** Marc Andreessen 今天刚刚发推文，我来做一个总结。结论是，只有初创公司和开源才能避免他在大科技公司中强调的问题。他问，大科技公司能否真正推出生成式 AI 产品？第一，来自内部活动家、员工暴徒、疯狂的高管、破碎的董事会、压力团体、极端主义监管机构、政府机构、媒体、“专家”不断升级的要求，以及一切都在破坏产出。第二，不断产生错误答案、画出糟糕图片或渲染糟糕视频的风险。谁知道它在任何时刻会说什么或做什么？第三，法律风险、产品责任、诽谤、选举法、许多其他事情等等。任何让国会生气的事情。第四，不断试图加强对可接受产出的控制，降低模型的质量，比如它的实际可用性、使用愉悦度和有效性等等。第五，糟糕的文本、图像、视频的公开实际上把这些例子放到了下一个版本的训练数据中。等等。所以他只是强调了这有多么困难。来自各种各样的人的不满。他只是说你无法创建一个让每个人都满意的系统。

**Yann LeCun:** 是的。

**Lex Fridman:** 所以如果你要自己进行微调并保持闭源，本质上问题就在于试图最小化不快乐的人数。

**Yann LeCun:** 是的。

**Lex Fridman:** 你说这几乎是不可能的，对吧？更好的办法是开源。

**Yann LeCun:** 基本上，是的。我的意思是，马克列举的很多东西都是正确的，这些东西确实吓坏了大公司。当然，国会调查是其中之一。法律责任。制造出让人伤害自己或伤害他人的东西。像大公司一样，非常小心不要生产这种类型的东西，因为他们……首先，他们不想伤害任何人。其次，他们想保住自己的生意。所以对于像这样不可避免地会形成政治观点和关于各种可能具有政治性或非政治性但人们可能不同意的事情的观点的系统来说，这基本上是不可能的。关于，你知道，道德问题和关于宗教的问题，以及来自不同社区的人们从一开始就会不同意的文化问题，对吧？所以只有相对较少的事情人们会达成一致，基本原则。但是除此之外，如果你想让这些系统有用，它们必然会冒犯一些人，不可避免地。

**Lex Fridman:** 所以开源更好——

**Yann LeCun:** 多样性更好，对吧？

**Lex Fridman:** 开源促进多样性。

**Yann LeCun:** 没错。开源促进多样性。

**Lex Fridman:** 这可能是一个迷人的世界，如果开源世界是真的，如果 Meta 领导并创造了这种开源基础模型的世界，将会出现，就像政府将拥有一个微调的模型。(笑)

**Yann LeCun:** 是的。

**Lex Fridman:** 然后可能，向左和向右投票的人将拥有他们自己的模型和偏好，能够选择。它可能会让我们更加分裂，但这取决于我们人类。我们必须弄清楚……基本上，这项技术使人类能够更有效地成为人类。所有人类提出的困难的伦理问题，我们都将留给我们自己去解决。

**Yann LeCun:** 是的，我的意思是，有一些限制……就像言论自由有限制一样，这些系统可能被授权产生的东西也必须有一些限制，一些护栏。所以我的意思是，这是我一直感兴趣的一件事，在我们之前讨论的那种架构中，系统的输出是通过满足一个目标进行推理的结果。该目标可以包括护栏。我们可以在开源系统中设置护栏。我的意思是，如果我们最终拥有用这个蓝图构建的系统，我们可以在这些系统中设置护栏，保证有一个最低限度的护栏，使系统无害、无毒等等。每个人都会同意的基本的东西。然后人们将添加的微调或人们将添加的额外护栏将迎合他们的社区，无论它是什么。

**Lex Fridman:** 是的，微调将更多地是关于什么是仇恨言论、什么是危险等等的灰色地带。我的意思是，你——

**Yann LeCun:** 或者不同的价值体系。

**Lex Fridman:** 不同的价值体系。但即使是关于如何制造生物武器的目标，例如，我认为这是你评论过的事情，或者至少有一篇论文，一群研究人员试图理解这些 LLMs 的社会影响。我想一个很好的门槛是，LLM 是否比搜索更容易做到，比如谷歌搜索？

**Yann LeCun:** 对。所以越来越多的关于这方面的研究似乎表明它没有帮助。所以拥有一个 LLM 并不能帮助你设计或制造生物武器或化学武器，如果你已经可以使用搜索引擎和图书馆。所以你得到的信息的增加或你获得它的容易程度并没有真正帮助你。这是第一件事。第二件事是，拥有一份关于如何制造化学武器的说明清单是一回事，例如，生物武器。真正制造它是另一回事。这比你想象的要难得多，而且 LLM 对此没有帮助。事实上，世界上没有人，甚至连国家都不使用生物武器，因为大多数时候他们不知道如何保护自己的人民免受它的侵害。所以它实际上太危险了，永远无法使用。它实际上被国际条约禁止。化学武器是不同的。它也被条约禁止，但问题是一样的。在不反噬肇事者的情况下使用它是很困难的。但我们可以问问埃隆·马斯克。就像我可以给你一个非常精确的关于如何制造火箭发动机的说明清单。即使你有一个由 50 名经验丰富的工程师组成的团队来制造它，你仍然需要炸毁其中的十几个才能得到一个能用的。化学武器、生物武器或类似的东西也是如此。它需要现实世界中的专业知识，而 LLM 对此没有帮助。

**Lex Fridman:** 它甚至需要我们一直在谈论的常识专业知识，也就是如何将基于语言的指令具体化到物理世界中，这需要大量的知识，而这些知识并不在指令中。

**Yann LeCun:** 是的，确切地说。实际上有很多生物学家对此发表了评论，作为对那些事情的回应，说你是否意识到做实验室工作有多难？这并不容易。

**Lex Fridman:** 是的。汉斯·莫拉维克再次出现。让我们继续谈谈 LLaMA。

---

**十六、 LLaMA 3 (1:57:56)**

**Lex Fridman:** 马克宣布 LLaMA 3 最终会发布，我不认为有一个发布日期，但你最兴奋的是什么？首先，LLaMA 2 已经发布了，也许还有未来的 LLaMA 3、4、5、6、10，只是 Meta 的开源的未来？

**Yann LeCun:** 嗯，有很多事情。所以会有各种版本的 LLaMA，它们是对以前的 LLaMA 的改进。更大、更好、多模态等等。然后在未来的几代中，能够进行规划、真正理解世界如何运作、也许是从视频中训练出来的系统，所以它们有一些世界模型。也许能够进行我之前谈到的那种推理和规划。这需要多长时间？就像什么时候朝那个方向发展的研究会进入 LLaMA 的产品线，如果你愿意这么说的话？我不知道，我不能告诉你。在我们到达那里之前，我们必须经历一些突破。但你将能够监控我们的进展，因为我们会发表我们的研究，对吧？所以上周我们发表了 V-JEPA 的工作，这是朝着从视频中训练系统迈出的第一步。然后下一步将是基于这种想法、从视频中训练出来的世界模型。DeepMind 也在进行类似的工作，还有加州大学伯克利分校也在进行世界模型和视频方面的工作。很多人都在研究这个。我认为出现了很多好主意。我的赌注是，这些系统将类似于 JEPA，它们不会是生成模型。我们将看看未来会怎样。在……有一位名叫 Danijar Hafner 的绅士做了非常出色的工作，他现在在 DeepMind，他研究过这种类型的模型，学习表示，然后通过强化训练将它们用于规划或学习任务。还有伯克利的 Pieter Abbeel、Sergey Levine 和其他一些人也做了很多这方面的工作。我实际上正在与他们合作，在一些赠款的背景下，以我的纽约大学的身份。然后也通过 Meta 进行合作，因为伯克利的实验室以某种方式与 Meta 有关联，与 FAIR 有关联。所以我认为这非常令人兴奋。我认为我对……感到非常兴奋。自从 10 年前 FAIR 成立以来，以及 30 年前，对不起，35 年前，当我们研究组合网络和神经网络的早期阶段时，我就没有对机器学习和人工智能的方向感到如此兴奋了。所以我非常兴奋，因为我看到了一条通往具有能够理解世界、记忆、计划、推理的系统的人类水平智能的道路。有一些想法可以朝着这个方向取得进展，这些想法可能有机会奏效。我对此感到非常兴奋。我喜欢的是，我们以某种方式走上了一条正确的道路，也许会在我的大脑变成白色的酱汁之前或在我需要退休之前取得成功。(笑)

**Lex Fridman:** 是的。是的。你是否也对……感到兴奋？对你来说，这是否很美妙，涉及到的大量 GPU，整个训练过程，在这个庞大的计算系统上。只是退一步，看看地球，人类一起建造了这些计算设备，并且能够训练这一个大脑，然后我们开源它。(笑) 就像诞生了这个在庞大计算系统上训练出来的开源大脑。还有关于如何训练、如何构建基础设施和硬件、冷却等等的细节。你是否仍然把大部分的兴奋放在它的理论方面？意思是软件。

**Yann LeCun:** 嗯，我多年前曾经是一个硬件工程师。(笑)

**Lex Fridman:** 是的，是的，没错。

**Yann LeCun:** 几十年前。

**Lex Fridman:** 硬件已经改进了一点。改变了一点，是的。

**Yann LeCun:** 我的意思是，当然规模是必要的，但不是充分的。

**Lex Fridman:** 绝对。

**Yann LeCun:** 所以我们当然需要计算。我的意思是，在计算能力方面，我们仍然远远落后于与人脑计算能力相匹配所需的计算能力。这可能会在接下来的几十年里发生，但我们仍然还有一段距离。当然，在功率效率方面，我们还差得很远。所以硬件方面还有很多进步的空间。现在很多进步都不是……我的意思是，有一部分来自硅技术，但很多来自架构创新，还有相当一部分来自实现已经流行的架构的更有效的方法。基本上是 Transformer 和卷积网络的结合，对吧？所以在我们饱和之前，我们还有一段路要走。我们将不得不提出新的原理、新的制造技术、新的基本组件，也许是基于与传统数字 CMOS 不同的原理。

**Lex Fridman:** 有趣。所以你认为为了构建 AmI，ami，我们可能也需要一些硬件创新？

**Yann LeCun:** 嗯，如果我们想让它无处不在，是的，当然。因为我们将不得不降低功耗。今天的 GPU，对吧？是 0.5 千瓦到 1 千瓦。人脑大约是 25 瓦。GPU 的功率远低于人脑。你需要大约 10 万或 100 万个才能与之匹敌。所以我们还差得很远。

---

**十七、 通用人工智能 (AGI) (2:04:20)**

**Lex Fridman:** 你经常说 AGI 不会很快到来。意思是，不是今年，不是未来几年，可能更远。你背后的基本直觉是什么？

**Yann LeCun:** 所以首先，它不会是一个事件，对吧？不知何故，这种被科幻小说和好莱坞推广的想法，即有人会发现 AGI 或人类水平的 AI 或 AmI 的秘密，无论你怎么称呼它，然后打开一台机器，我们就拥有了 AGI。这根本不会发生。它不会是一个事件。这将是一个渐进的过程。我们将拥有能够从视频中学习世界如何运作并学习良好表示的系统吗？是的。在我们让它们达到我们在人类身上观察到的规模和性能之前，还需要相当长的时间。这不会在一天之内发生。我们将拥有能够拥有大量相关记忆的系统吗？这样它们就能记住东西？是的。但同样，这不会在明天发生。我的意思是，有一些基本的技术需要开发。我们有很多，但是让这些技术与一个完整的系统一起工作是另一回事。我们将拥有能够推理和计划的系统吗，也许沿着我之前描述的目标驱动的 AI 架构的方向？是的，但是在我们让这些系统正常工作之前，还需要一段时间。在所有这些都能协同工作之前。除此之外，还有能够像人类大脑一样学习分层规划、分层表示、能够针对许多不同情况进行配置的系统。所有这些至少需要十年时间，可能更长，因为有很多我们现在没有看到的问题，我们还没有遇到过。所以我们不知道在这个框架内是否有一个简单的解决方案。这不仅仅是即将到来的事情。我的意思是，在过去的 12、15 年里，我一直听到人们声称 AGI 即将到来，并且一直都是错的。当他们说的时候，我就知道他们是错的。我称之为胡说八道。(笑)

**Lex Fridman:** 你认为为什么人们一直在说……首先，我的意思是，从人工智能这个术语诞生之初，就有一种永恒的乐观主义，这也许与其他技术不同。这是莫拉维克悖论吗？这是人们对 AGI 如此乐观的解释吗？

**Yann LeCun:** 我不认为这只是莫拉维克悖论。莫拉维克悖论是意识到世界并不像我们想象的那么容易的结果。所以首先，智力不是一个可以用标量、用一个数字来衡量的线性事物。你能说人类比猩猩更聪明吗？在某些方面，是的，但在某些方面，猩猩在很多领域比人类更聪明，这使它们能够在森林中生存，例如 (笑)。

**Lex Fridman:** 所以智商是对智力的一个非常有限的衡量。真正的智力比智商所衡量的要大。

**Yann LeCun:** 嗯，智商可以大致衡量人类的某些东西，但因为人类的形式相对统一，对吧？

**Lex Fridman:** 是的。

**Yann LeCun:** 但它只衡量一种类型的能力，这种能力可能与某些任务相关，但与其他任务无关。但是，如果你谈论的是其他智能实体，它们的基本的、容易做到的事情非常不同，那么这就没有任何意义了。所以智力是一系列技能的集合，以及高效地获得新技能的能力。对吧？一个特定的智能实体拥有的或能够快速学习的技能集合与其他实体的技能集合是不同的。因为这是一个多维的事物，技能集合是一个高维空间，你无法衡量。你无法比较两件事，看哪一个比另一个更聪明。它是多维的。

---

**十八、 AI 末日论者 (2:08:48)**

**Lex Fridman:** 所以你经常反驳所谓的 AI 末日论者。你能解释一下他们的观点以及你为什么认为他们是错的吗？

**Yann LeCun:** 好的。所以 AI 末日论者想象了各种各样的灾难性场景，关于 AI 如何逃脱我们的控制，并基本上杀死我们所有人。(笑) 这依赖于一系列假设，这些假设大多是错误的。所以第一个假设是，超级智能的出现可能是一个事件。在某个时刻，我们将找出秘密，我们将打开一台超级智能的机器。因为我们以前从未这样做过，所以它将接管世界并杀死我们所有人。这是错误的。它不会是一个事件。我们将拥有像猫一样聪明的系统，具有人类水平智能的所有特征，但它们的智力水平将像一只猫，或者也许是一只鹦鹉或类似的东西。然后我们将努力使这些东西更智能。当我们使它们更智能时，我们也会在其中设置一些护栏，并学习如何设置一些护栏，使它们表现得体。我们不会只用一个……这不会是一个努力，而是很多人会这样做。其中一些人将成功地制造出可控、安全并具有正确护栏的智能系统。如果其他人变坏了，那么我们可以用好的系统来对抗坏的系统。(笑) 所以这不会是我们暴露在一个会杀死我们所有人的单一的流氓 AI 面前。这根本不会发生。现在，还有另一个谬误，那就是因为系统是智能的，它就必然想要接管。有一些论点让人们害怕这一点，我认为这些论点也是完全错误的。所以其中一个论点是，在自然界中，似乎更聪明的物种最终会统治其他物种。甚至有时会通过设计，有时只是通过错误来消灭其他物种。所以有一种思维方式，你会说，好吧，如果 AI 系统比我们更智能，它们肯定会消灭我们，如果不是通过设计，仅仅是因为它们不在乎我们。这是荒谬的，有很多原因。第一个原因是，它们不会是一个物种。它们不会是一个与我们竞争的物种。它们不会有统治的欲望，因为统治的欲望是必须硬编码到智能系统中的东西。它在人类身上是硬编码的，在狒狒、黑猩猩、狼身上是硬编码的，但在猩猩身上不是。具有这种统治或服从或以其他方式获得地位的欲望的物种是特定于社会性物种的。像猩猩这样的非社会性物种没有这种欲望。对吧？它们几乎和我们一样聪明。对吧？

**Lex Fridman:** 对你来说，人类没有重大的动机将这种欲望编码到 AI 系统中。在某种程度上，如果有，会有其他的 AI 会因为这个而惩罚它们。胜过他们——

**Yann LeCun:** 嗯，有很多动机让 AI 系统服从于人类。对吧？

**Lex Fridman:** 对。

**Yann LeCun:** 我的意思是，这就是我们将要构建它们的方式，对吧？然后人们会说，哦，但是看看 LLMs。LLMs 是不可控的。他们是对的，LLMs 是不可控的。但是目标驱动的 AI，所以通过优化一个目标来得出答案的系统，意味着它们必须优化这个目标，而这个目标可以包括护栏。一个护栏是服从人类。另一个护栏是，如果它伤害了其他人，就不要服从人类——

**Lex Fridman:** 我以前在某个地方听说过，我不记得了——

**Yann LeCun:** 是的。(Lex 笑) 也许在一本书里。(笑)

**Lex Fridman:** 是的。但是说到那本书，是否也可能出现意想不到的后果？

**Yann LeCun:** 不，当然。所以这不是一个简单的问题，对吧？我的意思是，设计那些护栏，使系统表现得体，这不会是一个简单的、有灵丹妙药的、你可以用数学证明系统可以安全的问题。这将是一个非常渐进的、迭代的设计系统，我们以这样一种方式设置那些护栏，使系统表现得体。有时它们会做一些意想不到的事情，因为护栏不对，我们会纠正它们，使它们做对。不知何故，我们不能稍微做错一点，因为如果我们稍微做错一点，我们都会死，这种想法是荒谬的。我们将逐步进行。我多次使用的类比是涡轮喷气发动机的设计。我们是如何弄清楚如何制造出如此可靠的涡轮喷气发动机的，对吧？我的意思是，这些是极其复杂的硬件，它们在非常高的温度下运行，有时一次运行 20 个小时。我们可以乘坐双引擎喷气式客机以接近音速的速度飞越半个地球。这有多么不可思议？这简直难以置信。我们之所以能够做到这一点，是因为我们发明了关于如何制造安全涡轮喷气发动机的一般原理吗？不，这花了数十年的时间来微调这些系统的设计，使它们变得安全。通用电气或斯奈克玛或其他什么公司内部是否有一个专门负责涡轮喷气发动机安全的独立小组？不。设计的一切都是为了安全。因为一个更好的涡轮喷气发动机也是一个更安全的涡轮喷气发动机，一个更可靠的涡轮喷气发动机。人工智能也是如此。你是否需要特定的规定来使人工智能安全？不，你需要制造更好的人工智能系统，它们将是安全的，因为它们被设计得更有用、更可控。

**Lex Fridman:** 所以让我们想象一个系统，一个人工智能系统，它能够非常令人信服，并且可以说服你相信任何事情。我至少可以想象这样一个系统。我可以看到这样一个系统是类似武器的，因为它可以控制人们的思想，我们很容易上当受骗。我们想要相信一件事。你可以拥有一个控制它的人工智能系统，你可以看到政府将它用作武器。所以你认为如果你想象这样一个系统，它与核武器有什么相似之处吗？

**Yann LeCun:** 没有。

**Lex Fridman:** 那么为什么这项技术不同呢？所以你是说会有一个渐进的发展？

**Yann LeCun:** 是的。

**Lex Fridman:** 我的意思是，它可能很快，但它们将是迭代的。然后我们将能够做出反应等等。

**Yann LeCun:** 所以那个由弗拉基米尔·普京或 whatever 设计的 AI 系统，或者他的喽啰 (笑) 将会试图与每个美国人交谈，说服他们投票给——

**Lex Fridman:** 无论谁。

**Yann LeCun:** 无论谁取悦普京或其他什么。或者煽动人们互相攻击，就像他们一直试图做的那样。它们不会和你说话，它们会和你的 AI 助手说话，它将和它们一样聪明，对吧？因为正如我所说，在未来，你与数字世界的每一次互动都将由你的 AI 助手来调解。所以你要问的第一件事是，这是一个骗局吗？就像这个东西告诉我的是真的吗？它甚至无法到达你那里，因为它只会和你的 AI 助手说话，而你的 AI 甚至不会……它就像一个垃圾邮件过滤器，对吧？你甚至看不到电子邮件，垃圾邮件，对吧？它被自动放入一个你永远看不到的文件夹中。这将是同样的事情。那个试图说服你相信某事的 AI 系统，它将与一个至少和它一样聪明的 AI 系统交谈。并且会说，这是垃圾邮件。(笑) 它甚至不会引起你的注意。

**Lex Fridman:** 所以对你来说，任何一个 AI 系统都很难取得如此大的飞跃，以至于它可以说服甚至其他的 AI 系统？所以就像总会有这种竞赛，没有人遥遥领先？

**Yann LeCun:** 这就是世界的历史。世界的历史是，每当一个地方取得进步，就会有应对措施。这是一场猫捉老鼠的游戏。

**Lex Fridman:** 大多数情况下是这样，但这就是为什么核武器如此有趣，因为那是一种如此强大的武器，重要的是谁首先得到它。你可以想象希特勒、斯大林、毛首先得到这种武器，这对世界产生的影响与美国首先得到这种武器不同。对你来说，核武器就像……你不认为会有突破性的发现，然后是曼哈顿计划那样的人工智能努力？

**Yann LeCun:** 不。正如我所说，它不会是一个事件。这将是一个持续的进步。每当一个突破发生时，它都会很快地广泛传播。可能首先是在行业内。我的意思是，这不是一个政府或军事组织特别有创新精神的领域，他们实际上远远落后。所以这将来自行业。这种信息传播得非常快。我们在过去几年里已经看到了这一点，对吧？你有一个新的……甚至拿 AlphaGo 来说。这在三个月内就被复制出来了，即使没有特别详细的信息，对吧？

**Lex Fridman:** 是的。这个行业不擅长保密。(笑)

**Yann LeCun:** 但即使有，仅仅是你知道某件事情是可能的，就会让你意识到值得投入时间去真正做到它。你可能是第二个做到的人，但你还是会做到。比如说，对于自监督学习、transformers、仅解码器架构、LLMs 的所有创新。我的意思是，你不需要确切地知道它们是如何工作的，就能知道这是可能的，因为它已经被部署了，然后它就会被复制。然后为这些公司工作的人会流动。他们从一家公司跳到另一家公司。信息会传播。是什么造就了美国科技产业的成功，特别是硅谷的成功，正是因为这一点，因为信息流通得非常非常快，传播得非常快。所以整个地区都处于领先地位，因为信息的流通。

**Lex Fridman:** 也许让我们继续谈谈 AI 末日论者的心理。你以经典的 Yann LeCun 的方式，给出了一个很好的例子，当一项新技术出现时，你说工程师说，“我发明了这个新东西，我称之为圆珠笔。”然后推特圈回应说，“天哪，人们可以用它写可怕的东西，比如错误信息、宣传、仇恨言论。现在就禁止它！”然后写作末日论者出现了，类似于 AI 末日论者，“想象一下，如果每个人都能得到一支圆珠笔。这可能会摧毁社会。应该有一部法律禁止使用圆珠笔写仇恨言论，现在就监管圆珠笔。”然后铅笔行业的巨头说，“是的，圆珠笔非常危险，不像铅笔写的东西可以擦掉，圆珠笔写的东西永远存在。政府应该要求圆珠笔制造商获得许可。”我的意思是，这似乎是人类心理的一部分，当它遇到新技术时。关于这一点，你能谈谈什么深刻的见解？

**Yann LeCun:** 嗯，人们对新技术及其可能对社会产生的影响有一种自然的恐惧。人们对他们所知道的世界受到重大变革的威胁有一种本能的反应，这些变革要么是文化现象，要么是技术革命。他们担心自己的文化，担心自己的工作，担心孩子的未来和他们的生活方式，对吧？所以任何变化都会受到恐惧。你在历史上都可以看到这一点，任何技术革命或文化现象总是伴随着媒体上的团体或反应，这些团体或反应基本上把所有问题，社会当前的问题都归咎于那个特定的变化，对吧？电力在某个时候会杀死所有人。火车将是一件可怕的事情，因为你在超过每小时 50 公里时无法呼吸。所以有一个很棒的网站叫 Pessimists Archive，对吧？它有所有那些关于人们想象的由于技术创新或文化现象而会到来的所有可怕事情的剪报 (笑)。关于爵士乐或漫画书被指责为失业或年轻人不想再工作之类的美妙例子，对吧？这已经存在了几个世纪。这是下意识的反应。问题是，我们是拥抱变革还是抵制变革？什么是真正的危险，而不是想象中的危险？

**Lex Fridman:** 所以人们担心……我认为他们担心大科技公司的一件事，这是我们一直在谈论的事情，但我认为值得再次提及，他们担心人工智能将有多么强大，他们担心它掌握在一个集中的权力手中，只有少数人集中控制。所以这就是对大科技公司的怀疑。这些公司可以赚取巨额利润并控制这项技术。通过这样做，利用、滥用社会中的小人物。

**Yann LeCun:** 嗯，这正是我们需要开源平台的原因。

**Lex Fridman:** 是的。我只是想……(笑) 把这一点说得更清楚。

**Yann LeCun:** 是的。

---

**十九、 对未来的希望 (2:38:00) 与 总结 (2:45:54)**

**Lex Fridman:**  Joscha Bach 在推特上发了一些你 LOL 的东西，提到了 HAL 9,000。引用：“我很欣赏你的论点，我完全理解你的挫败感，但是否应该打开舱门是一个复杂而微妙的问题。”所以你是 Meta AI 的负责人。这确实让我很担心，我们的 AI 霸主会用这种性质的公司言论对我们说话，而你用你的方式抵制了这一点。这是你可以评论的事情吗，在一个大公司工作，你如何才能避免过度的恐惧，我想，通过谨慎造成的伤害？

**Yann LeCun:** 是的。再次强调，我认为解决这个问题的答案是开源平台，然后让各种各样的人能够构建代表世界各地不同文化、观点、语言和价值体系的多样性的 AI 助手。这样你就不会因为一个单一的 AI 实体而被束缚，被一种特定的思维方式洗脑。所以我的意思是，我认为这对社会来说是一个非常非常重要的问题。我看到的问题，这就是为什么我一直如此直言不讳，有时甚至有点讽刺——

**Lex Fridman:** 永远不要停止。永远不要停止，Yann。(两人都笑了) 我们喜欢它。

**Yann LeCun:** 是因为我认为通过专有 AI 系统集中权力的危险性比其他任何事情都要大得多。如果我们真的想要多样化的观点 AI 系统，在未来，我们都将通过 AI 系统进行互动，我们需要这些系统是多样化的，以保持思想、信条、政治观点等等的多样性，并维护民主。而与之相反的是那些认为出于安全原因，我们应该把 AI 系统锁起来的人，因为把它交给每个人太危险了，因为它可能被恐怖分子利用或其他什么。那将导致一个非常糟糕的未来，我们所有的信息都将由少数公司通过专有系统来控制。

**Lex Fridman:** 所以你相信人类会用这项技术构建对人类整体有益的系统？

**Yann LeCun:** 这不就是民主和言论自由的意义所在吗？

**Lex Fridman:** 我想是的。

**Yann LeCun:** 你相信机构会做正确的事情吗？你相信人们会做正确的事情吗？是的，会有坏人做坏事，但他们不会拥有比好人更先进的技术。所以这将是我的好 AI 对抗你的坏 AI，对吧？我的意思是，这就是我们刚才谈到的例子，也许某个流氓国家会建立一些 AI 系统，试图说服每个人发动内战或其他什么，或者选举一个有利的统治者。但他们将不得不通过我们的 AI 系统，对吧？(笑)

**Lex Fridman:** 一个口音浓重的 AI 系统将试图说服我们的——

**Yann LeCun:** 而且他们的句子里没有任何冠词。(两人都笑了)

**Lex Fridman:** 嗯，至少，这将是荒谬的喜剧。好的。所以既然我们谈到了物理现实——

**Lex Fridman:** 我很想问问你对机器人在这个物理现实中的未来的愿景。你所说的很多智能都会使机器人成为与我们人类更有效的合作者。所以自从特斯拉的 Optimus 团队向我们展示了人形机器人的一些进展以来，我认为这确实振兴了整个行业，我认为波士顿动力公司已经领导了很长很长时间。所以现在有各种各样的公司，Figure AI，显然还有波士顿动力公司——

**Yann LeCun:** Unitree。

**Lex Fridman:** Unitree。但是有很多。这很好。这很好。我的意思是，我喜欢它。所以你认为很快就会有数百万的人形机器人在周围走动吗？

**Yann LeCun:** 不会很快，但这将会发生。就像下一个十年，我认为在机器人领域将会非常有趣。就像机器人产业的出现已经等待了 10 年、20 年，除了像预编程的行为之类的东西之外，并没有真正出现。主要的问题是，又是莫拉维克悖论。就像我们如何让系统理解世界是如何运作的并进行计划？所以我们可以为非常专门的任务做到这一点。波士顿动力公司的方法基本上是使用大量手工制作的动力学模型和预先仔细的计划，这是非常经典的机器人技术，带有一点创新，一点感知，但它仍然不是……就像他们不能制造一个家用机器人，对吧？我们距离完全自主的 L5 级自动驾驶还有一段距离。我们当然还远远没有达到通过 20 小时的驾驶就能训练自己的系统来实现 L5 级自动驾驶的程度，就像任何 17 岁的年轻人一样。所以，直到我们再次拥有世界模型，能够训练自己理解世界如何运作的系统，我们在机器人领域才会有重大的进展。所以现在很多从事机器人硬件工作的人都在押注或指望人工智能将朝着这个方向取得足够的进展。

**Lex Fridman:** 他们也希望在其中发现一个产品——

**Yann LeCun:** 是的。

**Lex Fridman:** 在你拥有一个真正强大的世界模型之前，将有一个几乎强大的世界模型。人们试图在一个笨拙的机器人中找到一个产品，我想。就像不是一个非常高效的机器人。所以有一个工厂环境，人形机器人可以帮助自动化工厂的某些方面。我认为这是一个非常困难的任务，因为需要所有的安全措施和所有这些东西，我认为在家里更有趣。但是然后你开始思考……我想你提到了装洗碗机，对吧？

**Yann LeCun:** 是的。

**Lex Fridman:** 就像我想这是你正在研究的主要问题之一。

**Yann LeCun:** 我的意思是还有打扫卫生。(笑)

**Lex Fridman:** 是的。

**Yann LeCun:** 打扫房子，饭后收拾桌子，洗碗，所有这些任务，做饭。我的意思是，所有原则上可以自动化但实际上非常复杂、非常困难的任务。

**Lex Fridman:** 但是即使是在充满不确定性的空间中进行基本的导航——

**Yann LeCun:** 这还是可以的。就像你现在可以做的那样。导航是没有问题的。

**Lex Fridman:** 嗯，以一种对我们人类来说有吸引力的方式进行导航是另一回事。

**Yann LeCun:** 是的。这不一定会……我的意思是，我们实际上有演示，因为在 FAIR 有一个所谓的具身人工智能小组，他们没有制造自己的机器人，而是使用商用机器人。你可以告诉机器狗去冰箱，它们实际上可以打开冰箱，它们可能可以拿起冰箱里的一罐东西，然后把它带给你。所以它可以导航，它可以抓取物体，只要它已经被训练来识别它们，现在的视觉系统工作得很好。但它不是一个完全通用的机器人，它不够复杂，无法完成像收拾餐桌这样的事情。(笑)

**Lex Fridman:** 是的，对我来说，这是让人形机器人进入家庭的一个令人兴奋的未来。总的来说，机器人越来越多地进入家庭，因为它让人类真正在物理空间中与人工智能系统互动。通过这样做，它使我们能够在哲学上、心理上探索我们与机器人的关系。这可能真的非常非常有趣。所以我希望你在整个 JEPA 的事情上很快取得进展。(笑)

**Yann LeCun:** 嗯，我的意思是，我希望事情能按计划进行。我的意思是，我们一直在研究这个从视频中进行自监督学习的想法已经有 10 年了。在过去的两三年里才取得了重大进展。

**Lex Fridman:** 你实际上提到过，即使没有大量的计算资源，也可能有很多有趣的突破。所以如果你有兴趣攻读这方面的博士学位，仍然有很多机会可以做创新的工作。所以，你会给一个即将读研并攻读博士学位的本科生什么建议？

**Yann LeCun:** 所以基本上，我已经列出了它们。这个想法，你如何通过观察来训练一个世界模型？你不必一定在巨大的数据集上进行训练。我的意思是，最终可能需要在大数据集上进行训练才能产生像我们在 LLMs 中看到的那样的涌现特性。但我认为有很多好主意可以在不一定扩展的情况下完成。然后是如何利用学习到的世界模型进行规划？如果系统演变的世界不是物理世界，而是比如说互联网的世界，或者某种世界，其中一个行动包括在搜索引擎中进行搜索，或者查询数据库，或者运行模拟，或者调用计算器，或者解微分方程，你如何让一个系统真正地计划一系列的行动来给出问题的解决方案？所以计划的问题不仅仅是计划物理行动的问题，它可以是计划使用工具的行动，对于一个对话系统或任何类型的智能系统来说。这方面有一些工作，但不是很多。FAIR 有一些工作，一个叫做 Toolformer，这是几年前的事了，还有一些最近关于规划的工作，但我认为我们还没有任何好的解决方案。然后是分层规划的问题。我提到的从纽约到巴黎的旅行计划的例子，这是分层的，但几乎我们采取的每一个行动都在某种意义上涉及到分层规划。我们真的完全不知道如何做到这一点。就像在人工智能中，分层规划的演示为零，其中必要的各种层次的表示已经被学习到。我们可以做两级的分层规划，当我们设计这两级的时候。例如，你有一个狗腿机器人，对吧？你想让它从客厅走到厨房。你可以计划一条避开障碍物的路径。然后你可以把它发送到一个低级规划器，该规划器会弄清楚如何移动腿来遵循轨迹，对吧？所以这是有效的，但是这两级规划是手工设计的，对吧？我们指定了适当的抽象层次，每个抽象层次上的表示必须是什么。你如何学习这个？你如何学习行动计划的分层表示，对吧？通过卷积网络和深度学习，我们可以训练系统学习感知的层次表示。当你试图表示的是行动计划时，等价的东西是什么？

**Lex Fridman:** 对于行动计划。是的。所以你基本上想要一个机器狗或人形机器人，它可以启动并独自从纽约旅行到巴黎。

**Yann LeCun:** 例如。

**Lex Fridman:** 它在 TSA 可能会遇到一些麻烦，但是——

**Yann LeCun:** 不，即使是做一些相当简单的事情，比如家务。

**Lex Fridman:** 当然。

**Yann LeCun:** 比如做饭或类似的事情。

**Lex Fridman:** 是的。这涉及到很多东西。这是一项超级复杂的任务。再一次，我们认为这是理所当然的。你对人类的未来有什么希望？我们谈论了这么多令人兴奋的技术，这么多令人兴奋的可能性。当你展望未来 10 年、20 年、50 年、100 年时，是什么给了你希望？如果你看看社交媒体，就会发现战争正在发生，存在分裂、仇恨，所有这些也是人性的一部分的东西。但在所有这些之中，是什么给了你希望？

**Yann LeCun:** 我喜欢这个问题。我们可以通过人工智能让人类更聪明。好的？我的意思是，人工智能基本上会放大人类的智能。就好像我们每个人都将拥有一群聪明的 AI 助手。它们可能比我们更聪明。它们会按照我们的吩咐去做，也许会以比我们自己做得好得多的方式执行任务，因为它们会比我们更聪明。所以这就像每个人都将成为一群超级聪明的虚拟人的老板。所以我们不应该对此感到威胁，就像我们不应该对成为一群人的经理感到威胁一样，其中一些人比我们更聪明。在这方面，我当然有很多经验。(笑) 和比我聪明的人一起工作。这实际上是一件很棒的事情。所以拥有比我们更聪明的机器，在我们所有的任务、我们的日常生活中，无论是专业的还是个人的，协助我们，我认为这将是一件绝对美妙的事情。因为智力是最需要的商品。我的意思是，人类犯下的所有错误都是因为缺乏智力，真的，或者缺乏知识，这是相关的。所以让人们更聪明只会更好。我的意思是，就像公共教育是一件好事一样，书籍是一件好事，互联网本质上也是一件好事。甚至社交网络也是一件好事，如果你正确地运行它们的话。(笑) 这很困难，但你可以做到。因为它有助于信息的交流和知识的传播。所以人工智能将使人类更聪明。我一直在使用的类比是，也许在人类历史上与人工智能助手的普及所带来的影响相当的事件是印刷机的发明。它让每个人都变得更聪明。人们可以获得书籍。书籍比以前便宜得多。所以更多的人有动力去学习阅读，而这在以前是不存在的。人们变得更聪明了。它促成了启蒙运动，对吧？没有印刷机就不会有启蒙运动。它促成了哲学、理性主义，摆脱了宗教教条、民主、科学。当然，没有这个，就不会有美国革命或法国革命。所以我们可能仍然处于封建制度之下。所以它彻底改变了世界，因为人们变得更聪明，并且了解了事物。现在，它也造成了欧洲 200 年的宗教冲突，对吧？因为人们读到的第一件事是圣经，并且意识到也许对圣经的解释与牧师告诉他们的不同。所以这导致了新教运动，并造成了裂痕。事实上，天主教会不喜欢印刷机的想法，但他们别无选择。所以它有一些坏的影响，也有一些好的影响。我认为今天没有人会说印刷机的发明总体上产生了负面影响，尽管它在欧洲造成了 200 年的宗教冲突。现在比较一下，我对自己想出这个类比感到非常自豪，但后来意识到以前有人也提出了同样的想法。将此与奥斯曼帝国发生的事情进行比较。奥斯曼帝国禁止印刷机 200 年。它并没有禁止所有语言，只禁止了阿拉伯语。你实际上可以在奥斯曼帝国印刷拉丁语或希伯来语或其他任何语言的书籍，只是不能印刷阿拉伯语。我以为这是因为统治者只是想保持对人民的控制，以及宗教教条和一切。但在与阿联酋人工智能部长 Omar Al Olama 交谈后，他告诉我，不，还有另一个原因。另一个原因是，这是为了保护书法家这个行业，对吧？有一种艺术形式是用这种美丽的阿拉伯诗歌或任何宗教文本来书写。这是一个非常强大的抄写员行业，基本上控制了帝国的一大块。我们不能让他们失业。所以他们在某种程度上禁止了印刷机，以保护这个行业。现在，今天的 AI 的类比是什么？我们通过禁止 AI 来保护谁？要求监管 AI 以保护他们工作的人是谁？当然，这是一个真正的问题，像 AI 这样的技术变革对就业市场和劳动力市场的影响是什么？有一些经济学家在这方面比我专业得多，但当我和他们交谈时，他们告诉我，我们不会失去工作。这不会导致大规模失业。这只会是不同职业的逐渐转变。未来 10 年或 15 年将会热门的职业，我们今天不知道它们会是什么。就像如果我们回到过去 20 年，谁能想到 20 年前，甚至 5、10 年前最热门的工作是移动应用开发人员？当时智能手机还没有发明。

**Lex Fridman:** 未来的大部分工作可能都在元宇宙中。(笑)

**Yann LeCun:** 嗯，有可能。是的。

**Lex Fridman:** 但关键是，你不可能预测。但你是对的。我的意思是，你提出了很多强有力的观点。我相信人是生来善良的，所以如果 AI，特别是开源的 AI，能够让他们更聪明，那么它只会增强人类的善良力量。

**Yann LeCun:** 所以我也有同感。好的？我认为人是生来善良的。(笑) 实际上很多末日论者之所以是末日论者，是因为他们不相信人是生来善良的。他们要么不相信人，要么不相信机构会做正确的事情，从而使人们表现得体。

**Lex Fridman:** 嗯，我想我们两个都相信人性，我想我代表很多人说，感谢你推动开源运动，推动将研究和人工智能都开源，让人们可以使用它，还有模型本身，也让它开源。所以感谢你。感谢你以如此丰富多彩和美妙的方式在互联网上表达你的想法。我希望你永远不要停止。你是我认识的最有趣的人之一，也是我的偶像。所以 Yann，再次感谢你和我交谈，感谢你做你自己。

**Yann LeCun:** 谢谢你，Lex。

**Lex Fridman:** 感谢收听 Yann LeCun 的本次谈话。为了支持这个播客，请查看说明中的赞助商。现在让我用阿瑟·C·克拉克的一些话来结束，“发现可能的极限的唯一方法是超越它们，进入不可能。”感谢收听，希望下次再见。
